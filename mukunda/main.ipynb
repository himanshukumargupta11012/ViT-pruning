{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 12:09:38.729412: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-06 12:09:38.745898: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741243178.765565 2454497 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741243178.771471 2454497 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-06 12:09:38.792081: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor, AutoConfig\n",
    "from transformers.models.vit.modeling_vit import ViTModel, ViTLayer, ViTEncoder, ViTConfig\n",
    "from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n",
    "\n",
    "import sys\n",
    "from loguru import logger\n",
    "from typing import Optional,Union, Tuple\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "current_file_name = os.path.splitext(os.path.basename(os.getcwd()))[0]\n",
    "logger.remove() #remove default\n",
    "logger.add(f\"{current_file_name}.html\", format=\"<b>{time}</b> {time:YYYY-MM-DD HH:mm} | {level} | {message}\", mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load CIFAR100 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "seed_val = 42\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size  = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "sim_threshold = 0.95\n",
    "mlp_threshold = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, arr,use_batch_norm=True):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(len(arr) - 2):\n",
    "            layers.append(nn.Linear(arr[i], arr[i+1]))\n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(arr[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(arr[-2],arr[-1]));\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.size = len(arr);\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.criterion =  nn.BCELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def train(self, inputs, targets):\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = self.criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "class ModifyLayer(ViTLayer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.embedding_size = config.hidden_size\n",
    "        self.mlp = NeuralNet([2 * self.embedding_size, 64, 1])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "        original: Optional[torch.Tensor] = None,  # custom parameter\n",
    "        train_mlp : bool = False\n",
    "        ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n",
    "\n",
    "        outputs = ()\n",
    "\n",
    "        cls_tokens = hidden_states[:, 0, :].unsqueeze(1) #[2, 1, 768]          \n",
    "        non_cls_tokens = hidden_states[:, 1:, :]        # [2, 196, 768]\n",
    "        cls_expanded = cls_tokens.expand(-1, non_cls_tokens.size(1), -1)  # Now shape: [2, 196, 768]\n",
    "        mlp_inputs = torch.cat((cls_expanded, non_cls_tokens), dim=-1) # Result shape: [2, 196, 1536]\n",
    "        mlp_inputs = mlp_inputs.reshape(-1,2*self.embedding_size) # result  shape: [2 * 196, 1536]\n",
    "        attention_mask = None\n",
    "\n",
    "        if train_mlp:\n",
    "            # 1) get the original outputs for netx layer\n",
    "            with torch.no_grad():\n",
    "                original_output = super().forward(original)\n",
    "                original = original_output[0]\n",
    "\n",
    "            # 2) getting similarity score\n",
    "            num_tokens = len(original[0])\n",
    "            similarity = F.cosine_similarity(original.reshape(batch_size*num_tokens,-1),hidden_states.reshape(batch_size*num_tokens,-1), dim=1)\n",
    "            similarity = similarity.reshape(batch_size,num_tokens)\n",
    "            attention_mask = similarity < sim_threshold #1 need to be used in transformer\n",
    "            attention_mask[:,0] = True\n",
    "        \n",
    "            # 3) mlp\n",
    "            mlp_loss = self.mlp.train(mlp_inputs.clone().detach(),attention_mask[:,1:].reshape(-1).unsqueeze(-1).float().clone().detach()) #att_mask shape: [2 * 196, 1]\n",
    "        else:\n",
    "            attention_mask = self.mlp(mlp_inputs) < mlp_threshold #1 need to be used in transformer\n",
    "            attention_mask[:,0] = True\n",
    "            \n",
    " \n",
    "        for i in range(attention_mask.size(0)):\n",
    "            trimmed_input = hidden_states[i][attention_mask[i] == 1]\n",
    "            layer_output = super().forward(trimmed_input.unsqueeze(0)) # expect 3d tensor\n",
    "            hidden_states[i][attention_mask[i] == 1] = layer_output[0]\n",
    "        \n",
    "\n",
    "        outputs = (hidden_states,) + outputs\n",
    "        return outputs, original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "class ModifyEncoder(ViTEncoder):\n",
    "    def __init__(self,config):\n",
    "        super().__init__(config)\n",
    "        self.layer = nn.ModuleList()\n",
    "        for i in range(config.num_hidden_layers):\n",
    "            self.layer.add_module(f'layer{i}',ModifyLayer(config))\n",
    "\n",
    "    def forward(self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False,\n",
    "        output_hidden_states: bool = False,\n",
    "        return_dict: bool = True,\n",
    "        output_mask: bool = False,\n",
    "        train_mlp : bool = False,\n",
    "        ) -> Union[tuple, BaseModelOutput]:\n",
    "\n",
    "        # optional to necessary\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_boolean_mask = () if output_mask else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "\n",
    "        original = hidden_states.clone().detach().to(device)  # make deep copy\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    layer_module.__call__,\n",
    "                    hidden_states,\n",
    "                    layer_head_mask,\n",
    "                    output_attentions,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs,original= layer_module(hidden_states, layer_head_mask, output_attentions,original,train_mlp)\n",
    "                print(\"loop done\")\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "            if output_mask:\n",
    "                all_boolean_mask = all_boolean_mask + (layer_outputs[1],)\n",
    "        \n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "        ), all_boolean_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# https://huggingface.co/docs/transformers/en/model_doc/vit#transformers.ViTModel (for more info on parameters)\n",
    "\n",
    "class Modifymodel(ViTModel): # return Union[tuple, BaseModelOutputwithpooling]\n",
    "    def __init__(self,config,processor):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.encoder = ModifyEncoder(config)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.processor = processor\n",
    "\n",
    "    def forward( self,\n",
    "        pixel_values: Optional[torch.Tensor] = None,\n",
    "        bool_masked_pos: Optional[torch.BoolTensor] = None, # bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`, *optional*): Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        interpolate_pos_encoding: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        output_mask: Optional[bool] = None,\n",
    "        train_mlp : bool = False,\n",
    "        ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "\n",
    "        # converting optinal type to normal type\n",
    "        if pixel_values is None:\n",
    "            raise ValueError(\"You have to specify pixel_values\")\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        # Prepare head mask if needed 1.0 in head_mask indicate we keep the head and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "        print(\"head_mask\",head_mask)\n",
    "        # start\n",
    "        # 1) add embeddings after processing\n",
    "        embedding_output = self.embeddings(pixel_values, bool_masked_pos=bool_masked_pos, interpolate_pos_encoding=interpolate_pos_encoding)\n",
    "        print('embedding_output',embedding_output.shape)\n",
    "        encoder_outputs, boolean_masks = self.encoder(\n",
    "            embedding_output,\n",
    "            head_mask=head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            output_mask=output_mask,\n",
    "            train_mlp = train_mlp\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        sequence_output = self.layernorm(sequence_output)\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "        \n",
    "        if not return_dict: # Whether or not to return a ModelOutput instead of a plain tuple.\n",
    "            head_outputs = (sequence_output, pooled_output) if pooled_output is not None else (sequence_output,)\n",
    "            return head_outputs + encoder_outputs[1:]\n",
    "        else:\n",
    "            outputs = BaseModelOutputWithPooling(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            )\n",
    "            logits = self.classifier(outputs['last_hidden_state'][:, 0])\n",
    "            setattr(outputs, 'logits', logits)\n",
    "            setattr(outputs, 'boolean_masks', boolean_masks)\n",
    "            return outputs\n",
    "        \n",
    "\n",
    "    def load_weights(self, pretrained_model_name_or_path: str):\n",
    "        pretrained_model = ViTModel.from_pretrained(pretrained_model_name_or_path)\n",
    "        self.load_state_dict(pretrained_model.state_dict(), strict=False)\n",
    "\n",
    "\n",
    "    def train(self, train_loader=train_loader, epochs=1):\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        loss_history = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for inputs, targets in train_loader:\n",
    "                processed_inputs = self.processor(inputs, return_tensors=\"pt\")\n",
    "                pixel_values = processed_inputs[\"pixel_values\"].to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                outputs = self.forward(pixel_values=pixel_values, train_mlp=True).logits\n",
    "                print(outputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            avg_loss = epoch_loss / len(train_loader)\n",
    "            loss_history.append(avg_loss)\n",
    "            logger.info(f\"<p>Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}</p>\")\n",
    "\n",
    "\n",
    "        plt.plot(loss_history, label='Training Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss Curve')\n",
    "        plt.legend()\n",
    "        plot_path = \"outputs/training_loss.png\"\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "\n",
    "        logger.info(f'<p>Training loss plot saved at: {plot_path}</p>')\n",
    "        logger.info(f'<img src=\"{plot_path}\" alt=\"Training Loss Curve\">')\n",
    "\n",
    "    def test(self, test_loader=test_loader):\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        total_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                processed_inputs = self.processor(inputs, return_tensors=\"pt\")\n",
    "                pixel_values = processed_inputs[\"pixel_values\"].to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                outputs = self.forward(pixel_values=pixel_values, train_mlp=False).logits\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(test_loader)\n",
    "        logger.info(f\"<p>Test Loss: {avg_loss:.4f}</p>\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def get_source_code(obj):\n",
    "    try:\n",
    "        return inspect.getsource(obj.__class__)\n",
    "    except Exception as e:\n",
    "        return f\"Unable to retrieve source code: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"tzhao3/DeiT-CIFAR100\"\n",
    "model = AutoModelForImageClassification.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@add_start_docstrings(\n",
      "    \"\"\"\n",
      "    ViT Model transformer with an image classification head on top (a linear layer on top of the final hidden state of\n",
      "    the [CLS] token) e.g. for ImageNet.\n",
      "\n",
      "    <Tip>\n",
      "\n",
      "        Note that it's possible to fine-tune ViT on higher resolution images than the ones it has been trained on, by\n",
      "        setting `interpolate_pos_encoding` to `True` in the forward of the model. This will interpolate the pre-trained\n",
      "        position embeddings to the higher resolution.\n",
      "\n",
      "    </Tip>\n",
      "    \"\"\",\n",
      "    VIT_START_DOCSTRING,\n",
      ")\n",
      "class ViTForImageClassification(ViTPreTrainedModel):\n",
      "    def __init__(self, config: ViTConfig) -> None:\n",
      "        super().__init__(config)\n",
      "\n",
      "        self.num_labels = config.num_labels\n",
      "        self.vit = ViTModel(config, add_pooling_layer=False)\n",
      "\n",
      "        # Classifier head\n",
      "        self.classifier = nn.Linear(config.hidden_size, config.num_labels) if config.num_labels > 0 else nn.Identity()\n",
      "\n",
      "        # Initialize weights and apply final processing\n",
      "        self.post_init()\n",
      "\n",
      "    @add_start_docstrings_to_model_forward(VIT_INPUTS_DOCSTRING)\n",
      "    @add_code_sample_docstrings(\n",
      "        checkpoint=_IMAGE_CLASS_CHECKPOINT,\n",
      "        output_type=ImageClassifierOutput,\n",
      "        config_class=_CONFIG_FOR_DOC,\n",
      "        expected_output=_IMAGE_CLASS_EXPECTED_OUTPUT,\n",
      "    )\n",
      "    def forward(\n",
      "        self,\n",
      "        pixel_values: Optional[torch.Tensor] = None,\n",
      "        head_mask: Optional[torch.Tensor] = None,\n",
      "        labels: Optional[torch.Tensor] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        interpolate_pos_encoding: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[tuple, ImageClassifierOutput]:\n",
      "        r\"\"\"\n",
      "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
      "            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n",
      "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
      "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
      "        \"\"\"\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        outputs = self.vit(\n",
      "            pixel_values,\n",
      "            head_mask=head_mask,\n",
      "            output_attentions=output_attentions,\n",
      "            output_hidden_states=output_hidden_states,\n",
      "            interpolate_pos_encoding=interpolate_pos_encoding,\n",
      "            return_dict=return_dict,\n",
      "        )\n",
      "\n",
      "        sequence_output = outputs[0]\n",
      "\n",
      "        logits = self.classifier(sequence_output[:, 0, :])\n",
      "\n",
      "        loss = None\n",
      "        if labels is not None:\n",
      "            # move labels to correct device to enable model parallelism\n",
      "            labels = labels.to(logits.device)\n",
      "            if self.config.problem_type is None:\n",
      "                if self.num_labels == 1:\n",
      "                    self.config.problem_type = \"regression\"\n",
      "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
      "                    self.config.problem_type = \"single_label_classification\"\n",
      "                else:\n",
      "                    self.config.problem_type = \"multi_label_classification\"\n",
      "\n",
      "            if self.config.problem_type == \"regression\":\n",
      "                loss_fct = MSELoss()\n",
      "                if self.num_labels == 1:\n",
      "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
      "                else:\n",
      "                    loss = loss_fct(logits, labels)\n",
      "            elif self.config.problem_type == \"single_label_classification\":\n",
      "                loss_fct = CrossEntropyLoss()\n",
      "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
      "            elif self.config.problem_type == \"multi_label_classification\":\n",
      "                loss_fct = BCEWithLogitsLoss()\n",
      "                loss = loss_fct(logits, labels)\n",
      "\n",
      "        if not return_dict:\n",
      "            output = (logits,) + outputs[1:]\n",
      "            return ((loss,) + output) if loss is not None else output\n",
      "\n",
      "        return ImageClassifierOutput(\n",
      "            loss=loss,\n",
      "            logits=logits,\n",
      "            hidden_states=outputs.hidden_states,\n",
      "            attentions=outputs.attentions,\n",
      "        )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_source_code(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at tzhao3/DeiT-CIFAR100 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Modifymodel(\n",
       "  (embeddings): ViTEmbeddings(\n",
       "    (patch_embeddings): ViTPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): ModifyEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x ModifyLayer(\n",
       "        (attention): ViTSdpaAttention(\n",
       "          (attention): ViTSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (mlp): NeuralNet(\n",
       "          (model): Sequential(\n",
       "            (0): Linear(in_features=1536, out_features=64, bias=True)\n",
       "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=64, out_features=1, bias=True)\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "          (criterion): BCELoss()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (pooler): ViTPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "modifymodel = Modifymodel(config,processor)\n",
    "modifymodel.load_weights(model_name)\n",
    "modifymodel.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar  2 15:27:52 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A5000               Off | 00000000:3B:00.0 Off |                  Off |\n",
      "| 30%   23C    P8              18W / 230W |   7589MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A5000               Off | 00000000:AF:00.0 Off |                  Off |\n",
      "| 30%   50C    P2              75W / 230W |   7854MiB / 24564MiB |     24%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A5000               Off | 00000000:D8:00.0 Off |                  Off |\n",
      "| 30%   25C    P8              17W / 230W |  24068MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1633      G   /usr/lib/xorg/Xorg                           25MiB |\n",
      "|    0   N/A  N/A      1738      G   /usr/bin/gnome-shell                          4MiB |\n",
      "|    0   N/A  N/A   2369178      C   ...4004/anaconda3/envs/itv2/bin/python     7548MiB |\n",
      "|    1   N/A  N/A      1633      G   /usr/lib/xorg/Xorg                           23MiB |\n",
      "|    1   N/A  N/A   2369178      C   ...4004/anaconda3/envs/itv2/bin/python     7544MiB |\n",
      "|    1   N/A  N/A   2444711      C   python3                                     272MiB |\n",
      "|    2   N/A  N/A      1633      G   /usr/lib/xorg/Xorg                           23MiB |\n",
      "|    2   N/A  N/A   1962094      C   .../miniconda/envs/kanishka/bin/python    22710MiB |\n",
      "|    2   N/A  N/A   2369178      C   ...4004/anaconda3/envs/itv2/bin/python     1314MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear all variables with confirmation\n",
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
