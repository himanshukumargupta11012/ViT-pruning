{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/ai21btech11011/miniforge3/envs/himanshu_mp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 25574/25574 [00:21<00:00, 1173.13 examples/s]\n",
      "Generating validation split: 100%|██████████| 2000/2000 [00:01<00:00, 1063.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load feature extractor and pretrained model\n",
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"1aurent/ADE20K\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'segmentation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39marray(\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msegmentation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'segmentation'"
     ]
    }
   ],
   "source": [
    "print(np.array(ds['train'][10][\"segmentation\"]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the image\n",
    "image = Image.open(\"path_to_your_image.jpg\").convert(\"RGB\")\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Check device and move tensors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2298)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "b = torch.randn(5, 1, 3)\n",
    "\n",
    "a = b.repeat(1, 4, 1)\n",
    "\n",
    "a = [0.0000, 0.0000, 0.0000, 0.0000, 0.0011, 0.1384, 0.4428, 0.6498, 0.7703,\n",
    "        0.6592, 0.0965, 0.0000]\n",
    "print(torch.tensor(a).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6174)\n",
      "tensor(0.6174)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()  # pos_weight=None (default)\n",
    "# logits = torch.tensor([0.2, -1.5, 2.0])  # Model outputs\n",
    "logits = torch.tensor([0.2, .4, .9])  # Model outputs\n",
    "targets = torch.tensor([1.0, 0.0, 1.0])  # True labels\n",
    "\n",
    "loss = loss_fn(logits, targets)\n",
    "print(loss)  # Standard BCE loss\n",
    "\n",
    "print(nn.BCELoss()(nn.Sigmoid()(logits), targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 196, 8, 768]) torch.Size([64, 196])\n",
      "torch.Size([64, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "offsets = torch.tensor([-1, 1, -15, -13, -14, 13, 14, 15])\n",
    "\n",
    "arr = torch.randn(64, 196, 768)\n",
    "\n",
    "bool_vector = (torch.rand(64, 196) > 0.5)\n",
    "\n",
    "\n",
    "# Compute the indices with bounds checking\n",
    "indices = torch.clamp(torch.arange(196).unsqueeze(1) + offsets, 0, 195)\n",
    "\n",
    "arr2 = arr[:, indices, :]\n",
    "\n",
    "print(arr2.shape, bool_vector.shape)\n",
    "\n",
    "masked_values = arr2 * bool_vector.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "masked_values = masked_values.mean(dim=2)\n",
    "\n",
    "print(masked_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m arr2 \u001b[38;5;241m=\u001b[39m arr2\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m768\u001b[39m)  \u001b[38;5;66;03m# Shape: (64, 196, 8, 768)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Gather values from arr using arr2 indices\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m arr3 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr2\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: (64, 196, 8, 768)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(arr3\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Given tensors\n",
    "arr = torch.rand(64, 196, 768)  # Shape: (64, 196, 768)\n",
    "arr = arr.expand\n",
    "arr2 = torch.randint(0, 196, (196, 8))  # Shape: (196, 8)\n",
    "\n",
    "# Expand arr2 to match batch size\n",
    "arr2 = arr2.unsqueeze(0).expand(64, -1, -1)  # Shape: (64, 196, 8)\n",
    "\n",
    "# Ensure arr2 has the same number of dimensions as arr\n",
    "arr2 = arr2.unsqueeze(-1).expand(-1, -1, -1, 768)  # Shape: (64, 196, 8, 768)\n",
    "\n",
    "# Gather values from arr using arr2 indices\n",
    "arr3 = torch.gather(arr, 1, arr2)  # Shape: (64, 196, 8, 768)\n",
    "\n",
    "print(arr3.shape)  # Output: torch.Size([64, 196, 8, 768])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dream11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
