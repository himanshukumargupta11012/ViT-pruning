{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/ai21btech11011/miniforge3/envs/himanshu_mp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.vit.modeling_vit import ViTModel, ViTLayer, ViTEncoder, ViTConfig\n",
    "from torch import nn\n",
    "import torch\n",
    "from typing import Optional\n",
    "from transformers import ViTForImageClassification\n",
    "import torch.nn.functional as F\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from ptflops import get_model_complexity_info\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for CIFAR-100 that integrates with AutoImageProcessor for processing\n",
    "class CIFAR100Dataset(Dataset):\n",
    "    def __init__(self, root, train=True, size=None, processor=None):\n",
    "        self.dataset = torchvision.datasets.CIFAR100(root=root, train=train, download=True)           # Load CIFAR-100 dataset from torchvision\n",
    "        if size is not None:\n",
    "            self.dataset = Subset(self.dataset, torch.arange(size))                                  # Optionally, load only count number of samples\n",
    "        \n",
    "        self.processor = processor                                                                    # Store the processor (used to preprocess images before passing to the model)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx] \n",
    "        \n",
    "        # Preprocess the image and convert it to tensor format suitable for the model\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        return inputs['pixel_values'].squeeze(0), label  \n",
    "    \n",
    "\n",
    "class ModifiedViTLayer(ViTLayer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.hidden_size = config.hidden_size\n",
    "        layer_sizes = [self.hidden_size, 64, 1]\n",
    "\n",
    "        mlp_layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            if i < len(mlp_layers) - 2:\n",
    "                mlp_layers.extend([nn.Linear(layer_sizes[i], layer_sizes[i + 1]), nn.ReLU()])\n",
    "            else:\n",
    "                mlp_layers.extend([nn.Linear(layer_sizes[i], layer_sizes[i + 1]), nn.Sigmoid()])                \n",
    "\n",
    "        self.mlp_layer = nn.Sequential(*mlp_layers)\n",
    "        self.threshold = 0.05\n",
    "        self.loss = 0\n",
    "        self.skip_ratio = 0\n",
    "\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        output_attentions: bool = False, compute_cosine = True):\n",
    "\n",
    "        mlp_output = self.mlp_layer(hidden_states[:, 1:])\n",
    "        boolean_mask = (mlp_output >= self.threshold).squeeze(-1)\n",
    "        cls_col = torch.ones((hidden_states.shape[0], 1), dtype=torch.bool).to(boolean_mask.device)\n",
    "        boolean_mask = torch.cat((cls_col, boolean_mask), dim=1)\n",
    "\n",
    "        output = hidden_states.clone()\n",
    "        batch_size = hidden_states.shape[0]\n",
    "        for i in range(batch_size):\n",
    "            output[i][boolean_mask[i]] = super().forward(hidden_states[i][boolean_mask[i]].unsqueeze(0))[0].squeeze(0)\n",
    "\n",
    "        if self.training or compute_cosine:\n",
    "            real_output = super().forward(hidden_states[:, 1:])[0]\n",
    "            cos_similarity = (F.cosine_similarity(real_output, hidden_states[:, 1:], dim=-1) + 1) / 2\n",
    "            \n",
    "            self.loss = nn.MSELoss()(cos_similarity, 1 - mlp_output.squeeze(-1))\n",
    "            self.mlp_accuracy_arr = ((1 - cos_similarity - self.threshold) * (mlp_output.squeeze(-1) - self.threshold) > 0)\n",
    "        else:\n",
    "            self.loss = 0\n",
    "            \n",
    "        self.skip_ratio = torch.sum(~boolean_mask)\n",
    "        return (output, )\n",
    "\n",
    "\n",
    "class ModifiedViTEncoder(ViTEncoder):\n",
    "    def __init__(self, config: ViTConfig):\n",
    "        super().__init__(config)\n",
    "        self.layer = nn.ModuleList([ModifiedViTLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "\n",
    "class ModifiedViTModel(ViTModel):\n",
    "    def __init__(self, config: ViTConfig):\n",
    "        super().__init__(config)\n",
    "        self.encoder = ModifiedViTEncoder(config)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, pixel_values, output_attentions=False, output_hidden_states=False, return_dict=True):\n",
    "        outputs = super().forward(pixel_values, output_attentions=output_attentions, output_hidden_states=output_hidden_states, return_dict=return_dict)\n",
    "        logits = self.classifier(outputs['last_hidden_state'][:, 0])\n",
    "        output = lambda: None\n",
    "        setattr(output, 'logits', logits)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "16.87 GMac\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Calculating Flops\n",
    "input_size = (3, 224, 224)\n",
    "pretrained_cifar_model = AutoModelForImageClassification.from_pretrained(\"Ahmed9275/Vit-Cifar100\")\n",
    "flops_original, params = get_model_complexity_info(pretrained_cifar_model, input_size, as_strings=True, print_per_layer_stat=False)\n",
    "print(flops_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model on the test data\n",
    "def test(model, dataloader, full_testing=False, progress=True):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    each_layer_skip = torch.zeros(len(model.encoder.layer))\n",
    "    if full_testing:\n",
    "        total_correct_mlp = torch.zeros(len(model.encoder.layer))\n",
    "        total_mlp_count = torch.zeros(len(model.encoder.layer))\n",
    "    data_size = len(dataloader.dataset)\n",
    "    with torch.no_grad():             # Disable gradient computation for efficiency\n",
    "        if progress:\n",
    "            dataloader = tqdm(dataloader, desc=f\"Testing\", leave=False)\n",
    "        for (inputs, labels) in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            if full_testing:\n",
    "                outputs = model(inputs)\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            predicted_class_indices = logits.argmax(dim=-1)    # Get the class index with the highest logit score\n",
    "\n",
    "            # Track the skip ratios from each layer\n",
    "            for i, vit_layer in enumerate(model.encoder.layer):\n",
    "                each_layer_skip[i] += vit_layer.skip_ratio.item()\n",
    "                if full_testing:\n",
    "                    total_correct_mlp[i] += (vit_layer.mlp_accuracy_arr).sum().item()\n",
    "                    total_mlp_count[i] += vit_layer.mlp_accuracy_arr.numel()\n",
    "            total_correct += torch.sum(predicted_class_indices == labels).item()\n",
    "\n",
    "    if full_testing:\n",
    "        print(f\"correct prediction ratio: {total_correct_mlp / total_mlp_count}\")\n",
    "    print(f\"Skip ratio: {each_layer_skip / data_size}\")\n",
    "    return total_correct / data_size\n",
    "\n",
    "\n",
    "\n",
    "# Training function to train the model over multiple epochs\n",
    "def train(model, train_loader, test_loader, num_epochs=10, loss_type='both', lr=1e-4, progress=True):\n",
    "    \n",
    "    results = {\"epoch\": [], \"train_loss\": [], \"test_accuracy\": []}\n",
    "    \n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    cosine_loss_ratio = 1.5\n",
    "\n",
    "    optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad], lr=lr)\n",
    "    temp_train_loader = train_loader\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Re-enable training mode at the start of each epoch\n",
    "        running_loss = 0.0\n",
    "        if progress:\n",
    "            train_loader = tqdm(train_loader, desc=f\"Epoch [{epoch + 1}/{num_epochs}]\", leave=False)\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)                \n",
    "\n",
    "            # Forward pass to get the classification logits, loss calculation\n",
    "            logits = model(inputs).logits\n",
    "\n",
    "            if loss_type in ['classification', 'both']:\n",
    "                classification_loss = criterion(logits, labels)\n",
    "\n",
    "            # Accumulate cosine similarity losses from each DHSLayer\n",
    "            if loss_type in ['cosine', 'both']:\n",
    "                cosine_loss = 0.0\n",
    "                for i, vit_layer in enumerate(model.encoder.layer):\n",
    "                    cosine_loss += vit_layer.loss\n",
    "            \n",
    "            # Calculate the total loss as a weighted sum of classification and cosine similarity losses\n",
    "            if loss_type == 'classification':\n",
    "                total_loss = classification_loss\n",
    "            elif loss_type == 'cosine':\n",
    "                total_loss = cosine_loss\n",
    "            elif loss_type == 'both':\n",
    "                total_loss = classification_loss + cosine_loss_ratio * cosine_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate running loss for this batch\n",
    "            running_loss += total_loss.item()\n",
    "\n",
    "            # Update the progress bar with current batch loss\n",
    "            if progress:\n",
    "                if (batch_idx + 1) % 5 == 0:\n",
    "                    train_loader.update(5)\n",
    "                train_loader.set_postfix({'batch_loss': total_loss.item()})\n",
    "\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        test_accuracy = test(model, test_loader, full_testing=True, progress=progress)\n",
    "        \n",
    "        results[\"epoch\"].append(epoch + 1)\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"test_accuracy\"].append(test_accuracy)\n",
    "        \n",
    "        # Print or log results for the current epoch\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Avg. Loss: {train_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define the training parameters\n",
    "load_pretrained = False            # Flag to load a pre-trained model\n",
    "data_path = 'data'\n",
    "train_size = 10000\n",
    "test_size = 1000\n",
    "pretrained_model_path = os.path.join('models', 'mlp_class_10000.pth')\n",
    "\n",
    "\n",
    "\n",
    "config = ViTConfig.from_pretrained(\"Ahmed9275/Vit-Cifar100\")\n",
    "config.num_labels = len(config.id2label)\n",
    "modified_vit_model = ModifiedViTModel(config).to(device)\n",
    "cifar_processor = AutoImageProcessor.from_pretrained(\"Ahmed9275/Vit-Cifar100\")   # Initialize the image processor for CIFAR-100 data pre-processing\n",
    "\n",
    "\n",
    "train_dataset = CIFAR100Dataset(root=data_path, train=True, size=train_size, processor=cifar_processor)\n",
    "test_dataset = CIFAR100Dataset(root=data_path, train=False, size=test_size, processor=cifar_processor)\n",
    "\n",
    "# Define the function to run training with given hyperparameters\n",
    "def run_experiment(batch_size, lr, num_epochs, loss_type):\n",
    "    # Print the hyperparameters\n",
    "    print(f\"\\n ---------- Running experiment with batch_size={batch_size}, lr={lr}, num_epochs={num_epochs}, loss_type={loss_type} ---------- \\n\")\n",
    "\n",
    "\n",
    "    # -----------   Copying Weights    ------------  #\n",
    "    if not load_pretrained:\n",
    "        new_state_dict = {}\n",
    "        for key in pretrained_cifar_model.state_dict().keys():\n",
    "            new_key = key.replace('vit.', '')\n",
    "            new_state_dict[new_key] = pretrained_cifar_model.state_dict()[key]\n",
    "\n",
    "        modified_vit_model.load_state_dict(new_state_dict, strict=False)\n",
    "    else:\n",
    "        modified_vit_model.load_state_dict(torch.load(pretrained_model_path))\n",
    "\n",
    "    modified_vit_model.to(device)\n",
    "    \n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Making grad true or false depending on training type\n",
    "    for param in modified_vit_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    if loss_type in ['cosine', 'both']:\n",
    "        for layer in modified_vit_model.encoder.layer:\n",
    "            for param in layer.mlp_layer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    if loss_type in ['classification', 'both']:\n",
    "        for param in modified_vit_model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "    # Training the model\n",
    "    results = train(modified_vit_model, train_loader, test_loader, num_epochs=num_epochs, loss_type=loss_type, lr=lr, progress=True)\n",
    "    \n",
    "    input_size = (3, 224, 224)\n",
    "    flops, params = get_model_complexity_info(modified_vit_model, input_size, as_strings=True, print_per_layer_stat=False)\n",
    "    \n",
    "    print(f\"FLOPS for this modified model: {flops}\")\n",
    "    # Extracting only the numerical part and converting to float\n",
    "    flops = float(flops.split()[0].replace(',', ''))\n",
    "    flops_original, params = get_model_complexity_info(pretrained_cifar_model, input_size, as_strings=True, print_per_layer_stat=False)\n",
    "    flops_original = float(flops_original.split()[0].replace(',', ''))\n",
    "    print(f\"Ratio of FLOPS: {flops / flops_original} \\n\")\n",
    "\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ---------- Running experiment with batch_size=64, lr=0.0001, num_epochs=15, loss_type=cosine ---------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9059, 0.8190, 0.5589, 0.2814, 0.1568, 0.2886,\n",
      "        0.3599, 0.8652, 0.8856])\n",
      "Skip ratio: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Epoch 1/15, Avg. Loss: 0.6363, Test Accuracy: 0.8990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9059, 0.8190, 0.5589, 0.2814, 0.1568, 0.2886,\n",
      "        0.3599, 0.8652, 0.8856])\n",
      "Skip ratio: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Epoch 2/15, Avg. Loss: 0.0892, Test Accuracy: 0.8990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9059, 0.8190, 0.5589, 0.2814, 0.1568, 0.2886,\n",
      "        0.3599, 0.8652, 0.8856])\n",
      "Skip ratio: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Epoch 3/15, Avg. Loss: 0.0365, Test Accuracy: 0.8990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9059, 0.8190, 0.5589, 0.2814, 0.1568, 0.2886,\n",
      "        0.3661, 0.8666, 0.8856])\n",
      "Skip ratio: tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        1.2690, 0.0000, 0.0000])\n",
      "Epoch 4/15, Avg. Loss: 0.0215, Test Accuracy: 0.8990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9059, 0.8190, 0.5589, 0.2832, 0.1568, 0.2886,\n",
      "        0.4770, 0.8811, 0.8862])\n",
      "Skip ratio: tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.4000,  0.0000,\n",
      "         0.0000, 26.3100,  0.0000,  0.0000])\n",
      "Epoch 5/15, Avg. Loss: 0.0155, Test Accuracy: 0.8980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9059, 0.8190, 0.5589, 0.3197, 0.1613, 0.2926,\n",
      "        0.5756, 0.8920, 0.8876])\n",
      "Skip ratio: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e-03,\n",
      "        1.2367e+01, 0.0000e+00, 0.0000e+00, 5.2036e+01, 0.0000e+00, 0.0000e+00])\n",
      "Epoch 6/15, Avg. Loss: 0.0125, Test Accuracy: 0.8970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9059, 0.8190, 0.5612, 0.3866, 0.1815, 0.3074,\n",
      "        0.6223, 0.8988, 0.8903])\n",
      "Skip ratio: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e-03, 3.0000e-02, 1.3860e+00,\n",
      "        4.0404e+01, 0.0000e+00, 0.0000e+00, 6.5504e+01, 0.0000e+00, 0.0000e+00])\n",
      "Epoch 7/15, Avg. Loss: 0.0107, Test Accuracy: 0.8960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9059, 0.8188, 0.5682, 0.4450, 0.2130, 0.3391,\n",
      "        0.7196, 0.8703, 0.8953])\n",
      "Skip ratio: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2000e-02, 2.3900e-01, 9.1400e+00,\n",
      "        6.6451e+01, 0.0000e+00, 3.1450e+00, 8.6348e+01, 0.0000e+00, 0.0000e+00])\n",
      "Epoch 8/15, Avg. Loss: 0.0096, Test Accuracy: 0.8840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9059, 0.8183, 0.5725, 0.4894, 0.2422, 0.5144,\n",
      "        0.6753, 0.8571, 0.9057])\n",
      "Skip ratio: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3000e-02, 8.4700e-01, 2.1983e+01,\n",
      "        8.5884e+01, 0.0000e+00, 4.7056e+01, 8.9546e+01, 0.0000e+00, 3.0000e-03])\n",
      "Epoch 9/15, Avg. Loss: 0.0087, Test Accuracy: 0.8610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9059, 0.8176, 0.5743, 0.5172, 0.2669, 0.5851,\n",
      "        0.6649, 0.8710, 0.9113])\n",
      "Skip ratio: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0700e-01, 1.8570e+00, 3.5594e+01,\n",
      "        9.9949e+01, 0.0000e+00, 7.6418e+01, 8.7756e+01, 0.0000e+00, 8.0000e-02])\n",
      "Epoch 10/15, Avg. Loss: 0.0081, Test Accuracy: 0.8560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9059, 0.8168, 0.5736, 0.5368, 0.2888, 0.6221,\n",
      "        0.6630, 0.8785, 0.9164])\n",
      "Skip ratio: tensor([  0.0000,   0.0000,   0.0000,   0.2170,   3.0580,  47.0490, 111.4190,\n",
      "          0.2470,  97.3050,  87.5770,   0.0000,   0.1490])\n",
      "Epoch 11/15, Avg. Loss: 0.0077, Test Accuracy: 0.8490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9057, 0.8166, 0.5731, 0.6174, 0.3099, 0.6177,\n",
      "        0.6552, 0.8804, 0.9253])\n",
      "Skip ratio: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 3.7000e-01, 4.2680e+00, 5.5945e+01,\n",
      "        1.3399e+02, 2.2937e+01, 1.0407e+02, 8.8994e+01, 0.0000e+00, 1.3200e-01])\n",
      "Epoch 12/15, Avg. Loss: 0.0075, Test Accuracy: 0.8410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9056, 0.8162, 0.5727, 0.6261, 0.4526, 0.5950,\n",
      "        0.6724, 0.8899, 0.9274])\n",
      "Skip ratio: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 5.4000e-01, 5.4100e+00, 6.2319e+01,\n",
      "        1.4118e+02, 6.2332e+01, 9.7825e+01, 9.9299e+01, 0.0000e+00, 8.2000e-02])\n",
      "Epoch 13/15, Avg. Loss: 0.0072, Test Accuracy: 0.8230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9055, 0.8165, 0.5740, 0.6310, 0.5556, 0.5958,\n",
      "        0.6823, 0.8964, 0.9309])\n",
      "Skip ratio: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 6.9800e-01, 6.2930e+00, 6.6715e+01,\n",
      "        1.4648e+02, 9.8219e+01, 9.9696e+01, 1.0688e+02, 0.0000e+00, 7.6000e-02])\n",
      "Epoch 14/15, Avg. Loss: 0.0070, Test Accuracy: 0.8080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9056, 0.8167, 0.5762, 0.6357, 0.6698, 0.6712,\n",
      "        0.6932, 0.8988, 0.9334])\n",
      "Skip ratio: tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 8.6100e-01, 6.8980e+00, 6.9824e+01,\n",
      "        1.5041e+02, 1.3400e+02, 1.2910e+02, 1.2110e+02, 0.0000e+00, 9.0000e-03])\n",
      "Epoch 15/15, Avg. Loss: 0.0067, Test Accuracy: 0.7460\n",
      "FLOPS for this modified model: 31.21 GMac\n",
      "Ratio of FLOPS: 1.850029638411381 \n",
      "\n",
      "\n",
      " ---------- Running experiment with batch_size=64, lr=0.0002, num_epochs=15, loss_type=both ---------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9076, 0.8321, 0.7177, 0.7519, 0.8394, 0.7820,\n",
      "        0.7433, 0.9859, 0.9897])\n",
      "Skip ratio: tensor([  0.0000,   0.0000,   0.0000,   1.3790,   6.1220,  70.2960, 163.5840,\n",
      "        182.2990, 158.6030, 146.4890,   3.9790,   2.2120])\n",
      "Epoch 1/15, Avg. Loss: 1.2502, Test Accuracy: 0.6890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9092, 0.8409, 0.7387, 0.7580, 0.8375, 0.7953,\n",
      "        0.7577, 0.9886, 0.9866])\n",
      "Skip ratio: tensor([  0.0000,   0.0000,   0.0000,   2.7510,  10.5870,  70.1280, 165.2120,\n",
      "        182.2020, 153.0800, 146.4540,   4.6780,   1.2580])\n",
      "Epoch 2/15, Avg. Loss: 1.1366, Test Accuracy: 0.6990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9098, 0.8438, 0.7428, 0.7776, 0.8429, 0.8020,\n",
      "        0.7693, 0.9889, 0.9899])\n",
      "Skip ratio: tensor([  0.0000,   0.0000,   0.0000,   2.5530,  10.6050,  57.2540, 167.7260,\n",
      "        182.4640, 147.8280, 144.7570,   5.5060,   2.1080])\n",
      "Epoch 3/15, Avg. Loss: 0.8694, Test Accuracy: 0.7400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9106, 0.8469, 0.7508, 0.7805, 0.8438, 0.8077,\n",
      "        0.7694, 0.9895, 0.9917])\n",
      "Skip ratio: tensor([  0.0000,   0.0000,   0.0000,   3.6800,  12.5280,  59.1570, 167.9240,\n",
      "        180.8070, 148.7640, 137.8170,   5.9080,   2.4930])\n",
      "Epoch 4/15, Avg. Loss: 0.7369, Test Accuracy: 0.7410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9111, 0.8492, 0.7565, 0.7858, 0.8487, 0.8119,\n",
      "        0.7763, 0.9902, 0.9920])\n",
      "Skip ratio: tensor([  0.0000,   0.0000,   0.0000,   4.2750,  12.5970,  59.0700, 166.9460,\n",
      "        180.8230, 148.2560, 142.0350,   5.9690,   2.5440])\n",
      "Epoch 5/15, Avg. Loss: 0.6632, Test Accuracy: 0.7350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9112, 0.8478, 0.7583, 0.7889, 0.8530, 0.8159,\n",
      "        0.7815, 0.9888, 0.9930])\n",
      "Skip ratio: tensor([  0.0000,   0.0000,   0.0000,   3.8430,  10.4110,  60.5050, 165.6550,\n",
      "        180.0280, 146.7090, 140.2260,   6.1730,   2.8430])\n",
      "Epoch 6/15, Avg. Loss: 0.6112, Test Accuracy: 0.7420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9114, 0.8515, 0.7641, 0.7887, 0.8543, 0.8206,\n",
      "        0.7833, 0.9884, 0.9907])\n",
      "Skip ratio: tensor([  0.0000,   0.0000,   0.0000,   4.0400,  14.1280,  57.9800, 163.1270,\n",
      "        180.3350, 148.9680, 139.0230,   6.1250,   2.4160])\n",
      "Epoch 7/15, Avg. Loss: 0.5746, Test Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9117, 0.8520, 0.7676, 0.7898, 0.8539, 0.8255,\n",
      "        0.7847, 0.9889, 0.9933])\n",
      "Skip ratio: tensor([  0.0000,   0.0000,   0.0000,   5.4310,  12.1450,  61.4100, 164.1440,\n",
      "        178.1730, 152.4220, 140.9240,   6.3010,   3.5320])\n",
      "Epoch 8/15, Avg. Loss: 0.5474, Test Accuracy: 0.7390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9124, 0.8535, 0.7669, 0.7938, 0.8566, 0.8283,\n",
      "        0.7882, 0.9890, 0.9933])\n",
      "Skip ratio: tensor([  0.0000,   0.0000,   0.0000,   4.7930,  14.5870,  55.9680, 164.5530,\n",
      "        178.2900, 152.6080, 142.8370,   6.3710,   4.2960])\n",
      "Epoch 9/15, Avg. Loss: 0.5257, Test Accuracy: 0.7450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9122, 0.8542, 0.7714, 0.7978, 0.8529, 0.8273,\n",
      "        0.7873, 0.9886, 0.9931])\n",
      "Skip ratio: tensor([  0.0000,   0.0000,   0.0000,   5.4310,  13.8040,  56.5250, 168.4360,\n",
      "        178.3920, 151.5050, 143.0020,   6.5080,   4.0710])\n",
      "Epoch 10/15, Avg. Loss: 0.5047, Test Accuracy: 0.7390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9123, 0.8551, 0.7730, 0.7916, 0.8593, 0.8295,\n",
      "        0.7982, 0.9872, 0.9923])\n",
      "Skip ratio: tensor([  0.0000,   0.0000,   0.0000,   5.2270,  13.6580,  60.0800, 158.1110,\n",
      "        177.7230, 147.2300, 140.2590,   6.4380,   3.8690])\n",
      "Epoch 11/15, Avg. Loss: 0.4927, Test Accuracy: 0.7580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9125, 0.8559, 0.7757, 0.7950, 0.8563, 0.8311,\n",
      "        0.7929, 0.9859, 0.9931])\n",
      "Skip ratio: tensor([  0.0000,   0.0000,   0.0000,   5.5590,  17.6310,  54.2530, 163.5100,\n",
      "        177.0350, 149.6110, 136.3080,   6.3070,   4.1620])\n",
      "Epoch 12/15, Avg. Loss: 0.4692, Test Accuracy: 0.7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9126, 0.8567, 0.7794, 0.7985, 0.8546, 0.8348,\n",
      "        0.7973, 0.9854, 0.9927])\n",
      "Skip ratio: tensor([  0.0000,   0.0000,   0.0000,   5.9430,  15.9650,  60.6470, 163.0590,\n",
      "        173.5890, 151.0300, 138.9710,   6.4560,   4.2980])\n",
      "Epoch 13/15, Avg. Loss: 0.4502, Test Accuracy: 0.7530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9127, 0.8565, 0.7770, 0.8027, 0.8591, 0.8365,\n",
      "        0.7970, 0.9866, 0.9928])\n",
      "Skip ratio: tensor([  0.0000,   0.0000,   0.0000,   5.2330,  15.4600,  56.3980, 163.2920,\n",
      "        176.6880, 155.0580, 142.7140,   6.6720,   4.8190])\n",
      "Epoch 14/15, Avg. Loss: 0.4394, Test Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction ratio: tensor([1.0000, 1.0000, 0.9998, 0.9129, 0.8569, 0.7794, 0.8009, 0.8566, 0.8384,\n",
      "        0.7975, 0.9855, 0.9924])\n",
      "Skip ratio: tensor([  0.0000,   0.0000,   0.0000,   5.4960,  16.2470,  57.6170, 163.0390,\n",
      "        173.1390, 154.7400, 135.6300,   6.7210,   4.4120])\n",
      "Epoch 15/15, Avg. Loss: 0.4281, Test Accuracy: 0.7460\n",
      "FLOPS for this modified model: 31.63 GMac\n",
      "Ratio of FLOPS: 1.874925903971547 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of hyperparameters combinations to try\n",
    "experiments = [\n",
    "    {\"batch_size\": 64, \"lr\": 1e-4, \"num_epochs\": 15, \"loss_type\": \"cosine\"},\n",
    "    {\"batch_size\": 64, \"lr\": 2e-4, \"num_epochs\": 15, \"loss_type\": \"both\"},\n",
    "    # Add more combinations here...\n",
    "]\n",
    "\n",
    "\n",
    "# Run all experiments and store the results in a DataFrame\n",
    "results_df = pd.DataFrame()\n",
    "for exp in experiments:\n",
    "    exp_results = run_experiment(**exp)\n",
    "    exp_df = pd.DataFrame(exp_results)\n",
    "    exp_df[\"batch_size\"] = exp[\"batch_size\"]\n",
    "    exp_df[\"lr\"] = exp[\"lr\"]\n",
    "    exp_df[\"num_epochs\"] = exp[\"num_epochs\"]\n",
    "    exp_df[\"loss_type\"] = exp[\"loss_type\"]\n",
    "    results_df = pd.concat([results_df, exp_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>loss_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.747893</td>\n",
       "      <td>0.899</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>15</td>\n",
       "      <td>cosine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  test_accuracy  batch_size      lr  num_epochs loss_type\n",
       "0      1    0.747893          0.899          64  0.0001          15    cosine"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "himanshu_mp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
