Text cell <3F4oWsmH2oaM>
# %% [markdown]
# Natural Language Processing
## Assignment - 2
### Group Members
1. Donal Loitam (AI21BTECH11009)
2. Sai Pradeep (AI21BTECH11013)
3. Suraj Kumar (AI21BTECH11029)

Code cell <BbJHGAMoTWA1>
# %% [code]
!pip install pandas numpy matplotlib seaborn nltk scikit-learn torch

Execution output from Mar 1, 2025 4:15 PM
11KB
	Stream
		Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)
		Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)
		Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)
		Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)
		Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)
		Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)
		Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)
		Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)
		Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)
		Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)
		Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)
		Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)
		Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)
		Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)
		Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)
		Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)
		Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)
		Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)
		Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)
		Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)
		Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)
		Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)
		Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)
		Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)
		Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)
		Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)
		Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)
		Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)
		Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)
		  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
		Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)
		  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
		Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)
		  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
		Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)
		  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
		Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)
		  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
		Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)
		  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
		Collecting nvidia-curand-cu12==10.3.5.147 (from torch)
		  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
		Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)
		  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
		Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)
		  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
		Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)
		Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)
		Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)
		  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
		Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)
		Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)
		Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)
		Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)
		Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)
		Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)
		[2K   [90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m363.4/363.4 MB[0m [31m3.8 MB/s[0m eta [36m0:00:00[0m
		[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)
		[2K   [90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m13.8/13.8 MB[0m [31m58.6 MB/s[0m eta [36m0:00:00[0m
		[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)
		[2K   [90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m24.6/24.6 MB[0m [31m14.6 MB/s[0m eta [36m0:00:00[0m
		[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)
		[2K   [90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m883.7/883.7 kB[0m [31m29.1 MB/s[0m eta [36m0:00:00[0m
		[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)
		[2K   [90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m664.8/664.8 MB[0m [31m2.6 MB/s[0m eta [36m0:00:00[0m
		[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)
		[2K   [90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m211.5/211.5 MB[0m [31m5.5 MB/s[0m eta [36m0:00:00[0m
		[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)
		[2K   [90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m56.3/56.3 MB[0m [31m12.0 MB/s[0m eta [36m0:00:00[0m
		[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)
		[2K   [90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m127.9/127.9 MB[0m [31m7.5 MB/s[0m eta [36m0:00:00[0m
		[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)
		[2K   [90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m207.5/207.5 MB[0m [31m5.8 MB/s[0m eta [36m0:00:00[0m
		[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)
		[2K   [90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m21.1/21.1 MB[0m [31m57.4 MB/s[0m eta [36m0:00:00[0m
		[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12
		  Attempting uninstall: nvidia-nvjitlink-cu12
		    Found existing installation: nvidia-nvjitlink-cu12 12.5.82
		    Uninstalling nvidia-nvjitlink-cu12-12.5.82:
		      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82
		  Attempting uninstall: nvidia-curand-cu12
		    Found existing installation: nvidia-curand-cu12 10.3.6.82
		    Uninstalling nvidia-curand-cu12-10.3.6.82:
		      Successfully uninstalled nvidia-curand-cu12-10.3.6.82
		  Attempting uninstall: nvidia-cufft-cu12
		    Found existing installation: nvidia-cufft-cu12 11.2.3.61
		    Uninstalling nvidia-cufft-cu12-11.2.3.61:
		      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61
		  Attempting uninstall: nvidia-cuda-runtime-cu12
		    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82
		    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:
		      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82
		  Attempting uninstall: nvidia-cuda-nvrtc-cu12
		    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82
		    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:
		      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82
		  Attempting uninstall: nvidia-cuda-cupti-cu12
		    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82
		    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:
		      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82
		  Attempting uninstall: nvidia-cublas-cu12
		    Found existing installation: nvidia-cublas-cu12 12.5.3.2
		    Uninstalling nvidia-cublas-cu12-12.5.3.2:
		      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2
		  Attempting uninstall: nvidia-cusparse-cu12
		    Found existing installation: nvidia-cusparse-cu12 12.5.1.3
		    Uninstalling nvidia-cusparse-cu12-12.5.1.3:
		      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3
		  Attempting uninstall: nvidia-cudnn-cu12
		    Found existing installation: nvidia-cudnn-cu12 9.3.0.75
		    Uninstalling nvidia-cudnn-cu12-9.3.0.75:
		      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75
		  Attempting uninstall: nvidia-cusolver-cu12
		    Found existing installation: nvidia-cusolver-cu12 11.6.3.83
		    Uninstalling nvidia-cusolver-cu12-11.6.3.83:
		      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83
		Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127

Code cell <J7j-fJ4J2ZQu>
# %% [code]
# Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer


import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence


Code cell <xnI7lZU6-a8w>
# %% [code]
# Download stopwords if not already available
nltk.download('stopwords')
Execution output from Mar 1, 2025 12:02 PM
0KB
	Stream
		[nltk_data] Downloading package stopwords to /root/nltk_data...
		[nltk_data]   Unzipping corpora/stopwords.zip.
	text/plain
		True

Text cell <pRhl03yETWA3>
# %% [markdown]
# DATASET 1

Text cell <tHuVyfKl4WoQ>
# %% [markdown]
## Data Understanding

Code cell <lKOwHt20TWA4>
# %% [code]
!pip install openpyxl
Execution output
0KB
	Stream
		Requirement already satisfied: openpyxl in /home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages (3.1.5)
		Requirement already satisfied: et-xmlfile in /home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages (from openpyxl) (2.0.0)

Code cell <yFQZVxRE4cNZ>
# %% [code]
file_path = 'sample_data/Dataset-1.xlsx'
df = pd.read_excel(file_path)
df.head()
Execution output from Mar 1, 2025 12:02 PM
17KB
	text/plain
		ID                                              TITLE  \
		0   1        Reconstructing Subject-Specific Effect Maps   
		1   2                 Rotation Invariance Neural Network   
		2   3  Spherical polyharmonics and Poisson kernels fo...   
		3   4  A finite element approximation for the stochas...   
		4   5  Comparative study of Discrete Wavelet Transfor...   
		
		                                            ABSTRACT  Computer Science  \
		0    Predictive models allow subject-specific inf...                 1   
		1    Rotation invariance and translation invarian...                 1   
		2    We introduce and develop the notion of spher...                 0   
		3    The stochastic Landau--Lifshitz--Gilbert (LL...                 0   
		4    Fourier-transform infra-red (FTIR) spectra o...                 1   
		
		   Physics  Mathematics  Statistics  Quantitative Biology  \
		0        0            0           0                     0   
		1        0            0           0                     0   
		2        0            1           0                     0   
		3        0            1           0                     0   
		4        0            0           1                     0   
		
		   Quantitative Finance  
		0                     0  
		1                     0  
		2                     0  
		3                     0  
		4                     0

Code cell <2pHo436BrJae>
# %% [code]
print(df.columns)
Execution output
0KB
	Stream
		Index(['ID', 'TITLE', 'ABSTRACT', 'Computer Science', 'Physics', 'Mathematics',
		       'Statistics', 'Quantitative Biology', 'Quantitative Finance'],
		      dtype='object')

Code cell <1yYHe6GF0Nxi>
# %% [code]
# Check for missing values,  class imbalance, and other issues.
print(df.info())
Execution output
1KB
	Stream
		<class 'pandas.core.frame.DataFrame'>
		RangeIndex: 20972 entries, 0 to 20971
		Data columns (total 9 columns):
		 #   Column                Non-Null Count  Dtype 
		---  ------                --------------  ----- 
		 0   ID                    20972 non-null  int64 
		 1   TITLE                 20972 non-null  object
		 2   ABSTRACT              20972 non-null  object
		 3   Computer Science      20972 non-null  int64 
		 4   Physics               20972 non-null  int64 
		 5   Mathematics           20972 non-null  int64 
		 6   Statistics            20972 non-null  int64 
		 7   Quantitative Biology  20972 non-null  int64 
		 8   Quantitative Finance  20972 non-null  int64 
		dtypes: int64(7), object(2)
		memory usage: 1.4+ MB
		None

Text cell <51SRBNwt4co6>
# %% [markdown]
## Data Preprocessing
1. Define a function for processing text
2. Create labels and convert labels into a binary multi-label format
3. Split data into train-test sets

Code cell <jUo2cPJQ4ee9>
# %% [code]
def process_text(text):
    if pd.isna(text):
        return ''
    text = text.lower()
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)  # Remove special characters

    # Remove stop words
    stop_words = set(stopwords.words('english'))
    words = text.split()
    words = [word for word in words if word not in stop_words]

    new_processed_text = ' '.join(words)
    return new_processed_text

Code cell <coo5KpJvrGvp>
# %% [code]
# Apply this function to title and abstract columns of the df
df['processed_title'] = df['TITLE'].apply(process_text)
df['processed_abstract'] = df['ABSTRACT'].apply(process_text)

# Merge these columns into one
df['processed_text'] = df['processed_title'] + ' ' + df['processed_abstract']
df.head()
Execution output from Mar 1, 2025 12:02 PM
23KB
	text/plain
		ID                                              TITLE  \
		0   1        Reconstructing Subject-Specific Effect Maps   
		1   2                 Rotation Invariance Neural Network   
		2   3  Spherical polyharmonics and Poisson kernels fo...   
		3   4  A finite element approximation for the stochas...   
		4   5  Comparative study of Discrete Wavelet Transfor...   
		
		                                            ABSTRACT  Computer Science  \
		0    Predictive models allow subject-specific inf...                 1   
		1    Rotation invariance and translation invarian...                 1   
		2    We introduce and develop the notion of spher...                 0   
		3    The stochastic Landau--Lifshitz--Gilbert (LL...                 0   
		4    Fourier-transform infra-red (FTIR) spectra o...                 1   
		
		   Physics  Mathematics  Statistics  Quantitative Biology  \
		0        0            0           0                     0   
		1        0            0           0                     0   
		2        0            1           0                     0   
		3        0            1           0                     0   
		4        0            0           1                     0   
		
		   Quantitative Finance                                    processed_title  \
		0                     0         reconstructing subjectspecific effect maps   
		1                     0                 rotation invariance neural network   
		2                     0  spherical polyharmonics poisson kernels polyha...   
		3                     0  finite element approximation stochastic maxwel...   
		4                     0  comparative study discrete wavelet transforms ...   
		
		                                  processed_abstract  \
		0  predictive models allow subjectspecific infere...   
		1  rotation invariance translation invariance gre...   
		2  introduce develop notion spherical polyharmoni...   
		3  stochastic landaulifshitzgilbert llg equation ...   
		4  fouriertransform infrared ftir spectra samples...   
		
		                                      processed_text  
		0  reconstructing subjectspecific effect maps pre...  
		1  rotation invariance neural network rotation in...  
		2  spherical polyharmonics poisson kernels polyha...  
		3  finite element approximation stochastic maxwel...  
		4  comparative study discrete wavelet transforms ...

Code cell <9X6zcEOirHZv>
# %% [code]
# Columns 3 to 8 are labels, so extract these column names and store it in a list
label_columns = df.columns[3:9]
print(label_columns)

# Prepare Labels (Multi-label classification)
df["labels"] = df[label_columns].apply(lambda x: list(np.where(x == 1)[0]), axis=1)

# Convert multi-labels into binary format
mlb = MultiLabelBinarizer()
y = mlb.fit_transform(df["labels"])
Execution output from Mar 1, 2025 12:03 PM
0KB
	Stream
		Index(['Computer Science', 'Physics', 'Mathematics', 'Statistics',
		       'Quantitative Biology', 'Quantitative Finance'],
		      dtype='object')

Code cell <uAPC_la90bKM>
# %% [code]
# Check for class imbalance
label_counts = df[label_columns].sum()
print(label_counts)
Execution output from Mar 1, 2025 12:03 PM
0KB
	Stream
		Computer Science        8594
		Physics                 6013
		Mathematics             5618
		Statistics              5206
		Quantitative Biology     587
		Quantitative Finance     249
		dtype: int64

Code cell <RLK3cKkXzRDz>
# %% [code]
df
Execution output
12KB
	text/plain
		ID                                              TITLE  \
		0          1        Reconstructing Subject-Specific Effect Maps   
		1          2                 Rotation Invariance Neural Network   
		2          3  Spherical polyharmonics and Poisson kernels fo...   
		3          4  A finite element approximation for the stochas...   
		4          5  Comparative study of Discrete Wavelet Transfor...   
		...      ...                                                ...   
		20967  20968  Contemporary machine learning: a guide for pra...   
		20968  20969  Uniform diamond coatings on WC-Co hard alloy c...   
		20969  20970  Analysing Soccer Games with Clustering and Con...   
		20970  20971  On the Efficient Simulation of the Left-Tail o...   
		20971  20972   Why optional stopping is a problem for Bayesians   
		
		                                                ABSTRACT  Computer Science  \
		0        Predictive models allow subject-specific inf...                 1   
		1        Rotation invariance and translation invarian...                 1   
		2        We introduce and develop the notion of spher...                 0   
		3        The stochastic Landau--Lifshitz--Gilbert (LL...                 0   
		4        Fourier-transform infra-red (FTIR) spectra o...                 1   
		...                                                  ...               ...   
		20967    Machine learning is finding increasingly bro...                 1   
		20968    Polycrystalline diamond coatings have been g...                 0   
		20969    We present a new approach for identifying si...                 1   
		20970    The sum of Log-normal variates is encountere...                 0   
		20971    Recently, optional stopping has been a subje...                 0   
		
		       Physics  Mathematics  Statistics  Quantitative Biology  \
		0            0            0           0                     0   
		1            0            0           0                     0   
		2            0            1           0                     0   
		3            0            1           0                     0   
		4            0            0           1                     0   
		...        ...          ...         ...                   ...   
		20967        1            0           0                     0   
		20968        1            0           0                     0   
		20969        0            0           0                     0   
		20970        0            1           1                     0   
		20971        0            1           1                     0   
		
		       Quantitative Finance  \
		0                         0   
		1                         0   
		2                         0   
		3                         0   
		4                         0   
		...                     ...   
		20967                     0   
		20968                     0   
		20969                     0   
		20970                     0   
		20971                     0   
		
		                                         processed_title  \
		0             reconstructing subjectspecific effect maps   
		1                     rotation invariance neural network   
		2      spherical polyharmonics poisson kernels polyha...   
		3      finite element approximation stochastic maxwel...   
		4      comparative study discrete wavelet transforms ...   
		...                                                  ...   
		20967  contemporary machine learning guide practition...   
		20968  uniform diamond coatings wcco hard alloy cutti...   
		20969       analysing soccer games clustering conceptors   
		20970  efficient simulation lefttail sum correlated l...   
		20971                optional stopping problem bayesians   
		
		                                      processed_abstract  \
		0      predictive models allow subjectspecific infere...   
		1      rotation invariance translation invariance gre...   
		2      introduce develop notion spherical polyharmoni...   
		3      stochastic landaulifshitzgilbert llg equation ...   
		4      fouriertransform infrared ftir spectra samples...   
		...                                                  ...   
		20967  machine learning finding increasingly broad ap...   
		20968  polycrystalline diamond coatings grown cemente...   
		20969  present new approach identifying situations be...   
		20970  sum lognormal variates encountered many challe...   
		20971  recently optional stopping subject debate baye...   
		
		                                          processed_text  labels  
		0      reconstructing subjectspecific effect maps pre...     [0]  
		1      rotation invariance neural network rotation in...     [0]  
		2      spherical polyharmonics poisson kernels polyha...     [2]  
		3      finite element approximation stochastic maxwel...     [2]  
		4      comparative study discrete wavelet transforms ...  [0, 3]  
		...                                                  ...     ...  
		20967  contemporary machine learning guide practition...  [0, 1]  
		20968  uniform diamond coatings wcco hard alloy cutti...     [1]  
		20969  analysing soccer games clustering conceptors p...     [0]  
		20970  efficient simulation lefttail sum correlated l...  [2, 3]  
		20971  optional stopping problem bayesians recently o...  [2, 3]  
		
		[20972 rows x 13 columns]

Code cell <xUQOqKXRzai0>
# %% [code]
# Split into Train and Test sets
X_train, X_test, y_train, y_test = train_test_split(df["processed_text"], y, test_size=0.2, random_state=42)

# Print the train and test set shape
print("Train set shape:", X_train.shape)
print("Test set shape:", X_test.shape)

# Display a sample processed data
print(f"Sample Text: {X_train.iloc[0]}")
print(f"Labels: {y_train[0]}")
Execution output from Mar 1, 2025 12:03 PM
1KB
	Stream
		Train set shape: (16777,)
		Test set shape: (4195,)
		Sample Text: reverse quantum annealing approach portfolio optimization problems investigate hybrid quantumclassical solution method meanvariance portfolio optimization problems starting real financial data statistics following principles modern portfolio theory generate parametrized samples portfolio optimization problems related quadratic binary optimization forms programmable analog dwave quantum annealer 2000q instances also solvable industryestablished genetic algorithm approach use classical benchmark investigate several options run quantum computation optimally ultimately discovering best results terms expected timetosolution function number variables hardest instances set obtained seeding quantum annealer solution candidate found greedy local search performing reverse annealing protocol optimized reverse annealing protocol found 100 times faster corresponding forward quantum annealing average
		Labels: [0 0 0 0 0 1]

Text cell <FJTlus7_4fE->
# %% [markdown]
## Feature extraction/Representation
Convert text data (titles and abstracts) into numerical representations
1. Bag of Words (BoW)
2. TF-IDF (Term Frequency-Inverse Document Frequency)
3. Word Embeddings (Word2Vec, GloVe)

- We will be using **TF-IDF** since it is known to work well with text classification using *Classical ML models*.
- Among word embeddings, we would experiment with **Word2Vec** and **GloVe** for *Deep Learning models*.

Code cell <18pYvIXa2nPN>
# %% [code]
!pip install gensim
!pip install scikit-learn
Execution output
1KB
	Stream
		Requirement already satisfied: gensim in /home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages (4.3.3)
		Requirement already satisfied: numpy<2.0,>=1.18.5 in /home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages (from gensim) (1.26.4)
		Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages (from gensim) (1.13.1)
		Requirement already satisfied: smart-open>=1.8.1 in /home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages (from gensim) (7.1.0)
		Requirement already satisfied: wrapt in /home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim) (1.17.2)
		Requirement already satisfied: scikit-learn in /home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages (1.6.1)
		Requirement already satisfied: numpy>=1.19.5 in /home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages (from scikit-learn) (1.26.4)
		Requirement already satisfied: scipy>=1.6.0 in /home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages (from scikit-learn) (1.13.1)
		Requirement already satisfied: joblib>=1.2.0 in /home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages (from scikit-learn) (1.4.2)
		Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages (from scikit-learn) (3.5.0)

Code cell <IOHqwB5XE9fd>
# %% [code]
# 1. Use BoW representation
from sklearn.feature_extraction.text import CountVectorizer

bow_vectorizer = CountVectorizer(max_features=5000)  # Keep only top 5000 words

X_train_bow = bow_vectorizer.fit_transform(X_train)
X_test_bow = bow_vectorizer.transform(X_test)

print("BoW representation shape:", X_train_bow.shape)
Execution output
0KB
	Stream
		BoW representation shape: (16777, 5000)

Code cell <9oSHE6dW0wLM>
# %% [code]
# 2. Use tf-idf vectoriser
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer(max_features=5000)    # Use 5000 features
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

print("TF-IDF representation shape:", X_train_tfidf.shape)
Execution output
0KB
	Stream
		TF-IDF representation shape: (16777, 5000)

Code cell <FlQNt3vJ2iqm>
# %% [code]
# 3. Use Word2Vec and train it from scratch
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess

# Tokenize the text data
tokenized_train_texts = [text.split() for text in X_train]
word2vec_model = Word2Vec(
    sentences=tokenized_train_texts,
    vector_size=100,            # what dimensions should the vector be of
    window=5,                   # Context window size
    min_count=2,
    sg=1                         # Use skip-gram (sg=1)
)


# Function to get average word2Vec
def get_average_word2vec(text, model, vec_size=100):
    words = text.split()
    vectorized_words = [model.wv[word] for word in words if word in model.wv]
    if not vectorized_words:
        return np.zeros(vec_size)
    avg_vector = np.mean(vectorized_words, axis=0)
    return avg_vector


X_train_w2v = np.array([get_average_word2vec(text, word2vec_model, 100) for text in X_train])
X_test_w2v = np.array([get_average_word2vec(text, word2vec_model, 100) for text in X_test])

print("Word2Vec representation shape:", X_train_w2v.shape)
Execution output from Mar 1, 2025 11:21 AM
0KB
	Stream
		Word2Vec representation shape: (16777, 100)

Code cell <CJIsHUWKEjL9>
# %% [code]
# FastText to be added soon

Code cell <5A3YldKwVrae>
# %% [code]


Text cell <VF3mUbAFVrpT>
# %% [markdown]
# MODELS

Text cell <ImiZmMggVeq9>
# %% [markdown]
# Classical
## 1. TFIDF + Classifier Chain model using Multinomial Naive Bayes
## 2. BOW + BinaryRelevanceSVM


Code cell <id5tu-N3MtlC>
# %% [code]
!pip install scikit-multilearn
!pip install --upgrade scikit-learn
!pip install --upgrade scikit-multilearn
!pip install scikit-learn
Execution output from Mar 1, 2025 12:03 PM
2KB
	Stream
		Collecting scikit-multilearn
		  Downloading scikit_multilearn-0.2.0-py3-none-any.whl.metadata (6.0 kB)
		Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)
		[?25l   [90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m0.0/89.4 kB[0m [31m?[0m eta [36m-:--:--[0m
[2K   [90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m [32m89.4/89.4 kB[0m [31m7.2 MB/s[0m eta [36m0:00:00[0m
		[?25hInstalling collected packages: scikit-multilearn
		Successfully installed scikit-multilearn-0.2.0
		Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)
		Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)
		Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)
		Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)
		Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)
		Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.11/dist-packages (0.2.0)
		Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)
		Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)
		Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)
		Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)
		Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)

Code cell <DgRd7Y_uEjiP>
# %% [code]
import sklearn.metrics
from skmultilearn.problem_transform import ClassifierChain
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import multilabel_confusion_matrix

from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix
def TestMultiLabel(y_true, y_pred, embedding_model,model_name):
    print("--------------")
    print(f"Results of {model_name} with {embedding_model} embeddings")

    exact_match = sklearn.metrics.accuracy_score(y_true, y_pred)
    print(f'Exact Match Ratio: {exact_match:.4f}')

    hamming_loss = sklearn.metrics.hamming_loss(y_true, y_pred)
    print(f'Hamming Loss: {hamming_loss:.4f}')

    recall = sklearn.metrics.recall_score(y_true, y_pred, average='samples')
    print(f'Recall: {recall:.4f}')

    precision = sklearn.metrics.precision_score(y_true, y_pred, average='samples')
    print(f'Precision: {precision:.4f}')

    f1_measure = sklearn.metrics.f1_score(y_true, y_pred, average='samples')
    print(f'F1 Score: {f1_measure:.4f}')

    cm = multilabel_confusion_matrix(y_true, y_pred)
    num_labels = cm.shape[0]

    # Create a plot with the confusion matrices for each label
    fig, axes = plt.subplots(1, num_labels, figsize=(25, 10))
    for i, ax in enumerate(axes):
        im = ax.imshow(cm[i], interpolation='nearest', cmap=plt.cm.Blues)

        # Add text annotations to the confusion matrix
        for j in range(2):
            for k in range(2):
                ax.text(k, j, f'{cm[i][j, k]}', ha='center', va='center', color='black')

        ax.set_title(f'Label {i}')
        ax.set_xticks([0, 1])
        ax.set_yticks([0, 1])
        ax.set_xlabel('Predicted')
        ax.set_ylabel('Actual')

    plt.subplots_adjust(right=0.85)
    # cbar_ax = fig.add_axes([0.88, 0.1, 0.02, 0.8])
    # fig.colorbar(im, cax=cbar_ax)

    plt.tight_layout()
    plt.show()
    print("--------------")

Code cell <EI3wfDJMdzdc>
# %% [code]
class ClassifierChainModel():
  def __init__(self,X_train,y_train):
    self.model = ClassifierChain(classifier=MultinomialNB())
    self.X_train = X_train
    self.y_train = y_train

  def fit(self):
    self.model.fit(self.X_train, self.y_train)

  def predict(self, X_test):
    out = self.model.predict(X_test)
    y_output=[]
    for i in range(0,out.shape[0]):
      y_output.append(out[i].toarray()[0])
      self.y_output=y_output
    return y_output

  def test(self, y_test):
    TestMultiLabel(y_test, self.y_output)


class BinaryRelenvanceSVM():
  def __init__(self,X_train,y_train):

    self.model = OneVsRestClassifier(SVC(kernel='linear'))
    self.X_train = X_train
    self.y_train = y_train

  def fit(self):
    self.model.fit(X_train_tfidf, y_train)

  def predict(self, X_test):
    self.y_output=self.model.predict(X_test_tfidf)
    return self.y_output

  def test(self,y_test):
    TestMultiLabel(y_test,self.y_output)


class BinaryRelenvanceRandomForest():
  def __init__(self, X_train, y_train):
    self.model = OneVsRestClassifier(RandomForestClassifier())
    self.X_train = X_train
    self.y_train = y_train

  def fit(self):
    self.model.fit(self.X_train, self.y_train)

  def predict(self, X_test):
    self.y_output = self.model.predict(X_test)
    return self.y_output

  def test(self, y_test):
    TestMultiLabel(y_test, self.y_output)


Text cell <6a6aFb-YTWA8>
# %% [markdown]
# Model 1 : Classifier Chain Multinomial Naive Bayes
### using tfidf embeddings

Code cell <lUqEJnkQXS-v>
# %% [code]
# TFIDF + ClassifierChain
tfidf_chain=ClassifierChainModel(X_train_tfidf, y_train)
tfidf_chain.fit()
y_tfidf_cc=tfidf_chain.predict(X_test_tfidf)
tfidf_chain.test(y_test)
Execution output
15KB
	Stream
		Exact Match Ratio: 0.6312
		Hamming Loss: 0.0847
		Recall (Samples): 0.8284
		Precision (Samples): 0.8043
		F1 Measure (Samples): 0.7974
		/home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
	text/plain
		<Figure size 1500x500 with 6 Axes>

Text cell <OcqP4caETWA8>
# %% [markdown]
# Model 2: BinaryRelanvenceSVM
### using BOW + BinaryRelanvenceSVM

Code cell <hm0fUBm-XYSL>
# %% [code]
# BOW + BinaryRelanvenceSVM
bow_svm=BinaryRelenvanceSVM(X_train_bow, y_train)
bow_svm.fit()
y_tfidf_cc=bow_svm.predict(X_test_bow)
bow_svm.test(y_test)
Execution output
15KB
	Stream
		Exact Match Ratio: 0.6491
		Hamming Loss: 0.0779
		Recall (Samples): 0.8095
		Precision (Samples): 0.8241
		F1 Measure (Samples): 0.7990
		/home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
	text/plain
		<Figure size 1500x500 with 6 Axes>

Text cell <cFZ-h_j1TWA8>
# %% [markdown]
# Model 2.1 : Random Forest
### tfidf embeddings with binary relevance Random Forest


Code cell <GGmsiLIwTWA8>
# %% [code]
# SVD + TFIDF + BinaryRelenvanceRandomForest
from sklearn.decomposition import TruncatedSVD
# n_components = 1000
# svd = TruncatedSVD(n_components=n_components)
# X_train_tfidf_svd = svd.fit_transform(X_train_tfidf)
# X_test_tfidf = svd.fit_transform(X_test_tfidf)

tfidf_rf=BinaryRelenvanceRandomForest(X_train_tfidf, y_train)
tfidf_rf.fit()
y_tfidf_cc=tfidf_rf.predict(X_test_tfidf)
tfidf_rf.test(y_test)

Execution output
15KB
	Stream
		Exact Match Ratio: 0.6167
		Hamming Loss: 0.0856
		Recall (Samples): 0.7740
		Precision (Samples): 0.7924
		F1 Measure (Samples): 0.7654
		/home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
	text/plain
		<Figure size 1500x500 with 6 Axes>

Code cell <l1G7CVCAXy3g>
# %% [code]
# # SVD + TFIDF + BinaryRelenvanceRandomForest
# from sklearn.decomposition import TruncatedSVD
# n_components = 100
# svd = TruncatedSVD(n_components=n_components)
# X_train_tfidf_svd = svd.fit_transform(X_train_tfidf)
# X_test_tfidf_svd = svd.fit_transform(X_test_tfidf)

# tfidf_rf=BinaryRelenvanceRandomForest(X_train_tfidf_svd, y_train)
# tfidf_rf.fit()
# y_tfidf_cc=tfidf_rf.predict(X_test_tfidf_svd)
# tfidf_rf.test(y_test)


Code cell <6CGBB6fDS46U>
# %% [code]
nltk.download('punkt_tab')

Execution output from Mar 1, 2025 11:31 AM
0KB
	Stream
		[nltk_data] Downloading package punkt_tab to /root/nltk_data...
		[nltk_data]   Unzipping tokenizers/punkt_tab.zip.
	text/plain
		True

Text cell <hojBjsgxM4za>
# %% [markdown]
# Deep Learning


Code cell <PtkMjR1jTWA9>
# %% [code]
import numpy as np

def Results(model, dataloader, criterion,model_name="BiLSTM",embedding_model='Word2Vec',type="test", device="none"):
    model.eval()
    total_loss = 0
    y_true = []
    y_pred = []
    with torch.no_grad():
        for inputs, targets in dataloader:
            inputs, targets = inputs.to(device), targets.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, targets)
            total_loss += loss.item()

            binary_preds = (outputs > 0.5).cpu().numpy()
            true_labels = targets.cpu().numpy()

            y_pred.append(binary_preds)
            y_true.append(true_labels)

    print(f'{model_name} with {embedding_model} embeddings {type} loss is {total_loss / len(dataloader)}')
    y_pred = np.vstack(y_pred)
    y_true = np.vstack(y_true)
    return y_pred, y_true

Code cell <4WPwXlQN9r_A>
# %% [code]
# split data into train, val, test as 50 20 30

mlb = MultiLabelBinarizer()
y = mlb.fit_transform(df["labels"])

X_train1, X_test_val1, y_train1, y_test_val1 = train_test_split(df["processed_text"], y, test_size=0.5, random_state=42)

X_test1, X_val1, y_test1, y_val1 = train_test_split(X_test_val1, y_test_val1, test_size=0.4, random_state=42)


Text cell <cLZJaCi-R9eE>
# %% [markdown]
# Model 2 : ANN

Code cell <t-PBVvXXSAt3>
# %% [code]
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

class ANNModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ANNModel, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.sigmoid(x)
        return x

# device=torch.device("cuda" if torch.cuda.is_available() else "cpu")





# from gensim.models import Word2Vec
# from nltk.tokenize import word_tokenize
# # Tokenize text into words
# train_tokens = X_train.apply(word_tokenize).tolist()

# # Train Word2Vec model
# word2vec_model = Word2Vec(sentences=train_tokens, vector_size=100, window=5, min_count=2, workers=4)





# def get_average_word2vec(text, model, vec_size=100):
#     words = text.split()
#     vectorized_words = [model.wv[word] for word in words if word in model.wv]
#     if not vectorized_words:
#         return np.zeros(vec_size)
#     avg_vector = np.mean(vectorized_words, axis=0)
#     return avg_vector

# X_train1_word2vec = np.array([get_average_word2vec(text, word2vec_model, 100) for text in X_train1])
# X_val1_word2vec = np.array([get_average_word2vec(text, word2vec_model, 100) for text in X_val1])
# X_test1_word2vec = np.array([get_average_word2vec(text, word2vec_model, 100) for text in X_test1])

# print("Word2Vec representation shape:", X_train1_word2vec.shape)






# X_train_tensor = torch.tensor(X_train1_word2vec, dtype=torch.float32).to(device)
# y_train_tensor = torch.tensor(y_train1, dtype=torch.float32).to(device)
# X_val_tensor = torch.tensor(X_val1_word2vec, dtype=torch.float32).to(device)
# y_val_tensor = torch.tensor(y_val1, dtype=torch.float32).to(device)
# X_test_tensor = torch.tensor(X_test1_word2vec, dtype=torch.float32).to(device)
# y_test_tensor = torch.tensor(y_test1, dtype=torch.float32).to(device)

# train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
# val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
# val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
# test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)
# model_ann = ANNModel(100, 128, 6).to(device)
# criterion = nn.BCELoss()
# optimizer = optim.Adam(model_ann.parameters(), lr=0.001)
# num_epochs = 10

# for epoch in range(num_epochs):
#     model_ann.train()
#     total_loss = 0
#     for X_batch, y_batch in train_dataloader:
#         optimizer.zero_grad()
#         output = model_ann(X_batch)
#         loss = criterion(output, y_batch)
#         loss.backward()
#         optimizer.step()
#         total_loss += loss.item()
#     model_ann.eval()
#     total_val_loss = 0
#     with torch.no_grad():
#         for X_val_batch, y_val_batch in val_dataloader:
#             output_val = model_ann(X_val_batch)
#             loss_val = criterion(output_val, y_val_batch)
#             total_val_loss += loss_val.item()
#     print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_loss/len(train_dataloader):.4f}, Val Loss: {total_val_loss/len(val_dataloader):.4f}")





# # Run Evaluation
# embedding_model="Word2Vec"
# y_train_pred, y_train_true = Results(model_ann, train_dataloader, criterion,model_name="ANN",type="train", embedding_model=embedding_model, device=device)
# y_val_pred,y_val_true = Results(model_ann, val_dataloader, criterion,model_name="ANN",type="validation", embedding_model=embedding_model, device=device)
# y_test_pred, y_test_true = Results(model_ann, test_dataloader, criterion,model_name="ANN",embedding_model=embedding_model,type="test", device=device)

# TestMultiLabel(y_train_true, y_train_pred,embedding_model=embedding_model,model_name="ANN")
# TestMultiLabel(y_val_true, y_val_pred,embedding_model=embedding_model,model_name="ANN")
# TestMultiLabel(y_test_true, y_test_pred,embedding_model=embedding_model,model_name="ANN")









# from gensim.models import FastText
# from nltk.tokenize import word_tokenize
# # Tokenize text into words
# train_tokens = X_train.apply(word_tokenize).tolist()

# # Train FastText model
# fastText_model = FastText(sentences=train_tokens, vector_size=100, window=5, min_count=2, workers=4)




# def get_average_fastText(text, model, vec_size=100):
#     words = text.split()
#     vectorized_words = [model.wv[word] for word in words if word in model.wv]
#     if not vectorized_words:
#         return np.zeros(vec_size)
#     avg_vector = np.mean(vectorized_words, axis=0)
#     return avg_vector

# X_train1_fastText = np.array([get_average_fastText(text, word2vec_model, 100) for text in X_train1])
# X_val1_fastText = np.array([get_average_fastText(text, word2vec_model, 100) for text in X_val1])
# X_test1_fastText = np.array([get_average_fastText(text, word2vec_model, 100) for text in X_test1])

# print("fast Text representation shape:", X_train1_fastText.shape)






# X_train_tensor = torch.tensor(X_train1_fastText, dtype=torch.float32).to(device)
# y_train_tensor = torch.tensor(y_train1, dtype=torch.float32).to(device)
# X_val_tensor = torch.tensor(X_val1_fastText, dtype=torch.float32).to(device)
# y_val_tensor = torch.tensor(y_val1, dtype=torch.float32).to(device)
# X_test_tensor = torch.tensor(X_test1_fastText, dtype=torch.float32).to(device)
# y_test_tensor = torch.tensor(y_test1, dtype=torch.float32).to(device)

# train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
# val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
# val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
# test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)






# model_ann = ANNModel(100, 128, 6).to(device)
# criterion = nn.BCELoss()
# optimizer = optim.Adam(model_ann.parameters(), lr=0.001)
# num_epochs = 10

# for epoch in range(num_epochs):
#     model_ann.train()
#     total_loss = 0
#     for X_batch, y_batch in train_dataloader:
#         optimizer.zero_grad()
#         output = model_ann(X_batch)
#         loss = criterion(output, y_batch)
#         loss.backward()
#         optimizer.step()
#         total_loss += loss.item()
#     model_ann.eval()
#     total_val_loss = 0
#     with torch.no_grad():
#         for X_val_batch, y_val_batch in val_dataloader:
#             output_val = model_ann(X_val_batch)
#             loss_val = criterion(output_val, y_val_batch)
#             total_val_loss += loss_val.item()
#     print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_loss/len(train_dataloader):.4f}, Val Loss: {total_val_loss/len(val_dataloader):.4f}")






# # Run Evaluation
# embedding_model="Fast Text"
# y_train_pred, y_train_true = Results(model_ann, train_dataloader, criterion,model_name="ANN",type="train", embedding_model=embedding_model, device=device)
# y_val_pred,y_val_true = Results(model_ann, val_dataloader, criterion,model_name="ANN",type="validation", embedding_model=embedding_model, device=device)
# y_test_pred, y_test_true = Results(model_ann, test_dataloader, criterion,model_name="ANN",embedding_model=embedding_model,type="test", device=device)

# TestMultiLabel(y_train_true, y_train_pred,embedding_model=embedding_model,model_name="ANN")
# TestMultiLabel(y_val_true, y_val_pred,embedding_model=embedding_model,model_name="ANN")
# TestMultiLabel(y_test_true, y_test_pred,embedding_model=embedding_model,model_name="ANN")




Code cell <UMlPDX23SngZ>
# %% [code]

device=torch.device("cuda" if torch.cuda.is_available() else "cpu")





from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
# Tokenize text into words
train_tokens = X_train.apply(word_tokenize).tolist()

# Train Word2Vec model
word2vec_model = Word2Vec(sentences=train_tokens, vector_size=100, window=5, min_count=2, workers=4)





def get_average_word2vec(text, model, vec_size=100):
    words = text.split()
    vectorized_words = [model.wv[word] for word in words if word in model.wv]
    if not vectorized_words:
        return np.zeros(vec_size)
    avg_vector = np.mean(vectorized_words, axis=0)
    return avg_vector

X_train1_word2vec = np.array([get_average_word2vec(text, word2vec_model, 100) for text in X_train1])
X_val1_word2vec = np.array([get_average_word2vec(text, word2vec_model, 100) for text in X_val1])
X_test1_word2vec = np.array([get_average_word2vec(text, word2vec_model, 100) for text in X_test1])

print("Word2Vec representation shape:", X_train1_word2vec.shape)

Execution output from Mar 1, 2025 11:32 AM
0KB
	Stream
		Word2Vec representation shape: (10486, 100)

Code cell <Q6bQMc40ShJT>
# %% [code]


X_train_tensor = torch.tensor(X_train1_word2vec, dtype=torch.float32).to(device)
y_train_tensor = torch.tensor(y_train1, dtype=torch.float32).to(device)
X_val_tensor = torch.tensor(X_val1_word2vec, dtype=torch.float32).to(device)
y_val_tensor = torch.tensor(y_val1, dtype=torch.float32).to(device)
X_test_tensor = torch.tensor(X_test1_word2vec, dtype=torch.float32).to(device)
y_test_tensor = torch.tensor(y_test1, dtype=torch.float32).to(device)

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)
model_ann = ANNModel(100, 128, 6).to(device)
criterion = nn.BCELoss()
optimizer = optim.Adam(model_ann.parameters(), lr=0.001)
num_epochs = 10

for epoch in range(num_epochs):
    model_ann.train()
    total_loss = 0
    for X_batch, y_batch in train_dataloader:
        optimizer.zero_grad()
        output = model_ann(X_batch)
        loss = criterion(output, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    model_ann.eval()
    total_val_loss = 0
    with torch.no_grad():
        for X_val_batch, y_val_batch in val_dataloader:
            output_val = model_ann(X_val_batch)
            loss_val = criterion(output_val, y_val_batch)
            total_val_loss += loss_val.item()
    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_loss/len(train_dataloader):.4f}, Val Loss: {total_val_loss/len(val_dataloader):.4f}")



Execution output from Mar 1, 2025 11:32 AM
1KB
	Stream
		Epoch [1/10], Train Loss: 0.2722, Val Loss: 0.2182
		Epoch [2/10], Train Loss: 0.2123, Val Loss: 0.2088
		Epoch [3/10], Train Loss: 0.2055, Val Loss: 0.2058
		Epoch [4/10], Train Loss: 0.2021, Val Loss: 0.2022
		Epoch [5/10], Train Loss: 0.1995, Val Loss: 0.2012
		Epoch [6/10], Train Loss: 0.1977, Val Loss: 0.1985
		Epoch [7/10], Train Loss: 0.1964, Val Loss: 0.1974
		Epoch [8/10], Train Loss: 0.1952, Val Loss: 0.1966
		Epoch [9/10], Train Loss: 0.1936, Val Loss: 0.1967
		Epoch [10/10], Train Loss: 0.1927, Val Loss: 0.1949

Code cell <8zpV8MAiSc3o>
# %% [code]
# Run Evaluation
embedding_model="Word2Vec"
y_train_pred, y_train_true = Results(model_ann, train_dataloader, criterion,model_name="ANN",type="train", embedding_model=embedding_model, device=device)
y_val_pred,y_val_true = Results(model_ann, val_dataloader, criterion,model_name="ANN",type="validation", embedding_model=embedding_model, device=device)
y_test_pred, y_test_true = Results(model_ann, test_dataloader, criterion,model_name="ANN",embedding_model=embedding_model,type="test", device=device)

TestMultiLabel(y_train_true, y_train_pred,embedding_model=embedding_model,model_name="ANN")
TestMultiLabel(y_val_true, y_val_pred,embedding_model=embedding_model,model_name="ANN")
TestMultiLabel(y_test_true, y_test_pred,embedding_model=embedding_model,model_name="ANN")
Execution output from Mar 1, 2025 11:34 AM
112KB
	Stream
		ANN with Word2Vec embeddings train loss is 0.18990007504003076
		ANN with Word2Vec embeddings validation loss is 0.19522457812545876
		ANN with Word2Vec embeddings test loss is 0.1938435354359864
		--------------
		Results of ANN with Word2Vec embeddings
		Exact Match Ratio: 0.6620
		Hamming Loss: 0.0785
		Recall: 0.8211
		Precision: 0.8329
		F1 Score: 0.8094
		/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
		--------------
		--------------
		Results of ANN with Word2Vec embeddings
		Exact Match Ratio: 0.6694
		Hamming Loss: 0.0785
		Recall: 0.8225
		Precision: 0.8351
		F1 Score: 0.8120
		/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
		--------------
		--------------
		Results of ANN with Word2Vec embeddings
		Exact Match Ratio: 0.6597
		Hamming Loss: 0.0797
		Recall: 0.8161
		Precision: 0.8343
		F1 Score: 0.8073
		/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
		--------------
	text/plain
		<Figure size 2500x1000 with 6 Axes>
		<Figure size 2500x1000 with 6 Axes>
		<Figure size 2500x1000 with 6 Axes>

Code cell <-qj4FCt_amgk>
# %% [code]
nltk.download('punkt_tab')
Execution output from Mar 1, 2025 12:05 PM
0KB
	Stream
		[nltk_data] Downloading package punkt_tab to /root/nltk_data...
		[nltk_data]   Unzipping tokenizers/punkt_tab.zip.
	text/plain
		True

Code cell <cCV3JChhSZoG>
# %% [code]
from gensim.models import FastText
from nltk.tokenize import word_tokenize
# Tokenize text into words
train_tokens = X_train.apply(word_tokenize).tolist()

# Train FastText model
fastText_model = FastText(sentences=train_tokens, vector_size=100, window=5, min_count=2, workers=4)

Code cell <LF4QBne5SWbt>
# %% [code]
def get_average_fastText(text, model, vec_size=100):
    words = text.split()
    vectorized_words = [model.wv[word] for word in words if word in model.wv]
    if not vectorized_words:
        return np.zeros(vec_size)
    avg_vector = np.mean(vectorized_words, axis=0)
    return avg_vector

X_train1_fastText = np.array([get_average_fastText(text, word2vec_model, 100) for text in X_train1])
X_val1_fastText = np.array([get_average_fastText(text, word2vec_model, 100) for text in X_val1])
X_test1_fastText = np.array([get_average_fastText(text, word2vec_model, 100) for text in X_test1])

print("fast Text representation shape:", X_train1_fastText.shape)
Execution output from Mar 1, 2025 11:34 AM
0KB
	Stream
		fast Text representation shape: (10486, 100)

Code cell <Xs1Agf7qSQcz>
# %% [code]

X_train_tensor = torch.tensor(X_train1_fastText, dtype=torch.float32).to(device)
y_train_tensor = torch.tensor(y_train1, dtype=torch.float32).to(device)
X_val_tensor = torch.tensor(X_val1_fastText, dtype=torch.float32).to(device)
y_val_tensor = torch.tensor(y_val1, dtype=torch.float32).to(device)
X_test_tensor = torch.tensor(X_test1_fastText, dtype=torch.float32).to(device)
y_test_tensor = torch.tensor(y_test1, dtype=torch.float32).to(device)

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
# test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)

Code cell <RcN_wsiTSIwc>
# %% [code]

model_ann = ANNModel(100, 128, 6).to(device)
criterion = nn.BCELoss()
optimizer = optim.Adam(model_ann.parameters(), lr=0.001)
num_epochs = 10

for epoch in range(num_epochs):
    model_ann.train()
    total_loss = 0
    for X_batch, y_batch in train_dataloader:
        optimizer.zero_grad()
        output = model_ann(X_batch)
        loss = criterion(output, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    model_ann.eval()
    total_val_loss = 0
    with torch.no_grad():
        for X_val_batch, y_val_batch in val_dataloader:
            output_val = model_ann(X_val_batch)
            loss_val = criterion(output_val, y_val_batch)
            total_val_loss += loss_val.item()
    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_loss/len(train_dataloader):.4f}, Val Loss: {total_val_loss/len(val_dataloader):.4f}")


Execution output from Mar 1, 2025 11:34 AM
1KB
	Stream
		Epoch [1/10], Train Loss: 0.2707, Val Loss: 0.2173
		Epoch [2/10], Train Loss: 0.2127, Val Loss: 0.2103
		Epoch [3/10], Train Loss: 0.2063, Val Loss: 0.2059
		Epoch [4/10], Train Loss: 0.2026, Val Loss: 0.2021
		Epoch [5/10], Train Loss: 0.1999, Val Loss: 0.1998
		Epoch [6/10], Train Loss: 0.1981, Val Loss: 0.1984
		Epoch [7/10], Train Loss: 0.1956, Val Loss: 0.1971
		Epoch [8/10], Train Loss: 0.1944, Val Loss: 0.1969
		Epoch [9/10], Train Loss: 0.1934, Val Loss: 0.1952
		Epoch [10/10], Train Loss: 0.1922, Val Loss: 0.1952

Code cell <J0g2wecpSGt1>
# %% [code]
# Run Evaluation
embedding_model="Fast Text"
y_train_pred, y_train_true = Results(model_ann, train_dataloader, criterion,model_name="ANN",type="train", embedding_model=embedding_model, device=device)
y_val_pred,y_val_true = Results(model_ann, val_dataloader, criterion,model_name="ANN",type="validation", embedding_model=embedding_model, device=device)
y_test_pred, y_test_true = Results(model_ann, test_dataloader, criterion,model_name="ANN",embedding_model=embedding_model,type="test", device=device)

TestMultiLabel(y_train_true, y_train_pred,embedding_model=embedding_model,model_name="ANN")
TestMultiLabel(y_val_true, y_val_pred,embedding_model=embedding_model,model_name="ANN")
TestMultiLabel(y_test_true, y_test_pred,embedding_model=embedding_model,model_name="ANN")
Execution output from Mar 1, 2025 11:34 AM
112KB
	Stream
		ANN with Fast Text embeddings train loss is 0.18991287311584484
		ANN with Fast Text embeddings validation loss is 0.19522457812545876
		ANN with Fast Text embeddings test loss is 0.1938435354359864
		--------------
		Results of ANN with Fast Text embeddings
		Exact Match Ratio: 0.6620
		Hamming Loss: 0.0785
		Recall: 0.8211
		Precision: 0.8329
		F1 Score: 0.8094
		/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
		--------------
		--------------
		Results of ANN with Fast Text embeddings
		Exact Match Ratio: 0.6694
		Hamming Loss: 0.0785
		Recall: 0.8225
		Precision: 0.8351
		F1 Score: 0.8120
		/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
		--------------
		--------------
		Results of ANN with Fast Text embeddings
		Exact Match Ratio: 0.6597
		Hamming Loss: 0.0797
		Recall: 0.8161
		Precision: 0.8343
		F1 Score: 0.8073
		/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
		--------------
	text/plain
		<Figure size 2500x1000 with 6 Axes>
		<Figure size 2500x1000 with 6 Axes>
		<Figure size 2500x1000 with 6 Axes>

Text cell <GaD_GcwWTWBU>
# %% [markdown]
### RNN_word2vec

Code cell <fgpAawnpTWBV>
# %% [code]

from gensim.models import Word2Vec
from gensim.utils import simple_preprocess

def Word2VecModel(X_train):
  # Tokenize the text data
  tokenized_train_texts = [text.split() for text in X_train]
  word2vec_model = Word2Vec(
      sentences=tokenized_train_texts,
      vector_size=100,            # what dimensions should the vector be of
      window=5,                   # Context window size
      min_count=2,
      sg=1                         # Use skip-gram (sg=1)
  )
  return word2vec_model
word2vec_model=Word2VecModel(X_train1)
Execution output from Mar 1, 2025 11:35 AM
11KB
	Error
		KeyboardInterrupt
		---------------------------------------------------------------------------
		KeyboardInterrupt                         Traceback (most recent call last)
		<ipython-input-30-403bc8984a1c> in <cell line: 0>()
		     13   )
		     14   return word2vec_model
		---> 15 word2vec_model=Word2VecModel(X_train1)
		
		<ipython-input-30-403bc8984a1c> in Word2VecModel(X_train)
		      5   # Tokenize the text data
		      6   tokenized_train_texts = [text.split() for text in X_train]
		----> 7   word2vec_model = Word2Vec(
		      8       sentences=tokenized_train_texts,
		      9       vector_size=100,            # what dimensions should the vector be of
		
		/usr/local/lib/python3.11/dist-packages/gensim/models/word2vec.py in __init__(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)
		    428             self._check_corpus_sanity(corpus_iterable=corpus_iterable, corpus_file=corpus_file, passes=(epochs + 1))
		    429             self.build_vocab(corpus_iterable=corpus_iterable, corpus_file=corpus_file, trim_rule=trim_rule)
		--> 430             self.train(
		    431                 corpus_iterable=corpus_iterable, corpus_file=corpus_file, total_examples=self.corpus_count,
		    432                 total_words=self.corpus_total_words, epochs=self.epochs, start_alpha=self.alpha,
		
		/usr/local/lib/python3.11/dist-packages/gensim/models/word2vec.py in train(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)
		   1071 
		   1072             if corpus_iterable is not None:
		-> 1073                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(
		   1074                     corpus_iterable, cur_epoch=cur_epoch, total_examples=total_examples,
		   1075                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay,
		
		/usr/local/lib/python3.11/dist-packages/gensim/models/word2vec.py in _train_epoch(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)
		   1432             thread.start()
		   1433 
		-> 1434         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(
		   1435             progress_queue, job_queue, cur_epoch=cur_epoch, total_examples=total_examples,
		   1436             total_words=total_words, report_delay=report_delay, is_corpus_file_mode=False,
		
		/usr/local/lib/python3.11/dist-packages/gensim/models/word2vec.py in _log_epoch_progress(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)
		   1287 
		   1288         while unfinished_worker_count > 0:
		-> 1289             report = progress_queue.get()  # blocks if workers too slow
		   1290             if report is None:  # a thread reporting that it finished
		   1291                 unfinished_worker_count -= 1
		
		/usr/lib/python3.11/queue.py in get(self, block, timeout)
		    169             elif timeout is None:
		    170                 while not self._qsize():
		--> 171                     self.not_empty.wait()
		    172             elif timeout < 0:
		    173                 raise ValueError("'timeout' must be a non-negative number")
		
		/usr/lib/python3.11/threading.py in wait(self, timeout)
		    325         try:    # restore state no matter what (e.g., KeyboardInterrupt)
		    326             if timeout is None:
		--> 327                 waiter.acquire()
		    328                 gotit = True
		    329             else:
		
		KeyboardInterrupt: 

Code cell <TQLxziS4TWBV>
# %% [code]

from torch.utils.data import Dataset
import numpy as np
import torch

class EmbeddingDataset(Dataset):
    def __init__(self, textdata, labels, word2vec_model, vec_size=100, max_len=100):
        self.xTrain = textdata
        self.vec_size = vec_size
        self.yTrain = labels
        self.model = word2vec_model
        self.max_len = max_len

    def __getitem__(self, idx):
        text = self.xTrain.iloc[idx]
        words = text.split()
        vectorized_words = [self.model.wv[word] for word in words if word in self.model.wv]
        if not vectorized_words:
            vectorized_words = [np.zeros(self.vec_size)]

        vectorized_words = torch.tensor(vectorized_words, dtype=torch.float32)

        if len(vectorized_words) < self.max_len:
            pad_size = self.max_len - len(vectorized_words)
            padding = torch.zeros((pad_size, self.vec_size))
            vectorized_words = torch.cat([vectorized_words, padding], dim=0)
        else:
            vectorized_words = vectorized_words[:self.max_len]

        currlabel = torch.tensor(self.yTrain[idx], dtype=torch.float32)

        return vectorized_words, currlabel

    def __len__(self):
        return len(self.xTrain)

train_dataset = EmbeddingDataset(X_train1,y_train1, word2vec_model)
train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False)

val_dataset = EmbeddingDataset(X_val1,y_val1, word2vec_model)
val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)

test_dataset = EmbeddingDataset(X_test1,y_test1, word2vec_model)
test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)

Code cell <9fq909rrTWBV>
# %% [code]
class GRUClassiferModel(nn.Module):
    def __init__(self, input_dim=100, hidden_dim=128, output_dim=6, num_layers=2):
        super(GRUClassiferModel, self).__init__()
        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x, _ = self.gru(x)
        x = x[:, -1, :]
        x = self.fc(x)
        out = self.sigmoid(x)
        return out

Code cell <FiYdI5e5TWBV>
# %% [code]
device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

model = GRUClassiferModel(100, 128, 6, 3).to(device)
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.005)

num_epochs = 10

for epoch in range(num_epochs):
    # Training Phase
    model.train()
    total_train_loss = 0

    for X, y in train_dataloader:
        X, y = X.to(device), y.to(device)
        optimizer.zero_grad()
        output = model(X)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()
        total_train_loss += loss.item()

    avg_train_loss = total_train_loss / len(train_dataloader)

    # Validation Phase
    model.eval()
    total_val_loss = 0

    with torch.no_grad():
        for X_val, y_val in val_dataloader:
            X_val, y_val = X_val.to(device), y_val.to(device)
            output_val = model(X_val)
            loss_val = criterion(output_val, y_val)
            total_val_loss += loss_val.item()

    avg_val_loss = total_val_loss / len(val_dataloader)

    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")
Execution output from Mar 1, 2025 11:40 AM
1KB
	Stream
		cuda
		<ipython-input-31-779dac088c8d>:20: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)
		  vectorized_words = torch.tensor(vectorized_words, dtype=torch.float32)
		Epoch [1/10], Train Loss: 0.2873, Val Loss: 0.2322
		Epoch [2/10], Train Loss: 0.2223, Val Loss: 0.2060
		Epoch [3/10], Train Loss: 0.2020, Val Loss: 0.2050
		Epoch [4/10], Train Loss: 0.1917, Val Loss: 0.2041
		Epoch [5/10], Train Loss: 0.1810, Val Loss: 0.2178
		Epoch [6/10], Train Loss: 0.1716, Val Loss: 0.2271
		Epoch [7/10], Train Loss: 0.1629, Val Loss: 0.2165
		Epoch [8/10], Train Loss: 0.1531, Val Loss: 0.2225
		Epoch [9/10], Train Loss: 0.1444, Val Loss: 0.2312
		Epoch [10/10], Train Loss: 0.1334, Val Loss: 0.2381

Code cell <6LKojzN0TWBV>
# %% [code]
# Run Evaluation
model_name="GRU"
embedding_model="Word2Vec"
y_train_pred, y_train_true = Results(model, train_dataloader, criterion,model_name,embedding_model,type="train", device=device)
y_val_pred,y_val_true = Results(model, val_dataloader, criterion,model_name,embedding_model,type="validation", device=device)
y_test_pred, y_test_true = Results(model, test_dataloader, criterion,model_name,embedding_model,type="test", device=device)

TestMultiLabel(y_train_true, y_train_pred,embedding_model,model_name)
TestMultiLabel(y_val_true, y_val_pred,embedding_model,model_name)
TestMultiLabel(y_test_true, y_test_pred,embedding_model,model_name)
Execution output from Mar 1, 2025 11:41 AM
113KB
	Stream
		GRU with Word2Vec embeddings train loss is 0.1280383569438283
		GRU with Word2Vec embeddings validation loss is 0.2381044885877407
		GRU with Word2Vec embeddings test loss is 0.24449233755920874
		--------------
		Results of GRU with Word2Vec embeddings
		Exact Match Ratio: 0.7513
		Hamming Loss: 0.0511
		Recall: 0.8860
		Precision: 0.9031
		F1 Score: 0.8792
		/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
		--------------
		--------------
		Results of GRU with Word2Vec embeddings
		Exact Match Ratio: 0.6393
		Hamming Loss: 0.0865
		Recall: 0.8085
		Precision: 0.8248
		F1 Score: 0.7980
		/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
		--------------
		--------------
		Results of GRU with Word2Vec embeddings
		Exact Match Ratio: 0.6279
		Hamming Loss: 0.0891
		Recall: 0.7987
		Precision: 0.8180
		F1 Score: 0.7892
		/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
		--------------
	text/plain
		<Figure size 2500x1000 with 6 Axes>
		<Figure size 2500x1000 with 6 Axes>
		<Figure size 2500x1000 with 6 Axes>

Text cell <9IxvVlm2TWBV>
# %% [markdown]
### RNN+fasttext

Code cell <AyP1-uInTWBV>
# %% [code]
from torch.utils.data import Dataset
import numpy as np
import torch

class EmbeddingDataset(Dataset):
    def __init__(self, textdata, labels, word2vec_model, vec_size=100, max_len=100):
        self.xTrain = textdata
        self.vec_size = vec_size
        self.yTrain = labels
        self.model = word2vec_model
        self.max_len = max_len

    def __getitem__(self, idx):
        text = self.xTrain.iloc[idx]
        words = text.split()
        vectorized_words = [self.model.wv[word] for word in words if word in self.model.wv]
        if not vectorized_words:
            vectorized_words = [np.zeros(self.vec_size)]

        vectorized_words = torch.tensor(vectorized_words, dtype=torch.float32)

        if len(vectorized_words) < self.max_len:
            pad_size = self.max_len - len(vectorized_words)
            padding = torch.zeros((pad_size, self.vec_size))
            vectorized_words = torch.cat([vectorized_words, padding], dim=0)
        else:
            vectorized_words = vectorized_words[:self.max_len]

        currlabel = torch.tensor(self.yTrain[idx], dtype=torch.float32)

        return vectorized_words, currlabel

    def __len__(self):
        return len(self.xTrain)


Code cell <99TdEI3-TWBV>
# %% [code]
from gensim.models import FastText
from gensim.utils import simple_preprocess

def FastTextModel(X_train):
  # Tokenize the text data
  tokenized_train_texts = [text.split() for text in X_train]
  fasttext_model = FastText(
      sentences=tokenized_train_texts,
      vector_size=100,            # what dimensions should the vector be of
      window=5,                   # Context window size
      min_count=2,
      sg=1                         # Use skip-gram (sg=1)
  )
  return fasttext_model
fasttext_model = FastTextModel(X_train1)
train_dataset = EmbeddingDataset(X_train1,y_train1, fasttext_model)
train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False)

val_dataset = EmbeddingDataset(X_val1,y_val1, fasttext_model)
val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)

test_dataset = EmbeddingDataset(X_test1,y_test1, fasttext_model)
test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)

Code cell <1aXzCne9TWBW>
# %% [code]
device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

model = GRUClassiferModel(100, 128, 6, 3).to(device)
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.005)

num_epochs = 10

for epoch in range(num_epochs):
    # Training Phase
    model.train()
    total_train_loss = 0

    for X, y in train_dataloader:
        X, y = X.to(device), y.to(device)
        optimizer.zero_grad()
        output = model(X)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()
        total_train_loss += loss.item()

    avg_train_loss = total_train_loss / len(train_dataloader)

    # Validation Phase
    model.eval()
    total_val_loss = 0

    with torch.no_grad():
        for X_val, y_val in val_dataloader:
            X_val, y_val = X_val.to(device), y_val.to(device)
            output_val = model(X_val)
            loss_val = criterion(output_val, y_val)
            total_val_loss += loss_val.item()

    avg_val_loss = total_val_loss / len(val_dataloader)

    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")
Execution output
0KB
	Stream
		cuda
		Epoch [1/10], Train Loss: 0.4383, Val Loss: 0.4271
		Epoch [2/10], Train Loss: 0.3765, Val Loss: 0.3388
		Epoch [3/10], Train Loss: 0.4035, Val Loss: 0.4010
		Epoch [4/10], Train Loss: 0.4041, Val Loss: 0.4063
		Epoch [5/10], Train Loss: 0.4039, Val Loss: 0.4060
		Epoch [6/10], Train Loss: 0.4035, Val Loss: 0.4116

Code cell <bO8gooxVTWBW>
# %% [code]
# Run Evaluation
model_name="GRU"
embedding_model="FastText"
y_train_pred, y_train_true = Results(model, train_dataloader, criterion,model_name,embedding_model,type="train", device=device)
y_val_pred,y_val_true = Results(model, val_dataloader, criterion,model_name,embedding_model,type="validation", device=device)
y_test_pred, y_test_true = Results(model, test_dataloader, criterion,model_name,embedding_model,type="test", device=device)

TestMultiLabel(y_train_true, y_train_pred,embedding_model,model_name)
TestMultiLabel(y_val_true, y_val_pred,embedding_model,model_name)
TestMultiLabel(y_test_true, y_test_pred,embedding_model,model_name)
Execution output
43KB
	Stream
		RNN with FastText embeddings train loss is 0.45462487056487944
		RNN with FastText embeddings validation loss is 0.45612005347555334
		RNN with FastText embeddings test loss is 0.4588056853925339
		--------------
		Results of RNN with FastText embeddings
		Exact Match Ratio: 0.2373
		Hamming Loss: 0.2364
		Recall: 0.3245
		Precision: 0.4154
		F1 Score: 0.3542
		--------------
		--------------
		Results of RNN with FastText embeddings
		Exact Match Ratio: 0.2362
		Hamming Loss: 0.2388
		Recall: 0.3218
		Precision: 0.4107
		F1 Score: 0.3509
		--------------
		--------------
		Results of RNN with FastText embeddings
		Exact Match Ratio: 0.2275
		Hamming Loss: 0.2429
		Recall: 0.3114
		Precision: 0.3998
		F1 Score: 0.3401
		--------------
	text/plain
		<Figure size 1500x500 with 6 Axes>
		<Figure size 1500x500 with 6 Axes>
		<Figure size 1500x500 with 6 Axes>

Text cell <sZ1HvKCpTWBW>
# %% [markdown]
# Model 3: Bidirectional LSTM
### using word2vec embeddings

Code cell <-VfflUkU95NO>
# %% [code]

from gensim.models import Word2Vec
from gensim.utils import simple_preprocess

def Word2VecModel(X_train):
  # Tokenize the text data
  tokenized_train_texts = [text.split() for text in X_train]
  word2vec_model = Word2Vec(
      sentences=tokenized_train_texts,
      vector_size=100,            # what dimensions should the vector be of
      window=5,                   # Context window size
      min_count=2,
      sg=1                         # Use skip-gram (sg=1)
  )
  return word2vec_model
word2vec_model=Word2VecModel(X_train1)

Code cell <xNrG4fTJ-2sw>
# %% [code]

from torch.utils.data import Dataset
import numpy as np
import torch

class EmbeddingDataset(Dataset):
    def __init__(self, textdata, labels, word2vec_model, vec_size=100, max_len=100):
        self.xTrain = textdata
        self.vec_size = vec_size
        self.yTrain = labels
        self.model = word2vec_model
        self.max_len = max_len

    def __getitem__(self, idx):
        text = self.xTrain.iloc[idx]
        words = text.split()
        vectorized_words = [self.model.wv[word] for word in words if word in self.model.wv]
        if not vectorized_words:
            vectorized_words = [np.zeros(self.vec_size)]

        vectorized_words = torch.tensor(vectorized_words, dtype=torch.float32)

        if len(vectorized_words) < self.max_len:
            pad_size = self.max_len - len(vectorized_words)
            padding = torch.zeros((pad_size, self.vec_size))
            vectorized_words = torch.cat([vectorized_words, padding], dim=0)
        else:
            vectorized_words = vectorized_words[:self.max_len]

        currlabel = torch.tensor(self.yTrain[idx], dtype=torch.float32)

        return vectorized_words, currlabel

    def __len__(self):
        return len(self.xTrain)

train_dataset = EmbeddingDataset(X_train1,y_train1, word2vec_model)
train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False)

val_dataset = EmbeddingDataset(X_val1,y_val1, word2vec_model)
val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)

test_dataset = EmbeddingDataset(X_test1,y_test1, word2vec_model)
test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)

Code cell <xBeJcFBUTWBW>
# %% [code]
class BiLSTMClassiferModel(nn.Module):
    def __init__(self, input_dim=100, hidden_dim=128, output_dim=6, num_layers=2):
        super(BiLSTMClassiferModel, self).__init__()
        self.bilstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_dim * 2, output_dim)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x, _ = self.bilstm(x)
        x = x[:, -1, :]
        x = self.fc(x)
        out = self.sigmoid(x)
        return out

Code cell <KSEhWjrAAVYh>
# %% [code]
device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

model = BiLSTMClassiferModel(100, 128, 6, 3).to(device)
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.005)

num_epochs = 10

for epoch in range(num_epochs):
    # Training Phase
    model.train()
    total_train_loss = 0

    for X, y in train_dataloader:
        X, y = X.to(device), y.to(device)
        optimizer.zero_grad()
        output = model(X)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()
        total_train_loss += loss.item()

    avg_train_loss = total_train_loss / len(train_dataloader)

    # Validation Phase
    model.eval()
    total_val_loss = 0

    with torch.no_grad():
        for X_val, y_val in val_dataloader:
            X_val, y_val = X_val.to(device), y_val.to(device)
            output_val = model(X_val)
            loss_val = criterion(output_val, y_val)
            total_val_loss += loss_val.item()

    avg_val_loss = total_val_loss / len(val_dataloader)

    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")
Execution output
1KB
	Stream
		cuda
		/tmp/ipykernel_1050052/4152153291.py:20: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)
		  vectorized_words = torch.tensor(vectorized_words, dtype=torch.float32)
		Epoch [1/10], Train Loss: 0.4193, Val Loss: 0.3453
		Epoch [2/10], Train Loss: 0.2856, Val Loss: 0.2485
		Epoch [3/10], Train Loss: 0.2366, Val Loss: 0.2336
		Epoch [4/10], Train Loss: 0.2126, Val Loss: 0.2082
		Epoch [5/10], Train Loss: 0.2020, Val Loss: 0.2020
		Epoch [6/10], Train Loss: 0.1919, Val Loss: 0.2012
		Epoch [7/10], Train Loss: 0.1855, Val Loss: 0.2016
		Epoch [8/10], Train Loss: 0.1783, Val Loss: 0.2020
		Epoch [9/10], Train Loss: 0.1711, Val Loss: 0.2075
		Epoch [10/10], Train Loss: 0.1639, Val Loss: 0.2141

Code cell <69Vr5chuTWBX>
# %% [code]
# Run Evaluation
model_name="BiLSTM"
embedding_model="Word2Vec"
y_train_pred, y_train_true = Results(model, train_dataloader, criterion,model_name,embedding_model,type="train", device=device)
y_val_pred,y_val_true = Results(model, val_dataloader, criterion,model_name,embedding_model,type="validation", device=device)
y_test_pred, y_test_true = Results(model, test_dataloader, criterion,model_name,embedding_model,type="test", device=device)

TestMultiLabel(y_train_true, y_train_pred,embedding_model,model_name)
TestMultiLabel(y_val_true, y_val_pred,embedding_model,model_name)
TestMultiLabel(y_test_true, y_test_pred,embedding_model,model_name)
Execution output
44KB
	Stream
		BiLSTM with Word2Vec embeddings train loss is 0.16137412190437317
		BiLSTM with Word2Vec embeddings validation loss is 0.21408054887345343
		BiLSTM with Word2Vec embeddings test loss is 0.21562017221944502
		--------------
		Results of BiLSTM with Word2Vec embeddings
		Exact Match Ratio: 0.6975
		Hamming Loss: 0.0667
		Recall: 0.8666
		Precision: 0.8587
		F1 Score: 0.8454
		/home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
		--------------
		--------------
		Results of BiLSTM with Word2Vec embeddings
		Exact Match Ratio: 0.6534
		Hamming Loss: 0.0826
		Recall: 0.8329
		Precision: 0.8239
		F1 Score: 0.8102
		/home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
		--------------
		--------------
		Results of BiLSTM with Word2Vec embeddings
		Exact Match Ratio: 0.6514
		Hamming Loss: 0.0827
		Recall: 0.8268
		Precision: 0.8217
		F1 Score: 0.8062
		/home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
		--------------
	text/plain
		<Figure size 1500x500 with 6 Axes>
		<Figure size 1500x500 with 6 Axes>
		<Figure size 1500x500 with 6 Axes>

Text cell <ycNaFneZTWBX>
# %% [markdown]
# using FastText Embeddings

Code cell <gMg8rNBzTWBX>
# %% [code]
from torch.utils.data import Dataset
import numpy as np
import torch

class EmbeddingDataset(Dataset):
    def __init__(self, textdata, labels, word2vec_model, vec_size=100, max_len=100):
        self.xTrain = textdata
        self.vec_size = vec_size
        self.yTrain = labels
        self.model = word2vec_model
        self.max_len = max_len

    def __getitem__(self, idx):
        text = self.xTrain.iloc[idx]
        words = text.split()
        vectorized_words = [self.model.wv[word] for word in words if word in self.model.wv]
        if not vectorized_words:
            vectorized_words = [np.zeros(self.vec_size)]

        vectorized_words = torch.tensor(vectorized_words, dtype=torch.float32)

        if len(vectorized_words) < self.max_len:
            pad_size = self.max_len - len(vectorized_words)
            padding = torch.zeros((pad_size, self.vec_size))
            vectorized_words = torch.cat([vectorized_words, padding], dim=0)
        else:
            vectorized_words = vectorized_words[:self.max_len]

        currlabel = torch.tensor(self.yTrain[idx], dtype=torch.float32)

        return vectorized_words, currlabel

    def __len__(self):
        return len(self.xTrain)


Code cell <qcD7xSvtTWBX>
# %% [code]
from gensim.models import FastText
from gensim.utils import simple_preprocess

def FastTextModel(X_train):
  # Tokenize the text data
  tokenized_train_texts = [text.split() for text in X_train]
  fasttext_model = FastText(
      sentences=tokenized_train_texts,
      vector_size=100,            # what dimensions should the vector be of
      window=5,                   # Context window size
      min_count=2,
      sg=1                         # Use skip-gram (sg=1)
  )
  return fasttext_model
fasttext_model = FastTextModel(X_train1)
train_dataset = EmbeddingDataset(X_train1,y_train1, fasttext_model)
train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False)

val_dataset = EmbeddingDataset(X_val1,y_val1, fasttext_model)
val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)

test_dataset = EmbeddingDataset(X_test1,y_test1, fasttext_model)
test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)

Code cell <M8oMRRDCTWBX>
# %% [code]
device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

model = BiLSTMClassiferModel(100, 128, 6, 3).to(device)
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.005)

num_epochs = 10

for epoch in range(num_epochs):
    # Training Phase
    model.train()
    total_train_loss = 0

    for X, y in train_dataloader:
        X, y = X.to(device), y.to(device)
        optimizer.zero_grad()
        output = model(X)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()
        total_train_loss += loss.item()

    avg_train_loss = total_train_loss / len(train_dataloader)

    # Validation Phase
    model.eval()
    total_val_loss = 0

    with torch.no_grad():
        for X_val, y_val in val_dataloader:
            X_val, y_val = X_val.to(device), y_val.to(device)
            output_val = model(X_val)
            loss_val = criterion(output_val, y_val)
            total_val_loss += loss_val.item()

    avg_val_loss = total_val_loss / len(val_dataloader)

    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")
Execution output
1KB
	Stream
		cuda
		Epoch [1/10], Train Loss: 0.3905, Val Loss: 0.2935
		Epoch [2/10], Train Loss: 0.2811, Val Loss: 0.2600
		Epoch [3/10], Train Loss: 0.2533, Val Loss: 0.2456
		Epoch [4/10], Train Loss: 0.2340, Val Loss: 0.2297
		Epoch [5/10], Train Loss: 0.2215, Val Loss: 0.2210
		Epoch [6/10], Train Loss: 0.2079, Val Loss: 0.2060
		Epoch [7/10], Train Loss: 0.2031, Val Loss: 0.2113
		Epoch [8/10], Train Loss: 0.1949, Val Loss: 0.2180
		Epoch [9/10], Train Loss: 0.1890, Val Loss: 0.2112
		Epoch [10/10], Train Loss: 0.1817, Val Loss: 0.2129

Code cell <zfJ4Tl-TTWBX>
# %% [code]
# Run Evaluation
model_name="BiLSTM"
embedding_model="FastText"
y_train_pred, y_train_true = Results(model, train_dataloader, criterion,model_name,embedding_model,type="train", device=device)
y_val_pred,y_val_true = Results(model, val_dataloader, criterion,model_name,embedding_model,type="validation", device=device)
y_test_pred, y_test_true = Results(model, test_dataloader, criterion,model_name,embedding_model,type="test", device=device)

TestMultiLabel(y_train_true, y_train_pred,embedding_model,model_name)
TestMultiLabel(y_val_true, y_val_pred,embedding_model,model_name)
TestMultiLabel(y_test_true, y_test_pred,embedding_model,model_name)
Execution output
44KB
	Stream
		BiLSTM with FastText embeddings train loss is 0.18143551274225478
		BiLSTM with FastText embeddings validation loss is 0.21288616435997415
		BiLSTM with FastText embeddings test loss is 0.2149132521766605
		--------------
		Results of BiLSTM with FastText embeddings
		Exact Match Ratio: 0.6739
		Hamming Loss: 0.0731
		Recall: 0.8507
		Precision: 0.8462
		F1 Score: 0.8300
		/home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
		--------------
		--------------
		Results of BiLSTM with FastText embeddings
		Exact Match Ratio: 0.6532
		Hamming Loss: 0.0824
		Recall: 0.8274
		Precision: 0.8252
		F1 Score: 0.8081
		/home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
		--------------
		--------------
		Results of BiLSTM with FastText embeddings
		Exact Match Ratio: 0.6473
		Hamming Loss: 0.0825
		Recall: 0.8281
		Precision: 0.8251
		F1 Score: 0.8079
		/home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
		--------------
	text/plain
		<Figure size 1500x500 with 6 Axes>
		<Figure size 1500x500 with 6 Axes>
		<Figure size 1500x500 with 6 Axes>

Text cell <xE8FX2TxTWBX>
# %% [markdown]
# Model 4: Feed Forward Neural Network

Code cell <PefOYmxuTWBY>
# %% [code]
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

class ANNModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ANNModel, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.sigmoid(x)
        return x

device=torch.device("cuda" if torch.cuda.is_available() else "cpu")





from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
# Tokenize text into words
train_tokens = X_train.apply(word_tokenize).tolist()

# Train Word2Vec model
word2vec_model = Word2Vec(sentences=train_tokens, vector_size=100, window=5, min_count=2, workers=4)





def get_average_word2vec(text, model, vec_size=100):
    words = text.split()
    vectorized_words = [model.wv[word] for word in words if word in model.wv]
    if not vectorized_words:
        return np.zeros(vec_size)
    avg_vector = np.mean(vectorized_words, axis=0)
    return avg_vector

X_train1_word2vec = np.array([get_average_word2vec(text, word2vec_model, 100) for text in X_train1])
X_val1_word2vec = np.array([get_average_word2vec(text, word2vec_model, 100) for text in X_val1])
X_test1_word2vec = np.array([get_average_word2vec(text, word2vec_model, 100) for text in X_test1])

print("Word2Vec representation shape:", X_train1_word2vec.shape)






X_train_tensor = torch.tensor(X_train1_word2vec, dtype=torch.float32).to(device)
y_train_tensor = torch.tensor(y_train1, dtype=torch.float32).to(device)
X_val_tensor = torch.tensor(X_val1_word2vec, dtype=torch.float32).to(device)
y_val_tensor = torch.tensor(y_val1, dtype=torch.float32).to(device)
X_test_tensor = torch.tensor(X_test1_word2vec, dtype=torch.float32).to(device)
y_test_tensor = torch.tensor(y_test1, dtype=torch.float32).to(device)

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)
model_ann = ANNModel(100, 128, 6).to(device)
criterion = nn.BCELoss()
optimizer = optim.Adam(model_ann.parameters(), lr=0.001)
num_epochs = 10

for epoch in range(num_epochs):
    model_ann.train()
    total_loss = 0
    for X_batch, y_batch in train_dataloader:
        optimizer.zero_grad()
        output = model_ann(X_batch)
        loss = criterion(output, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    model_ann.eval()
    total_val_loss = 0
    with torch.no_grad():
        for X_val_batch, y_val_batch in val_dataloader:
            output_val = model_ann(X_val_batch)
            loss_val = criterion(output_val, y_val_batch)
            total_val_loss += loss_val.item()
    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_loss/len(train_dataloader):.4f}, Val Loss: {total_val_loss/len(val_dataloader):.4f}")





# Run Evaluation
embedding_model="Word2Vec"
y_train_pred, y_train_true = Results(model_ann, train_dataloader, criterion,model_name="ANN",type="train", embedding_model=embedding_model, device=device)
y_val_pred,y_val_true = Results(model_ann, val_dataloader, criterion,model_name="ANN",type="validation", embedding_model=embedding_model, device=device)
y_test_pred, y_test_true = Results(model_ann, test_dataloader, criterion,model_name="ANN",embedding_model=embedding_model,type="test", device=device)

TestMultiLabel(y_train_true, y_train_pred,embedding_model=embedding_model,model_name="ANN")
TestMultiLabel(y_val_true, y_val_pred,embedding_model=embedding_model,model_name="ANN")
TestMultiLabel(y_test_true, y_test_pred,embedding_model=embedding_model,model_name="ANN")









from gensim.models import FastText
from nltk.tokenize import word_tokenize
# Tokenize text into words
train_tokens = X_train.apply(word_tokenize).tolist()

# Train FastText model
fastText_model = FastText(sentences=train_tokens, vector_size=100, window=5, min_count=2, workers=4)




def get_average_fastText(text, model, vec_size=100):
    words = text.split()
    vectorized_words = [model.wv[word] for word in words if word in model.wv]
    if not vectorized_words:
        return np.zeros(vec_size)
    avg_vector = np.mean(vectorized_words, axis=0)
    return avg_vector

X_train1_fastText = np.array([get_average_fastText(text, word2vec_model, 100) for text in X_train1])
X_val1_fastText = np.array([get_average_fastText(text, word2vec_model, 100) for text in X_val1])
X_test1_fastText = np.array([get_average_fastText(text, word2vec_model, 100) for text in X_test1])

print("fast Text representation shape:", X_train1_fastText.shape)






X_train_tensor = torch.tensor(X_train1_fastText, dtype=torch.float32).to(device)
y_train_tensor = torch.tensor(y_train1, dtype=torch.float32).to(device)
X_val_tensor = torch.tensor(X_val1_fastText, dtype=torch.float32).to(device)
y_val_tensor = torch.tensor(y_val1, dtype=torch.float32).to(device)
X_test_tensor = torch.tensor(X_test1_fastText, dtype=torch.float32).to(device)
y_test_tensor = torch.tensor(y_test1, dtype=torch.float32).to(device)

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)






model_ann = ANNModel(100, 128, 6).to(device)
criterion = nn.BCELoss()
optimizer = optim.Adam(model_ann.parameters(), lr=0.001)
num_epochs = 10

for epoch in range(num_epochs):
    model_ann.train()
    total_loss = 0
    for X_batch, y_batch in train_dataloader:
        optimizer.zero_grad()
        output = model_ann(X_batch)
        loss = criterion(output, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    model_ann.eval()
    total_val_loss = 0
    with torch.no_grad():
        for X_val_batch, y_val_batch in val_dataloader:
            output_val = model_ann(X_val_batch)
            loss_val = criterion(output_val, y_val_batch)
            total_val_loss += loss_val.item()
    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_loss/len(train_dataloader):.4f}, Val Loss: {total_val_loss/len(val_dataloader):.4f}")






# Run Evaluation
embedding_model="Fast Text"
y_train_pred, y_train_true = Results(model_ann, train_dataloader, criterion,model_name="ANN",type="train", embedding_model=embedding_model, device=device)
y_val_pred,y_val_true = Results(model_ann, val_dataloader, criterion,model_name="ANN",type="validation", embedding_model=embedding_model, device=device)
y_test_pred, y_test_true = Results(model_ann, test_dataloader, criterion,model_name="ANN",embedding_model=embedding_model,type="test", device=device)

TestMultiLabel(y_train_true, y_train_pred,embedding_model=embedding_model,model_name="ANN")
TestMultiLabel(y_val_true, y_val_pred,embedding_model=embedding_model,model_name="ANN")
TestMultiLabel(y_test_true, y_test_pred,embedding_model=embedding_model,model_name="ANN")




Text cell <R1kaJ3HZTWBY>
# %% [markdown]
# using word2vec embeddings

Code cell <bTl5_zX8TWBY>
# %% [code]
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
# Tokenize text into words
train_tokens = X_train.apply(word_tokenize).tolist()

# Train Word2Vec model
word2vec_model = Word2Vec(sentences=train_tokens, vector_size=100, window=5, min_count=2, workers=4)

Code cell <S2cf8g3ETWBY>
# %% [code]
def get_average_word2vec(text, model, vec_size=100):
    words = text.split()
    vectorized_words = [model.wv[word] for word in words if word in model.wv]
    if not vectorized_words:
        return np.zeros(vec_size)
    avg_vector = np.mean(vectorized_words, axis=0)
    return avg_vector

X_train1_word2vec = np.array([get_average_word2vec(text, word2vec_model, 100) for text in X_train1])
X_val1_word2vec = np.array([get_average_word2vec(text, word2vec_model, 100) for text in X_val1])
X_test1_word2vec = np.array([get_average_word2vec(text, word2vec_model, 100) for text in X_test1])

print("Word2Vec representation shape:", X_train1_word2vec.shape)
Execution output
0KB
	Stream
		Word2Vec representation shape: (10486, 100)

Code cell <jPsb-exATWBY>
# %% [code]

X_train_tensor = torch.tensor(X_train1_word2vec, dtype=torch.float32).to(device)
y_train_tensor = torch.tensor(y_train1, dtype=torch.float32).to(device)
X_val_tensor = torch.tensor(X_val1_word2vec, dtype=torch.float32).to(device)
y_val_tensor = torch.tensor(y_val1, dtype=torch.float32).to(device)
X_test_tensor = torch.tensor(X_test1_word2vec, dtype=torch.float32).to(device)
y_test_tensor = torch.tensor(y_test1, dtype=torch.float32).to(device)

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)
model_ann = ANNModel(100, 128, 6).to(device)
criterion = nn.BCELoss()
optimizer = optim.Adam(model_ann.parameters(), lr=0.001)
num_epochs = 10

for epoch in range(num_epochs):
    model_ann.train()
    total_loss = 0
    for X_batch, y_batch in train_dataloader:
        optimizer.zero_grad()
        output = model_ann(X_batch)
        loss = criterion(output, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    model_ann.eval()
    total_val_loss = 0
    with torch.no_grad():
        for X_val_batch, y_val_batch in val_dataloader:
            output_val = model_ann(X_val_batch)
            loss_val = criterion(output_val, y_val_batch)
            total_val_loss += loss_val.item()
    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_loss/len(train_dataloader):.4f}, Val Loss: {total_val_loss/len(val_dataloader):.4f}")

Execution output
1KB
	Stream
		Epoch [1/10], Train Loss: 0.2718, Val Loss: 0.2190
		Epoch [2/10], Train Loss: 0.2123, Val Loss: 0.2096
		Epoch [3/10], Train Loss: 0.2057, Val Loss: 0.2038
		Epoch [4/10], Train Loss: 0.2022, Val Loss: 0.2013
		Epoch [5/10], Train Loss: 0.1994, Val Loss: 0.1998
		Epoch [6/10], Train Loss: 0.1967, Val Loss: 0.2014
		Epoch [7/10], Train Loss: 0.1956, Val Loss: 0.1969
		Epoch [8/10], Train Loss: 0.1942, Val Loss: 0.1960
		Epoch [9/10], Train Loss: 0.1929, Val Loss: 0.1953
		Epoch [10/10], Train Loss: 0.1920, Val Loss: 0.1946

Code cell <SY8pGcO5TWBY>
# %% [code]
# Run Evaluation
embedding_model="Word2Vec"
y_train_pred, y_train_true = Results(model_ann, train_dataloader, criterion,model_name="ANN",type="train", embedding_model=embedding_model, device=device)
y_val_pred,y_val_true = Results(model_ann, val_dataloader, criterion,model_name="ANN",type="validation", embedding_model=embedding_model, device=device)
y_test_pred, y_test_true = Results(model_ann, test_dataloader, criterion,model_name="ANN",embedding_model=embedding_model,type="test", device=device)

TestMultiLabel(y_train_true, y_train_pred,embedding_model=embedding_model,model_name="ANN")
TestMultiLabel(y_val_true, y_val_pred,embedding_model=embedding_model,model_name="ANN")
TestMultiLabel(y_test_true, y_test_pred,embedding_model=embedding_model,model_name="ANN")
Execution output
44KB
	Stream
		ANN with Word2Vec embeddings train loss is 0.1892875649898154
		ANN with Word2Vec embeddings validation loss is 0.19461183074974653
		ANN with Word2Vec embeddings test loss is 0.19355586980502618
		--------------
		Results of ANN with Word2Vec embeddings
		Exact Match Ratio: 0.6607
		Hamming Loss: 0.0776
		Recall: 0.8220
		Precision: 0.8344
		F1 Score: 0.8102
		/home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
		--------------
		--------------
		Results of ANN with Word2Vec embeddings
		Exact Match Ratio: 0.6665
		Hamming Loss: 0.0783
		Recall: 0.8224
		Precision: 0.8353
		F1 Score: 0.8116
		/home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
		--------------
		--------------
		Results of ANN with Word2Vec embeddings
		Exact Match Ratio: 0.6536
		Hamming Loss: 0.0798
		Recall: 0.8158
		Precision: 0.8312
		F1 Score: 0.8052
		/home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
		--------------
	text/plain
		<Figure size 1500x500 with 6 Axes>
		<Figure size 1500x500 with 6 Axes>
		<Figure size 1500x500 with 6 Axes>

Text cell <cNebQ8NXTWBY>
# %% [markdown]
### using FastText Embeddings

Code cell <kuYjdkkUTWBY>
# %% [code]
from gensim.models import FastText
from nltk.tokenize import word_tokenize
# Tokenize text into words
train_tokens = X_train.apply(word_tokenize).tolist()

# Train FastText model
fastText_model = FastText(sentences=train_tokens, vector_size=100, window=5, min_count=2, workers=4)

Code cell <cWoDfVWLAaYa>
# %% [code]
def get_average_fastText(text, model, vec_size=100):
    words = text.split()
    vectorized_words = [model.wv[word] for word in words if word in model.wv]
    if not vectorized_words:
        return np.zeros(vec_size)
    avg_vector = np.mean(vectorized_words, axis=0)
    return avg_vector

X_train1_fastText = np.array([get_average_fastText(text, word2vec_model, 100) for text in X_train1])
X_val1_fastText = np.array([get_average_fastText(text, word2vec_model, 100) for text in X_val1])
X_test1_fastText = np.array([get_average_fastText(text, word2vec_model, 100) for text in X_test1])

print("fast Text representation shape:", X_train1_fastText.shape)
Execution output
0KB
	Stream
		fast Text representation shape: (10486, 100)

Code cell <heeY7u-xTWBZ>
# %% [code]

X_train_tensor = torch.tensor(X_train1_fastText, dtype=torch.float32).to(device)
y_train_tensor = torch.tensor(y_train1, dtype=torch.float32).to(device)
X_val_tensor = torch.tensor(X_val1_fastText, dtype=torch.float32).to(device)
y_val_tensor = torch.tensor(y_val1, dtype=torch.float32).to(device)
X_test_tensor = torch.tensor(X_test1_fastText, dtype=torch.float32).to(device)
y_test_tensor = torch.tensor(y_test1, dtype=torch.float32).to(device)

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)


Code cell <Re8VikdKTWBZ>
# %% [code]
model_ann = ANNModel(100, 128, 6).to(device)
criterion = nn.BCELoss()
optimizer = optim.Adam(model_ann.parameters(), lr=0.001)
num_epochs = 10

for epoch in range(num_epochs):
    model_ann.train()
    total_loss = 0
    for X_batch, y_batch in train_dataloader:
        optimizer.zero_grad()
        output = model_ann(X_batch)
        loss = criterion(output, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    model_ann.eval()
    total_val_loss = 0
    with torch.no_grad():
        for X_val_batch, y_val_batch in val_dataloader:
            output_val = model_ann(X_val_batch)
            loss_val = criterion(output_val, y_val_batch)
            total_val_loss += loss_val.item()
    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_loss/len(train_dataloader):.4f}, Val Loss: {total_val_loss/len(val_dataloader):.4f}")


Execution output
1KB
	Stream
		Epoch [1/10], Train Loss: 0.2780, Val Loss: 0.2219
		Epoch [2/10], Train Loss: 0.2141, Val Loss: 0.2096
		Epoch [3/10], Train Loss: 0.2068, Val Loss: 0.2055
		Epoch [4/10], Train Loss: 0.2030, Val Loss: 0.2019
		Epoch [5/10], Train Loss: 0.2009, Val Loss: 0.2004
		Epoch [6/10], Train Loss: 0.1983, Val Loss: 0.1992
		Epoch [7/10], Train Loss: 0.1966, Val Loss: 0.1992
		Epoch [8/10], Train Loss: 0.1953, Val Loss: 0.1972
		Epoch [9/10], Train Loss: 0.1942, Val Loss: 0.1959
		Epoch [10/10], Train Loss: 0.1928, Val Loss: 0.1948

Code cell <9vxg_OC9TWBZ>
# %% [code]
# Run Evaluation
embedding_model="Fast Text"
y_train_pred, y_train_true = Results(model_ann, train_dataloader, criterion,model_name="ANN",type="train", embedding_model=embedding_model, device=device)
y_val_pred,y_val_true = Results(model_ann, val_dataloader, criterion,model_name="ANN",type="validation", embedding_model=embedding_model, device=device)
y_test_pred, y_test_true = Results(model_ann, test_dataloader, criterion,model_name="ANN",embedding_model=embedding_model,type="test", device=device)

TestMultiLabel(y_train_true, y_train_pred,embedding_model=embedding_model,model_name="ANN")
TestMultiLabel(y_val_true, y_val_pred,embedding_model=embedding_model,model_name="ANN")
TestMultiLabel(y_test_true, y_test_pred,embedding_model=embedding_model,model_name="ANN")
Execution output
44KB
	Stream
		ANN with Fast Text embeddings train loss is 0.19115709624730232
		ANN with Fast Text embeddings validation loss is 0.19480190245491086
		ANN with Fast Text embeddings test loss is 0.19364888947173423
		--------------
		Results of ANN with Fast Text embeddings
		Exact Match Ratio: 0.6598
		Hamming Loss: 0.0781
		Recall: 0.8165
		Precision: 0.8287
		F1 Score: 0.8051
		/home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
		--------------
		--------------
		Results of ANN with Fast Text embeddings
		Exact Match Ratio: 0.6644
		Hamming Loss: 0.0790
		Recall: 0.8138
		Precision: 0.8308
		F1 Score: 0.8053
		/home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
		--------------
		--------------
		Results of ANN with Fast Text embeddings
		Exact Match Ratio: 0.6506
		Hamming Loss: 0.0801
		Recall: 0.8120
		Precision: 0.8290
		F1 Score: 0.8023
		/home/ai21btech11012/ViT-pruning/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.
		  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))
		--------------
	text/plain
		<Figure size 1500x500 with 6 Axes>
		<Figure size 1500x500 with 6 Axes>
		<Figure size 1500x500 with 6 Axes>

Text cell <H2Sc5ji-TWBZ>
# %% [markdown]
# Conclusion

Text cell <8cjbNJu7gPks>
# %% [markdown]
## Dataset - 2

Code cell <ILwbAOeilmOI>
# %% [code]
# Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence


Code cell <HAvfTN6Blo5Q>
# %% [code]
# Download stopwords if not already available
nltk.download('stopwords')
Execution output from Mar 1, 2025 4:15 PM
0KB
	Stream
		[nltk_data] Downloading package stopwords to /root/nltk_data...
		[nltk_data]   Package stopwords is already up-to-date!
	text/plain
		True

Code cell <qm_LdvS_gOKo>
# %% [code]
file_path = 'sample_data/A2D2.xlsx'
df = pd.read_excel(file_path)
df.head()
Execution output from Mar 1, 2025 4:15 PM
26KB
	text/plain
		ID                                            Content         Domain
		0   1  engali Binodiini Ekti Natir Putul Chaalchitro ...  Entertainment
		1   2   ChiefsAholic A Wolf In Chiefs Clothing articl...  Entertainment
		2   3  Kabandha Your Rating Write a review Optional C...  Entertainment
		3   4  In Bruges 2008 R 1h 47m IMDb RATING 79 10 474K...  Entertainment
		4   5  Men in Black 2012 PG13 1h 46m IMDb RATING 68 1...  Entertainment

Code cell <RD2b8ayFgUBo>
# %% [code]
# Check for missing values,  class imbalance, and other issues.
print(df.info())
Execution output from Mar 1, 2025 4:16 PM
0KB
	Stream
		<class 'pandas.core.frame.DataFrame'>
		RangeIndex: 3927 entries, 0 to 3926
		Data columns (total 3 columns):
		 #   Column   Non-Null Count  Dtype 
		---  ------   --------------  ----- 
		 0   ID       3927 non-null   int64 
		 1   Content  3927 non-null   object
		 2   Domain   3927 non-null   object
		dtypes: int64(1), object(2)
		memory usage: 92.2+ KB
		None

Code cell <LjzxnhwogWod>
# %% [code]
# Check the unique labels and their counts
df['Domain'].value_counts()
Execution output from Mar 1, 2025 4:16 PM
1KB
	text/plain
		Domain
		Technology       1474
		Healthcare        913
		Entertainment     636
		Tourism           562
		Sports            342
		Name: count, dtype: int64

Text cell <XPoJFhFigoM2>
# %% [markdown]
## Data Preprocessing
Now that we know this is a multi-class classification problem and NOT multi-label classification problem, we :-
1. Use the process_text function defined above for processing text
2. Create and encode labels
3. Split data into train-val-test sets

Code cell <0V8s9m-zgrZk>
# %% [code]
def process_text(text):
    if pd.isna(text):
        return ''
    text = text.lower()
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)  # Remove special characters

    # Remove stop words
    stop_words = set(stopwords.words('english'))
    words = text.split()
    words = [word for word in words if word not in stop_words]

    new_processed_text = ' '.join(words)
    return new_processed_text

Code cell <faHmXwPogtmV>
# %% [code]
df["Processed_Content"] = df["Content"].apply(process_text)
df.head()
Execution output from Mar 1, 2025 4:17 PM
36KB
	text/plain
		ID                                            Content         Domain  \
		0   1  engali Binodiini Ekti Natir Putul Chaalchitro ...  Entertainment   
		1   2   ChiefsAholic A Wolf In Chiefs Clothing articl...  Entertainment   
		2   3  Kabandha Your Rating Write a review Optional C...  Entertainment   
		3   4  In Bruges 2008 R 1h 47m IMDb RATING 79 10 474K...  Entertainment   
		4   5  Men in Black 2012 PG13 1h 46m IMDb RATING 68 1...  Entertainment   
		
		                                   Processed_Content  
		0  engali binodiini ekti natir putul chaalchitro ...  
		1  chiefsaholic wolf chiefs clothing articleshowc...  
		2  kabandha rating write review optional characte...  
		3  bruges 2008 r 1h 47m imdb rating 79 10 474k ra...  
		4  men black 2012 pg13 1h 46m imdb rating 68 10 3...

Code cell <T8IzrGYcgvgA>
# %% [code]
df["Label"] = df["Domain"].astype("category").cat.codes
label_mapping = dict(enumerate(df["Domain"].astype("category").cat.categories))
print(label_mapping)
Execution output from Mar 1, 2025 4:17 PM
0KB
	Stream
		{0: 'Entertainment', 1: 'Healthcare', 2: 'Sports', 3: 'Technology', 4: 'Tourism'}

Code cell <9Wvyk7aJgxSP>
# %% [code]
# Split into Train and Test sets
train_texts, val_and_test_texts, train_labels, val_and_test_labels = train_test_split(
    df["Processed_Content"], df["Label"], test_size=0.5, random_state=42, stratify=df["Label"]
)

val_texts, test_texts, val_labels, test_labels = train_test_split(
    val_and_test_texts, val_and_test_labels, test_size=0.4, random_state=42, stratify=val_and_test_labels
)

Code cell <1FZmaBx_gy-C>
# %% [code]
train_df = pd.DataFrame({"text": train_texts, "label": train_labels})
val_df = pd.DataFrame({"text": val_texts, "label": val_labels})
test_df = pd.DataFrame({"text": test_texts, "label": test_labels})

print(f"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}")
Execution output from Mar 1, 2025 4:17 PM
0KB
	Stream
		Train size: 1963, Validation size: 1178, Test size: 786

Text cell <ojm-CZ7rg4FZ>
# %% [markdown]
## Feature extraction/Representation
Convert text data (titles and abstracts) into numerical representations
1. Bag of Words (BoW)
2. TF-IDF (Term Frequency-Inverse Document Frequency)
3. Word Embeddings (Word2Vec, FastText)

- We will be using **TF-IDF** since it is known to work well with text classification using *Classical ML models*.
- Among word embeddings, we would experiment with **Word2Vec** and **FastText** for *Deep Learning models*.

Code cell <anfR2IPxTWBb>
# %% [code]
y_test=test_df["label"].values
y_train=train_df["label"].values
y_val=val_df["label"].values

Code cell <UBzcBxncWNNX>
# %% [code]
nltk.download('punkt_tab')
Execution output from Mar 1, 2025 4:25 PM
0KB
	Stream
		[nltk_data] Downloading package punkt_tab to /root/nltk_data...
		[nltk_data]   Unzipping tokenizers/punkt_tab.zip.
	text/plain
		True

Code cell <v9vhaW3kUV-Z>
# %% [code]
# 1. Use BoW representation
from sklearn.feature_extraction.text import CountVectorizer

bow_vectorizer = CountVectorizer(max_features=5000)  # Keep only top 5000 words

X_train_bow = bow_vectorizer.fit_transform(train_df["text"])
X_val_bow = bow_vectorizer.transform(val_df["text"])
X_test_bow = bow_vectorizer.transform(test_df["text"])

print("BoW representation shape:", X_train_bow.shape)
Execution output from Mar 1, 2025 4:17 PM
0KB
	Stream
		BoW representation shape: (1963, 5000)

Code cell <Pmhf_URTg7Jw>
# %% [code]
# 2. TF-IDF
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words="english")

X_train_tfidf = tfidf_vectorizer.fit_transform(train_df["text"])
X_val_tfidf = tfidf_vectorizer.transform(val_df["text"])
X_test_tfidf = tfidf_vectorizer.transform(test_df["text"])

# Print shape to confirm
print("TF-IDF Shape:", X_train_tfidf.shape)
Execution output from Mar 1, 2025 4:25 PM
0KB
	Stream
		TF-IDF Shape: (1963, 5000)

Code cell <QYl84Qr_VaIE>
# %% [code]
from gensim.models import Word2Vec

# Tokenize text into words
train_tokens = train_df["text"].apply(word_tokenize).tolist()

# Train Word2Vec model
word2vec_model = Word2Vec(sentences=train_tokens, vector_size=100, window=5, min_count=2, workers=4)

from gensim.models import FastText

# Train FastText model
fasttext_model = FastText(sentences=train_tokens, vector_size=100, window=5, min_count=2, workers=4)

Code cell <pz_W7ZXeTWBb>
# %% [code]
# # Evaluate the model
# import numpy as np
# import seaborn as sns
# import matplotlib.pyplot as plt
# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# def EvalMetricsMultiClass(y_true, y_pred, num_classes):
#     acc = accuracy_score(y_true, y_pred)
#     class_report = classification_report(y_true, y_pred, target_names=[f"Class {i}" for i in range(num_classes)], output_dict=True)
#     cm = confusion_matrix(y_true, y_pred)
#     # print(class_report)

#     print(f"Accuracy: {acc:.4f}")
#     print("\nClassification Report:")
#     for i in range(num_classes):
#         idx=f'Class {i}'
#         # print(class_report[idx])

#         print(f"Class {i}: Precision={class_report[f'Class {i}']['precision']:.2f}, Recall={class_report[f'Class {i}']['recall']:.2f}, F1-score={class_report[f'Class {i}']['f1-score']:.2f}")

#     # Confusion Matrix Heatmap
#     plt.figure(figsize=(5,4))
#     sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=[f"Class {i}" for i in range(num_classes)], yticklabels=[f"Class {i}" for i in range(num_classes)])
#     plt.xlabel("Predicted Label")
#     plt.ylabel("True Label")
#     plt.title("Confusion Matrix")
#     plt.show()

#     # Per-Class Precision, Recall, F1 Score Bar Chart
#     metrics = ["precision", "recall", "f1-score"]
#     values = {metric: [class_report[f"Class {i}"][metric] for i in range(num_classes)] for metric in metrics}

#     x = np.arange(num_classes)
#     width = 0.2
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

def EvalMetricsMultiClass(y_true, y_pred,classes=5):
    class_names = ['Entertainment', 'Healthcare', 'Sports', 'Technology', 'Tourism']

    accuracy = accuracy_score(y_true, y_pred)
    class_report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)
    cm = confusion_matrix(y_true, y_pred)

    precision = class_report['macro avg']['precision']
    recall = class_report['macro avg']['recall']
    f1 = class_report['macro avg']['f1-score']

    print(f"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}")

    print("\nClassification Report:")
    for class_name in class_names:
        metrics = class_report[class_name]
        print(f"{class_name}: Precision={metrics['precision']:.2f}, Recall={metrics['recall']:.2f}, F1-score={metrics['f1-score']:.2f}")

    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title("Confusion Matrix")
    plt.show()


Code cell <SpfdGmdLXe1W>
# %% [code]
def get_average_word2vec(text, model, vector_size=100):
    """
        Convert text into a single averaged word vector.
        Used as input to classical ML models which expects fixed length input

        Args:
        - text (str): Input sentence or document.
        - model (Word2Vec): Trained Word2Vec model.
        - vector_size (int): Size of word vectors.

        Returns:
        - numpy array of shape (vector_size,)
                                                                                """
    tokens = word_tokenize(text)  # Tokenize text
    vectors = [model.wv[word] for word in tokens if word in model.wv]  # Get vectors if word exists
    return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)  # Average or return zero vector



def get_word_embeddings(text, model, vector_size=100):
    """
    Convert text into a sequence of word embeddings.

        Args:
        - text (str): Input sentence or document.
        - model (Word2Vec): Trained Word2Vec model.
        - vector_size (int): Size of word vectors.

        Returns:
        - List of numpy arrays (each of shape (vector_size,))
                                                                                                    """
    tokens = word_tokenize(text)
    embeddings = [model.wv[word] if word in model.wv else np.zeros(vector_size) for word in tokens]
    return embeddings

def get_average_word2vec(text, model, vec_size=100):
    words = text.split()
    vectorized_words = [model.wv[word] for word in words if word in model.wv]
    if not vectorized_words:
        return np.zeros(vec_size)
    avg_vector = np.mean(vectorized_words, axis=0)
    return avg_vector

X_train2_w2v = np.array([get_average_word2vec(text, word2vec_model, 100) for text in train_texts])
X_val2_w2v = np.array([get_average_word2vec(text, word2vec_model, 100) for text in val_texts])
X_test2_w2v = np.array([get_average_word2vec(text, word2vec_model, 100) for text in test_texts])

print("Word2Vec representation shape:", X_train2_w2v.shape)

def get_average_fasttext(text, model, vec_size=100):
    words = text.split()
    vectorized_words = [model.wv[word] for word in words if word in model.wv]
    if not vectorized_words:
        return np.zeros(vec_size)
    avg_vector = np.mean(vectorized_words, axis=0)
    return avg_vector

X_train2_fasttext = np.array([get_average_fasttext(text, fasttext_model, 100) for text in train_texts])
X_val2_fasttext = np.array([get_average_fasttext(text, fasttext_model, 100) for text in val_texts])
X_test2_fasttext = np.array([get_average_fasttext(text, fasttext_model, 100) for text in test_texts])

print("FastText representation shape:", X_train2_fasttext.shape)
Execution output from Mar 1, 2025 4:31 PM
0KB
	Stream
		Word2Vec representation shape: (1963, 100)
		FastText representation shape: (1963, 100)

Text cell <DI7hOnXtTWBb>
# %% [markdown]
# Classical Machine Learning Approach

Text cell <CeAetv6KTWBb>
# %% [markdown]
# Model 1 : Support Vector Classifier (SVC)


Text cell <F9kvcauaUnbL>
# %% [markdown]
### (i) BoW embeddings + SVC

Code cell <8_bbpv6HUkL1>
# %% [code]
import numpy as np
import pandas as pd
from sklearn.svm import SVC

class SVCModel():
    def __init__(self, X_train, y_train):
        self.model = SVC(kernel='linear')
        self.X_train = X_train
        self.y_train = y_train

    def fit(self):
        self.model.fit(self.X_train, self.y_train)

    def predict(self, X_test):
        self.y_output = self.model.predict(X_test)
        return self.y_output

    def test(self, y_test,type='test'):
        print(f"SVC {type} accuracy:", accuracy_score(y_test, self.y_output))

Code cell <MDIPEVlcUsMa>
# %% [code]

model_name='SVC'
embedding_model='Bow'


print('----------')
print(f'{model_name} with {embedding_model} embeddings')
svcModel=SVCModel(X_train_bow, y_train)
svcModel.fit()
y_svc=svcModel.predict(X_train_bow)
svcModel.test(y_train,'train')
y_svc=svcModel.predict(X_val_bow)
svcModel.test(y_val,'validation')
y_svc=svcModel.predict(X_test_bow)
svcModel.test(y_test,'test')
EvalMetricsMultiClass(y_test, y_svc, 5)
print('----------')
Execution output from Mar 1, 2025 4:19 PM
42KB
	Stream
		----------
		SVC with Bow embeddings
		SVC train accuracy: 1.0
		SVC validation accuracy: 0.9855687606112055
		SVC test accuracy: 0.9809160305343512
		Accuracy: 0.9809, Precision: 0.9833, Recall: 0.9845, F1-Score: 0.9838
		
		Classification Report:
		Entertainment: Precision=1.00, Recall=1.00, F1-score=1.00
		Healthcare: Precision=0.96, Recall=0.97, F1-score=0.96
		Sports: Precision=0.99, Recall=1.00, F1-score=0.99
		Technology: Precision=0.98, Recall=0.98, F1-score=0.98
		Tourism: Precision=0.99, Recall=0.97, F1-score=0.98
		----------
	text/plain
		<Figure size 500x400 with 2 Axes>

Text cell <WCDSCZ-VUv9B>
# %% [markdown]
### (ii) TF-IDF embeddings + SVC

Code cell <-PiK7lByUvSJ>
# %% [code]
model_name='SVC'
embedding_model='TF-IDF'

print('----------')
print(f'{model_name} with {embedding_model} embeddings')
svcModel=SVCModel(X_train_tfidf, y_train)
svcModel.fit()
y_svc=svcModel.predict(X_train_tfidf)
svcModel.test(y_train,'train')
y_svc=svcModel.predict(X_val_tfidf)
svcModel.test(y_val,'validation')
y_svc=svcModel.predict(X_test_tfidf)
svcModel.test(y_test,'test')
EvalMetricsMultiClass(y_test, y_svc, 5)
print('----------')
Execution output from Mar 1, 2025 4:19 PM
40KB
	Stream
		----------
		SVC with TF-IDF embeddings
		SVC train accuracy: 1.0
		SVC validation accuracy: 0.9991511035653651
		SVC test accuracy: 1.0
		Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-Score: 1.0000
		
		Classification Report:
		Entertainment: Precision=1.00, Recall=1.00, F1-score=1.00
		Healthcare: Precision=1.00, Recall=1.00, F1-score=1.00
		Sports: Precision=1.00, Recall=1.00, F1-score=1.00
		Technology: Precision=1.00, Recall=1.00, F1-score=1.00
		Tourism: Precision=1.00, Recall=1.00, F1-score=1.00
		----------
	text/plain
		<Figure size 500x400 with 2 Axes>

Text cell <AD1fYgP6TWBc>
# %% [markdown]
# Deep Learning Approach

Text cell <ePhnW-XvTWBc>
# %% [markdown]
#### Here we use Word2Vec and FastText Embeddings

Code cell <OhX6RwcWWu-->
# %% [code]
# Model 2: Feed Forward Neural Network

Text cell <byQT_8xEXObS>
# %% [markdown]
# Model 2: Feed Forward Neural Network


Code cell <7IOJdAWZXn_5>
# %% [code]
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

class ANNModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ANNModel, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.sigmoid(x)
        return x

Text cell <oe7rSFjnXRQ9>
# %% [markdown]
### (i) word2vec embeddings

Code cell <jonRVnAAXPty>
# %% [code]

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

X_train_tensor = torch.tensor(X_train2_w2v, dtype=torch.float32).to(device)
y_train_tensor = torch.tensor(train_labels.values, dtype=torch.float32).to(device)
X_val_tensor = torch.tensor(X_val2_w2v, dtype=torch.float32).to(device)
y_val_tensor = torch.tensor(val_labels.values, dtype=torch.float32).to(device)
X_test_tensor = torch.tensor(X_test2_w2v, dtype=torch.float32).to(device)
y_test_tensor = torch.tensor(test_labels.values, dtype=torch.float32).to(device)

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)


Code cell <e4-a5CVqhAWX>
# %% [code]
model = ANNModel(100, 128, 5)
model = model.to('cuda:0')
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
num_epochs = 10

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    correct_train = 0
    total_train = 0
    for X_batch, y_batch in train_dataloader:
        optimizer.zero_grad()
        X_batch, y_batch = X_batch.to('cuda:0'), y_batch.to('cuda:0')
        output = model(X_batch)
        y_batch = y_batch.long()
        loss = criterion(output, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        _, predicted = torch.max(output, 1)
        correct_train += (predicted == y_batch).sum().item()
        total_train += y_batch.size(0)

    train_accuracy = correct_train / total_train

    model.eval()
    total_val_loss = 0
    correct_val = 0
    total_val = 0
    with torch.no_grad():
        for X_val_batch, y_val_batch in val_dataloader:
            X_val_batch, y_val_batch = X_val_batch.to('cuda:0'), y_val_batch.to('cuda:0')
            output_val = model(X_val_batch)
            y_val_batch = y_val_batch.long()
            loss_val = criterion(output_val, y_val_batch)
            total_val_loss += loss_val.item()
            _, output = torch.max(output_val, 1)
            correct_val += (output == y_val_batch).sum().item()
            total_val += y_val_batch.size(0)

    val_accuracy = correct_val / total_val

    print(f"Epoch [{epoch+1}/{num_epochs}], Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}")

model.eval()
correct_test = 0
total_test = 0
with torch.no_grad():
    for X_test_batch, y_test_batch in test_dataloader:
        X_test_batch, y_test_batch = X_test_batch.to('cuda:0'), y_test_batch.to('cuda:0')
        y_test_batch = y_test_batch.long()
        output_test = model(X_test_batch)
        _, predicted_test = torch.max(output_test, 1)
        correct_test += (predicted_test == y_test_batch).sum().item()
        total_test += y_test_batch.size(0)

test_accuracy = correct_test / total_test
print(f"Test Accuracy: {test_accuracy:.4f}")

Execution output from Mar 1, 2025 4:32 PM
1KB
	Stream
		Epoch [1/10], Train Acc: 0.7692, Val Acc: 0.8973
		Epoch [2/10], Train Acc: 0.9659, Val Acc: 0.9898
		Epoch [3/10], Train Acc: 0.9893, Val Acc: 0.9907
		Epoch [4/10], Train Acc: 0.9903, Val Acc: 0.9924
		Epoch [5/10], Train Acc: 0.9908, Val Acc: 0.9932
		Epoch [6/10], Train Acc: 0.9924, Val Acc: 0.9941
		Epoch [7/10], Train Acc: 0.9944, Val Acc: 0.9958
		Epoch [8/10], Train Acc: 0.9949, Val Acc: 0.9958
		Epoch [9/10], Train Acc: 0.9954, Val Acc: 0.9958
		Epoch [10/10], Train Acc: 0.9964, Val Acc: 0.9958
		Test Accuracy: 0.9962

Code cell <5s3LjrRNXyRy>
# %% [code]
# evaluating model
y_true = []
y_pred = []

with torch.no_grad():
    for X_test_batch, y_test_batch in test_dataloader:
        X_test_batch, y_test_batch = X_test_batch.to('cuda:0'), y_test_batch.to('cuda:0')
        y_test_batch = y_test_batch.long()

        output_test = model(X_test_batch)
        _, predicted_test = torch.max(output_test, 1)

        y_true.extend(y_test_batch.cpu().numpy())
        y_pred.extend(predicted_test.cpu().numpy())

y_true = np.array(y_true)
y_pred = np.array(y_pred)

EvalMetricsMultiClass(y_true, y_pred,5)
Execution output from Mar 1, 2025 4:32 PM
40KB
	Stream
		Accuracy: 0.9962, Precision: 0.9971, Recall: 0.9971, F1-Score: 0.9971
		
		Classification Report:
		Entertainment: Precision=0.99, Recall=1.00, F1-score=1.00
		Healthcare: Precision=1.00, Recall=0.99, F1-score=0.99
		Sports: Precision=1.00, Recall=1.00, F1-score=1.00
		Technology: Precision=0.99, Recall=1.00, F1-score=0.99
		Tourism: Precision=1.00, Recall=1.00, F1-score=1.00
	text/plain
		<Figure size 500x400 with 2 Axes>

Text cell <ginbY0rTX1UU>
# %% [markdown]
### (ii) FastText Embedding

Code cell <sN7rCuJgX2DH>
# %% [code]

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

X_train_tensor = torch.tensor(X_train2_fasttext, dtype=torch.float32).to(device)
y_train_tensor = torch.tensor(train_labels.values, dtype=torch.float32).to(device)
X_val_tensor = torch.tensor(X_val2_fasttext, dtype=torch.float32).to(device)
y_val_tensor = torch.tensor(val_labels.values, dtype=torch.float32).to(device)
X_test_tensor = torch.tensor(X_test2_fasttext, dtype=torch.float32).to(device)
y_test_tensor = torch.tensor(test_labels.values, dtype=torch.float32).to(device)

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)


Code cell <U-O-BAG_X4kH>
# %% [code]
model = ANNModel(100, 128, 5)
model = model.to('cuda:0')
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
num_epochs = 10

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    correct_train = 0
    total_train = 0
    for X_batch, y_batch in train_dataloader:
        optimizer.zero_grad()
        X_batch, y_batch = X_batch.to('cuda:0'), y_batch.to('cuda:0')
        output = model(X_batch)
        y_batch = y_batch.long()
        loss = criterion(output, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        _, predicted = torch.max(output, 1)
        correct_train += (predicted == y_batch).sum().item()
        total_train += y_batch.size(0)

    train_accuracy = correct_train / total_train

    model.eval()
    total_val_loss = 0
    correct_val = 0
    total_val = 0
    with torch.no_grad():
        for X_val_batch, y_val_batch in val_dataloader:
            X_val_batch, y_val_batch = X_val_batch.to('cuda:0'), y_val_batch.to('cuda:0')
            output_val = model(X_val_batch)
            y_val_batch = y_val_batch.long()
            loss_val = criterion(output_val, y_val_batch)
            total_val_loss += loss_val.item()
            _, output = torch.max(output_val, 1)
            correct_val += (output == y_val_batch).sum().item()
            total_val += y_val_batch.size(0)

    val_accuracy = correct_val / total_val

    print(f"Epoch [{epoch+1}/{num_epochs}], Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}")

model.eval()
correct_test = 0
total_test = 0
with torch.no_grad():
    for X_test_batch, y_test_batch in test_dataloader:
        X_test_batch, y_test_batch = X_test_batch.to('cuda:0'), y_test_batch.to('cuda:0')
        y_test_batch = y_test_batch.long()
        output_test = model(X_test_batch)
        _, predicted_test = torch.max(output_test, 1)
        correct_test += (predicted_test == y_test_batch).sum().item()
        total_test += y_test_batch.size(0)

test_accuracy = correct_test / total_test
print(f"Test Accuracy: {test_accuracy:.4f}")

Execution output from Mar 1, 2025 4:33 PM
1KB
	Stream
		Epoch [1/10], Train Acc: 0.7876, Val Acc: 0.9550
		Epoch [2/10], Train Acc: 0.9827, Val Acc: 0.9873
		Epoch [3/10], Train Acc: 0.9883, Val Acc: 0.9873
		Epoch [4/10], Train Acc: 0.9888, Val Acc: 0.9890
		Epoch [5/10], Train Acc: 0.9903, Val Acc: 0.9898
		Epoch [6/10], Train Acc: 0.9918, Val Acc: 0.9924
		Epoch [7/10], Train Acc: 0.9929, Val Acc: 0.9915
		Epoch [8/10], Train Acc: 0.9939, Val Acc: 0.9924
		Epoch [9/10], Train Acc: 0.9939, Val Acc: 0.9958
		Epoch [10/10], Train Acc: 0.9944, Val Acc: 0.9949
		Test Accuracy: 0.9924

Code cell <45so-RV7X_jZ>
# %% [code]
# evaluating model
y_true = []
y_pred = []

with torch.no_grad():
    for X_test_batch, y_test_batch in test_dataloader:
        X_test_batch, y_test_batch = X_test_batch.to('cuda:0'), y_test_batch.to('cuda:0')
        y_test_batch = y_test_batch.long()

        output_test = model(X_test_batch)
        _, predicted_test = torch.max(output_test, 1)

        y_true.extend(y_test_batch.cpu().numpy())
        y_pred.extend(predicted_test.cpu().numpy())

y_true = np.array(y_true)
y_pred = np.array(y_pred)

EvalMetricsMultiClass(y_true, y_pred, 5)
Execution output from Mar 1, 2025 4:33 PM
41KB
	Stream
		Accuracy: 0.9924, Precision: 0.9925, Recall: 0.9902, F1-Score: 0.9912
		
		Classification Report:
		Entertainment: Precision=0.97, Recall=1.00, F1-score=0.98
		Healthcare: Precision=1.00, Recall=0.98, F1-score=0.99
		Sports: Precision=1.00, Recall=0.97, F1-score=0.99
		Technology: Precision=0.99, Recall=1.00, F1-score=0.99
		Tourism: Precision=1.00, Recall=1.00, F1-score=1.00
	text/plain
		<Figure size 500x400 with 2 Axes>

Code cell <QcjE76beTWBc>
# %% [code]
# !pip uninstall numpy gensim
# !pip install numpy gensim


Code cell <grhHqbtlhH2w>
# %% [code]
# from gensim.models import Word2Vec

# # Tokenize text into words
# train_tokens = train_df["text"].apply(word_tokenize).tolist()

# # Train Word2Vec model
# word2vec_model = Word2Vec(sentences=train_tokens, vector_size=100, window=5, min_count=2, workers=4)

Code cell <QteBW7OChJ_1>
# %% [code]
# from gensim.models import FastText

# # Train FastText model
# fasttext_model = FastText(sentences=train_tokens, vector_size=100, window=5, min_count=2, workers=4)

Text cell <UT0or_xbTWBd>
# %% [markdown]
# Model 3: Feed Forward Neural Network
### averaged sentence word2vec embeddings

Code cell <ZyDqobwUkjrq>
# %% [code]
# import torch
# import torch.nn as nn
# import torch.optim as optim
# from torch.utils.data import DataLoader, TensorDataset

# class ANNModel(nn.Module):
#     def __init__(self, input_size, hidden_size, output_size):
#         super(ANNModel, self).__init__()
#         self.fc1 = nn.Linear(input_size, hidden_size)
#         self.relu = nn.ReLU()
#         self.fc2 = nn.Linear(hidden_size, output_size)
#         self.sigmoid = nn.Sigmoid()

#     def forward(self, x):
#         x = self.fc1(x)
#         x = self.relu(x)
#         x = self.fc2(x)
#         x = self.sigmoid(x)
#         return x

Code cell <LOT91wBLo5LH>
# %% [code]
# def get_average_word2vec(text, model, vec_size=100):
#     words = text.split()
#     vectorized_words = [model.wv[word] for word in words if word in model.wv]
#     if not vectorized_words:
#         return np.zeros(vec_size)
#     avg_vector = np.mean(vectorized_words, axis=0)
#     return avg_vector

# X_train2_w2v = np.array([get_average_word2vec(text, word2vec_model, 100) for text in train_texts])
# X_val2_w2v = np.array([get_average_word2vec(text, word2vec_model, 100) for text in val_texts])
# X_test2_w2v = np.array([get_average_word2vec(text, word2vec_model, 100) for text in test_texts])

# print("Word2Vec representation shape:", X_train2_w2v.shape)
Execution output
0KB
	Stream
		Word2Vec representation shape: (1963, 100)

Code cell <2Z1QlxCKoAGh>
# %% [code]

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# X_train_tensor = torch.tensor(X_train2_w2v, dtype=torch.float32).to(device)
# y_train_tensor = torch.tensor(train_labels.values, dtype=torch.float32).to(device)
# X_val_tensor = torch.tensor(X_val2_w2v, dtype=torch.float32).to(device)
# y_val_tensor = torch.tensor(val_labels.values, dtype=torch.float32).to(device)
# X_test_tensor = torch.tensor(X_test2_w2v, dtype=torch.float32).to(device)
# y_test_tensor = torch.tensor(test_labels.values, dtype=torch.float32).to(device)

# train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
# val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
# val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
# test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)


Code cell <6Lxl1uHCTWBd>
# %% [code]
# model = ANNModel(100, 128, 5)
# model = model.to('cuda:0')
# criterion = nn.CrossEntropyLoss()
# optimizer = optim.Adam(model.parameters(), lr=0.001)
# num_epochs = 10

# for epoch in range(num_epochs):
#     model.train()
#     total_loss = 0
#     correct_train = 0
#     total_train = 0
#     for X_batch, y_batch in train_dataloader:
#         optimizer.zero_grad()
#         X_batch, y_batch = X_batch.to('cuda:0'), y_batch.to('cuda:0')
#         output = model(X_batch)
#         y_batch = y_batch.long()
#         loss = criterion(output, y_batch)
#         loss.backward()
#         optimizer.step()
#         total_loss += loss.item()
#         _, predicted = torch.max(output, 1)
#         correct_train += (predicted == y_batch).sum().item()
#         total_train += y_batch.size(0)

#     train_accuracy = correct_train / total_train

#     model.eval()
#     total_val_loss = 0
#     correct_val = 0
#     total_val = 0
#     with torch.no_grad():
#         for X_val_batch, y_val_batch in val_dataloader:
#             X_val_batch, y_val_batch = X_val_batch.to('cuda:0'), y_val_batch.to('cuda:0')
#             output_val = model(X_val_batch)
#             y_val_batch = y_val_batch.long()
#             loss_val = criterion(output_val, y_val_batch)
#             total_val_loss += loss_val.item()
#             _, output = torch.max(output_val, 1)
#             correct_val += (output == y_val_batch).sum().item()
#             total_val += y_val_batch.size(0)

#     val_accuracy = correct_val / total_val

#     print(f"Epoch [{epoch+1}/{num_epochs}], Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}")

# model.eval()
# correct_test = 0
# total_test = 0
# with torch.no_grad():
#     for X_test_batch, y_test_batch in test_dataloader:
#         X_test_batch, y_test_batch = X_test_batch.to('cuda:0'), y_test_batch.to('cuda:0')
#         y_test_batch = y_test_batch.long()
#         output_test = model(X_test_batch)
#         _, predicted_test = torch.max(output_test, 1)
#         correct_test += (predicted_test == y_test_batch).sum().item()
#         total_test += y_test_batch.size(0)

# test_accuracy = correct_test / total_test
# print(f"Test Accuracy: {test_accuracy:.4f}")

Execution output
1KB
	Stream
		Epoch [1/10], Train Acc: 0.7402, Val Acc: 0.9083
		Epoch [2/10], Train Acc: 0.9679, Val Acc: 0.9915
		Epoch [3/10], Train Acc: 0.9913, Val Acc: 0.9941
		Epoch [4/10], Train Acc: 0.9913, Val Acc: 0.9924
		Epoch [5/10], Train Acc: 0.9918, Val Acc: 0.9958
		Epoch [6/10], Train Acc: 0.9944, Val Acc: 0.9958
		Epoch [7/10], Train Acc: 0.9944, Val Acc: 0.9949
		Epoch [8/10], Train Acc: 0.9944, Val Acc: 0.9966
		Epoch [9/10], Train Acc: 0.9964, Val Acc: 0.9966
		Epoch [10/10], Train Acc: 0.9969, Val Acc: 0.9966
		Test Accuracy: 0.9962

Code cell <MoXcH6X4TWBd>
# %% [code]
# # evaluating model
# y_true = []
# y_pred = []

# with torch.no_grad():
#     for X_test_batch, y_test_batch in test_dataloader:
#         X_test_batch, y_test_batch = X_test_batch.to('cuda:0'), y_test_batch.to('cuda:0')
#         y_test_batch = y_test_batch.long()

#         output_test = model(X_test_batch)
#         _, predicted_test = torch.max(output_test, 1)

#         y_true.extend(y_test_batch.cpu().numpy())
#         y_pred.extend(predicted_test.cpu().numpy())

# y_true = np.array(y_true)
# y_pred = np.array(y_pred)

# EvalMetricsMultiClass(y_true, y_pred, num_classes=5)
Execution output
32KB
	Stream
		Accuracy: 0.9962
		
		Classification Report:
		Class 0: Precision=0.99, Recall=1.00, F1-score=1.00
		Class 1: Precision=1.00, Recall=0.99, F1-score=0.99
		Class 2: Precision=1.00, Recall=1.00, F1-score=1.00
		Class 3: Precision=0.99, Recall=1.00, F1-score=0.99
		Class 4: Precision=1.00, Recall=1.00, F1-score=1.00
	text/plain
		<Figure size 500x400 with 2 Axes>

Text cell <7VPmWwktTWBd>
# %% [markdown]
### Ann+Fasttext

Code cell <xK87bSzkTWBd>
# %% [code]
# import torch
# import torch.nn as nn
# import torch.optim as optim
# from torch.utils.data import DataLoader, TensorDataset

# class ANNModel(nn.Module):
#     def __init__(self, input_size, hidden_size, output_size):
#         super(ANNModel, self).__init__()
#         self.fc1 = nn.Linear(input_size, hidden_size)
#         self.relu = nn.ReLU()
#         self.fc2 = nn.Linear(hidden_size, output_size)
#         self.sigmoid = nn.Sigmoid()

#     def forward(self, x):
#         x = self.fc1(x)
#         x = self.relu(x)
#         x = self.fc2(x)
#         x = self.sigmoid(x)
#         return x

Code cell <7WfpxoxETWBd>
# %% [code]
# def get_average_fasttext(text, model, vec_size=100):
#     words = text.split()
#     vectorized_words = [model.wv[word] for word in words if word in model.wv]
#     if not vectorized_words:
#         return np.zeros(vec_size)
#     avg_vector = np.mean(vectorized_words, axis=0)
#     return avg_vector

# X_train2_fasttext = np.array([get_average_fasttext(text, fasttext_model, 100) for text in train_texts])
# X_val2_fasttext = np.array([get_average_fasttext(text, fasttext_model, 100) for text in val_texts])
# X_test2_fasttext = np.array([get_average_fasttext(text, fasttext_model, 100) for text in test_texts])

# print("Word2Vec representation shape:", X_train2_fasttext.shape)
Execution output
0KB
	Stream
		Word2Vec representation shape: (1963, 100)

Code cell <Kz_SiOJcTWBe>
# %% [code]

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# X_train_tensor = torch.tensor(X_train2_fasttext, dtype=torch.float32).to(device)
# y_train_tensor = torch.tensor(train_labels.values, dtype=torch.float32).to(device)
# X_val_tensor = torch.tensor(X_val2_fasttext, dtype=torch.float32).to(device)
# y_val_tensor = torch.tensor(val_labels.values, dtype=torch.float32).to(device)
# X_test_tensor = torch.tensor(X_test2_fasttext, dtype=torch.float32).to(device)
# y_test_tensor = torch.tensor(test_labels.values, dtype=torch.float32).to(device)

# train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
# val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
# val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)
# test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
# test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)


Code cell <ukkv_AalTWBe>
# %% [code]
# model = ANNModel(100, 128, 5)
# model = model.to('cuda:0')
# criterion = nn.CrossEntropyLoss()
# optimizer = optim.Adam(model.parameters(), lr=0.001)
# num_epochs = 10

# for epoch in range(num_epochs):
#     model.train()
#     total_loss = 0
#     correct_train = 0
#     total_train = 0
#     for X_batch, y_batch in train_dataloader:
#         optimizer.zero_grad()
#         X_batch, y_batch = X_batch.to('cuda:0'), y_batch.to('cuda:0')
#         output = model(X_batch)
#         y_batch = y_batch.long()
#         loss = criterion(output, y_batch)
#         loss.backward()
#         optimizer.step()
#         total_loss += loss.item()
#         _, predicted = torch.max(output, 1)
#         correct_train += (predicted == y_batch).sum().item()
#         total_train += y_batch.size(0)

#     train_accuracy = correct_train / total_train

#     model.eval()
#     total_val_loss = 0
#     correct_val = 0
#     total_val = 0
#     with torch.no_grad():
#         for X_val_batch, y_val_batch in val_dataloader:
#             X_val_batch, y_val_batch = X_val_batch.to('cuda:0'), y_val_batch.to('cuda:0')
#             output_val = model(X_val_batch)
#             y_val_batch = y_val_batch.long()
#             loss_val = criterion(output_val, y_val_batch)
#             total_val_loss += loss_val.item()
#             _, output = torch.max(output_val, 1)
#             correct_val += (output == y_val_batch).sum().item()
#             total_val += y_val_batch.size(0)

#     val_accuracy = correct_val / total_val

#     print(f"Epoch [{epoch+1}/{num_epochs}], Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}")

# model.eval()
# correct_test = 0
# total_test = 0
# with torch.no_grad():
#     for X_test_batch, y_test_batch in test_dataloader:
#         X_test_batch, y_test_batch = X_test_batch.to('cuda:0'), y_test_batch.to('cuda:0')
#         y_test_batch = y_test_batch.long()
#         output_test = model(X_test_batch)
#         _, predicted_test = torch.max(output_test, 1)
#         correct_test += (predicted_test == y_test_batch).sum().item()
#         total_test += y_test_batch.size(0)

# test_accuracy = correct_test / total_test
# print(f"Final Test Accuracy: {test_accuracy:.4f}")

Execution output
1KB
	Stream
		Epoch [1/10], Train Acc: 0.6964, Val Acc: 0.9711
		Epoch [2/10], Train Acc: 0.9852, Val Acc: 0.9890
		Epoch [3/10], Train Acc: 0.9888, Val Acc: 0.9907
		Epoch [4/10], Train Acc: 0.9903, Val Acc: 0.9907
		Epoch [5/10], Train Acc: 0.9924, Val Acc: 0.9915
		Epoch [6/10], Train Acc: 0.9924, Val Acc: 0.9941
		Epoch [7/10], Train Acc: 0.9939, Val Acc: 0.9932
		Epoch [8/10], Train Acc: 0.9939, Val Acc: 0.9941
		Epoch [9/10], Train Acc: 0.9944, Val Acc: 0.9941
		Epoch [10/10], Train Acc: 0.9949, Val Acc: 0.9958
		Final Test Accuracy: 0.9924

Code cell <I7wk82Y2TWBe>
# %% [code]
# # evaluating model
# y_true = []
# y_pred = []

# with torch.no_grad():
#     for X_test_batch, y_test_batch in test_dataloader:
#         X_test_batch, y_test_batch = X_test_batch.to('cuda:0'), y_test_batch.to('cuda:0')
#         y_test_batch = y_test_batch.long()

#         output_test = model(X_test_batch)
#         _, predicted_test = torch.max(output_test, 1)

#         y_true.extend(y_test_batch.cpu().numpy())
#         y_pred.extend(predicted_test.cpu().numpy())

# y_true = np.array(y_true)
# y_pred = np.array(y_pred)

# EvalMetricsMultiClass(y_true, y_pred, num_classes=5)
Execution output
32KB
	Stream
		Accuracy: 0.9924
		
		Classification Report:
		Class 0: Precision=0.98, Recall=1.00, F1-score=0.99
		Class 1: Precision=1.00, Recall=0.98, F1-score=0.99
		Class 2: Precision=1.00, Recall=0.97, F1-score=0.99
		Class 3: Precision=0.99, Recall=1.00, F1-score=0.99
		Class 4: Precision=1.00, Recall=1.00, F1-score=1.00
	text/plain
		<Figure size 500x400 with 2 Axes>

Code cell <oXLiYvUZZC1f>
# %% [code]


Text cell <RHC4vWIFZDSE>
# %% [markdown]
# Model 3 : RNN

Text cell <SKh-KdYNZLHc>
# %% [markdown]
### (i) Word2Vec Embedding

Code cell <8l8Ej2I3TWBe>
# %% [code]
from torch.utils.data import Dataset

class EmbeddingDataset2(Dataset):
    def __init__(self, textdata,labels,model,vec_size=100,max_len=100):
        self.xTrain = textdata
        self.vec_size=vec_size
        self.yTrain=labels
        self.model = model
        self.max_len=max_len

    def __getitem__(self, idx):
        text = self.xTrain.iloc[idx]
        # print()
        words = text.split()
        vectorized_words = [self.model.wv[word] for word in words if word in self.model.wv]
        if not vectorized_words:
          return np.zeros(self.vec_size)
        currlabel = torch.tensor(self.yTrain[idx], dtype=torch.float32)

        return torch.tensor(vectorized_words, dtype=torch.float32), currlabel

    def __len__(self):
        return len(self.xTrain)



train_dataset = EmbeddingDataset2(train_texts, y_train_tensor,word2vec_model)
train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)
val_dataset = EmbeddingDataset2(val_texts, y_val_tensor,word2vec_model)
val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)
test_dataset = EmbeddingDataset2(test_texts, y_test_tensor,word2vec_model)
test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)


Code cell <mAgUjcoQTWBe>
# %% [code]
class RNNModel(nn.Module):
    def __init__(self, input_dim=100, hidden_dim=256, output_dim=5, num_layers=3, dropout=0.3):
        super(RNNModel, self).__init__()
        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)
        self.layer_norm = nn.LayerNorm(hidden_dim)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x, _ = self.rnn(x)
        x = x[:, -1, :]
        x = self.layer_norm(x)
        x = self.dropout(x)
        out = self.fc(x)
        return out

Code cell <PtW7u44eTWBe>
# %% [code]

model = RNNModel(100,128,5,2)
model = model.to('cuda:0')
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
num_epochs = 10

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    correct_train = 0
    total_train = 0
    for X_batch, y_batch in train_dataloader:
        optimizer.zero_grad()
        X_batch, y_batch = X_batch.to('cuda:0'), y_batch.to('cuda:0')
        output = model(X_batch)
        y_batch = y_batch.long()
        loss = criterion(output, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        _, predicted = torch.max(output, 1)
        correct_train += (predicted == y_batch).sum().item()
        total_train += y_batch.size(0)

    train_accuracy = correct_train / total_train

    model.eval()
    total_val_loss = 0
    correct_val = 0
    total_val = 0
    with torch.no_grad():
        for X_val_batch, y_val_batch in val_dataloader:
            X_val_batch, y_val_batch = X_val_batch.to('cuda:0'), y_val_batch.to('cuda:0')
            output_val = model(X_val_batch)
            y_val_batch = y_val_batch.long()
            loss_val = criterion(output_val, y_val_batch)
            total_val_loss += loss_val.item()
            _, output = torch.max(output_val, 1)
            correct_val += (output == y_val_batch).sum().item()
            total_val += y_val_batch.size(0)

    val_accuracy = correct_val / total_val

    print(f"Epoch [{epoch+1}/{num_epochs}], Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}")

model.eval()
correct_test = 0
total_test = 0
with torch.no_grad():
    for X_test_batch, y_test_batch in test_dataloader:
        X_test_batch, y_test_batch = X_test_batch.to('cuda:0'), y_test_batch.to('cuda:0')
        y_test_batch = y_test_batch.long()
        output_test = model(X_test_batch)
        _, predicted_test = torch.max(output_test, 1)
        correct_test += (predicted_test == y_test_batch).sum().item()
        total_test += y_test_batch.size(0)

test_accuracy = correct_test / total_test
print(f"Final Test Accuracy: {test_accuracy:.4f}")

Execution output
1KB
	Stream
		/tmp/ipykernel_1169806/3519196531.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
		  currlabel = torch.tensor(self.yTrain[idx], dtype=torch.float32)
		Epoch [1/10], Train Acc: 0.7718, Val Acc: 0.7988
		Epoch [2/10], Train Acc: 0.7387, Val Acc: 0.7988
		Epoch [3/10], Train Acc: 0.8054, Val Acc: 0.8430
		Epoch [4/10], Train Acc: 0.7489, Val Acc: 0.7725
		Epoch [5/10], Train Acc: 0.8074, Val Acc: 0.8438
		Epoch [6/10], Train Acc: 0.8747, Val Acc: 0.8447
		Epoch [7/10], Train Acc: 0.8803, Val Acc: 0.8795
		Epoch [8/10], Train Acc: 0.8696, Val Acc: 0.6231
		Epoch [9/10], Train Acc: 0.8314, Val Acc: 0.8463
		Epoch [10/10], Train Acc: 0.8299, Val Acc: 0.9100
		Final Test Accuracy: 0.9135

Code cell <ecTYKeGVTWBe>
# %% [code]
y_true = []
y_pred = []

with torch.no_grad():
    for X_test_batch, y_test_batch in test_dataloader:
        X_test_batch, y_test_batch = X_test_batch.to('cuda:0'), y_test_batch.to('cuda:0')
        y_test_batch = y_test_batch.long()

        output_test = model(X_test_batch)
        _, predicted_test = torch.max(output_test, 1)

        y_true.extend(y_test_batch.cpu().numpy())
        y_pred.extend(predicted_test.cpu().numpy())

y_true = np.array(y_true)
y_pred = np.array(y_pred)

EvalMetricsMultiClass(y_true, y_pred, num_classes=5)
Execution output
34KB
	Stream
		/tmp/ipykernel_1169806/3519196531.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
		  currlabel = torch.tensor(self.yTrain[idx], dtype=torch.float32)
		Accuracy: 0.9135
		
		Classification Report:
		Class 0: Precision=1.00, Recall=0.89, F1-score=0.94
		Class 1: Precision=0.88, Recall=0.86, F1-score=0.87
		Class 2: Precision=0.61, Recall=0.91, F1-score=0.73
		Class 3: Precision=0.98, Recall=0.98, F1-score=0.98
		Class 4: Precision=1.00, Recall=0.86, F1-score=0.92
	text/plain
		<Figure size 500x400 with 2 Axes>

Text cell <Kn2yJO2nTWBe>
# %% [markdown]
## RNN+fasttext

Code cell <LmL5kk8fTWBe>
# %% [code]
from torch.utils.data import Dataset

class EmbeddingDataset2(Dataset):
    def __init__(self, textdata,labels,model,vec_size=100,max_len=100):
        self.xTrain = textdata
        self.vec_size=vec_size
        self.yTrain=labels
        self.model = model
        self.max_len=max_len

    def __getitem__(self, idx):
        text = self.xTrain.iloc[idx]
        # print()
        words = text.split()
        vectorized_words = [self.model.wv[word] for word in words if word in self.model.wv]
        if not vectorized_words:
          return np.zeros(self.vec_size)
        currlabel = torch.tensor(self.yTrain[idx], dtype=torch.float32)

        return torch.tensor(vectorized_words, dtype=torch.float32), currlabel

    def __len__(self):
        return len(self.xTrain)



train_dataset = EmbeddingDataset2(train_texts, y_train_tensor,fasttext_model)
train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)
val_dataset = EmbeddingDataset2(val_texts, y_val_tensor,fasttext_model)
val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)
test_dataset = EmbeddingDataset2(test_texts, y_test_tensor,fasttext_model)
test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)


Code cell <HTTIl9-0TWBf>
# %% [code]
class RNNModel(nn.Module):
    def __init__(self, input_dim=100, hidden_dim=256, output_dim=5, num_layers=3, dropout=0.3):
        super(RNNModel, self).__init__()
        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)
        self.layer_norm = nn.LayerNorm(hidden_dim)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x, _ = self.rnn(x)
        x = x[:, -1, :]
        x = self.layer_norm(x)
        x = self.dropout(x)
        out = self.fc(x)
        return out

Code cell <9HCv1X3ATWBf>
# %% [code]

model = RNNModel(100,128,5,2)
model = model.to('cuda:0')
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
num_epochs = 10

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    correct_train = 0
    total_train = 0
    for X_batch, y_batch in train_dataloader:
        optimizer.zero_grad()
        X_batch, y_batch = X_batch.to('cuda:0'), y_batch.to('cuda:0')
        output = model(X_batch)
        y_batch = y_batch.long()
        loss = criterion(output, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        _, predicted = torch.max(output, 1)
        correct_train += (predicted == y_batch).sum().item()
        total_train += y_batch.size(0)

    train_accuracy = correct_train / total_train

    model.eval()
    total_val_loss = 0
    correct_val = 0
    total_val = 0
    with torch.no_grad():
        for X_val_batch, y_val_batch in val_dataloader:
            X_val_batch, y_val_batch = X_val_batch.to('cuda:0'), y_val_batch.to('cuda:0')
            output_val = model(X_val_batch)
            y_val_batch = y_val_batch.long()
            loss_val = criterion(output_val, y_val_batch)
            total_val_loss += loss_val.item()
            _, output = torch.max(output_val, 1)
            correct_val += (output == y_val_batch).sum().item()
            total_val += y_val_batch.size(0)

    val_accuracy = correct_val / total_val

    print(f"Epoch [{epoch+1}/{num_epochs}], Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}")

model.eval()
correct_test = 0
total_test = 0
with torch.no_grad():
    for X_test_batch, y_test_batch in test_dataloader:
        X_test_batch, y_test_batch = X_test_batch.to('cuda:0'), y_test_batch.to('cuda:0')
        y_test_batch = y_test_batch.long()
        output_test = model(X_test_batch)
        _, predicted_test = torch.max(output_test, 1)
        correct_test += (predicted_test == y_test_batch).sum().item()
        total_test += y_test_batch.size(0)

test_accuracy = correct_test / total_test
print(f"Final Test Accuracy: {test_accuracy:.4f}")

Execution output
1KB
	Stream
		/tmp/ipykernel_1169806/2704010313.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
		  currlabel = torch.tensor(self.yTrain[idx], dtype=torch.float32)
		Epoch [1/10], Train Acc: 0.8299, Val Acc: 0.8608
		Epoch [2/10], Train Acc: 0.8956, Val Acc: 0.7394
		Epoch [3/10], Train Acc: 0.8839, Val Acc: 0.8650
		Epoch [4/10], Train Acc: 0.8874, Val Acc: 0.9066
		Epoch [5/10], Train Acc: 0.9063, Val Acc: 0.8642
		Epoch [6/10], Train Acc: 0.9012, Val Acc: 0.8820
		Epoch [7/10], Train Acc: 0.9047, Val Acc: 0.8930
		Epoch [8/10], Train Acc: 0.9328, Val Acc: 0.9295
		Epoch [9/10], Train Acc: 0.9368, Val Acc: 0.9329
		Epoch [10/10], Train Acc: 0.9246, Val Acc: 0.9202
		Final Test Accuracy: 0.9249

Code cell <TdXbKyjjTWBf>
# %% [code]
y_true = []
y_pred = []

with torch.no_grad():
    for X_test_batch, y_test_batch in test_dataloader:
        X_test_batch, y_test_batch = X_test_batch.to('cuda:0'), y_test_batch.to('cuda:0')
        y_test_batch = y_test_batch.long()

        output_test = model(X_test_batch)
        _, predicted_test = torch.max(output_test, 1)

        y_true.extend(y_test_batch.cpu().numpy())
        y_pred.extend(predicted_test.cpu().numpy())

y_true = np.array(y_true)
y_pred = np.array(y_pred)

EvalMetricsMultiClass(y_true, y_pred, num_classes=5)
Execution output
34KB
	Stream
		/tmp/ipykernel_1169806/2704010313.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
		  currlabel = torch.tensor(self.yTrain[idx], dtype=torch.float32)
		Accuracy: 0.9249
		
		Classification Report:
		Class 0: Precision=0.96, Recall=0.94, F1-score=0.95
		Class 1: Precision=0.87, Recall=0.96, F1-score=0.91
		Class 2: Precision=0.91, Recall=0.75, F1-score=0.82
		Class 3: Precision=0.93, Recall=0.91, F1-score=0.92
		Class 4: Precision=0.99, Recall=0.99, F1-score=0.99
	text/plain
		<Figure size 500x400 with 2 Axes>

Text cell <6YBdnpBcZrQy>
# %% [markdown]
# Model 4 : Bidirectional LSTM

Text cell <ejObr15qTWBf>
# %% [markdown]
### (i) Word2Vec Embedding

Code cell <tUodWj7yTWBf>
# %% [code]
from torch.utils.data import Dataset

class EmbeddingDataset2(Dataset):
    def __init__(self, textdata,labels,model,vec_size=100,max_len=100):
        self.xTrain = textdata
        self.vec_size=vec_size
        self.yTrain=labels
        self.model = model
        self.max_len=max_len

    def __getitem__(self, idx):
        text = self.xTrain.iloc[idx]
        # print()
        words = text.split()
        vectorized_words = [self.model.wv[word] for word in words if word in self.model.wv]
        if not vectorized_words:
          return np.zeros(self.vec_size)
        currlabel = torch.tensor(self.yTrain[idx], dtype=torch.float32)

        return torch.tensor(vectorized_words, dtype=torch.float32), currlabel

    def __len__(self):
        return len(self.xTrain)



train_dataset = EmbeddingDataset2(train_texts, y_train_tensor,word2vec_model)
train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)
val_dataset = EmbeddingDataset2(val_texts, y_val_tensor,word2vec_model)
val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)
test_dataset = EmbeddingDataset2(test_texts, y_test_tensor,word2vec_model)
test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)


Code cell <oY_chF6YTWBf>
# %% [code]
class LSTMModel(nn.Module):
    def __init__(self, input_dim=100, hidden_dim=256, output_dim=5, num_layers=3, dropout=0.3):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)
        self.layer_norm = nn.LayerNorm(hidden_dim * 2)
        self.fc = nn.Linear(hidden_dim * 2, output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x, _ = self.lstm(x)
        x = x[:, -1, :]
        x = self.layer_norm(x)
        x = self.dropout(x)
        out = self.fc(x)
        return out

Code cell <6X5bIFQhTWBf>
# %% [code]

model = LSTMModel(100,128,5,2)
model = model.to('cuda:0')
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
num_epochs = 10

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    correct_train = 0
    total_train = 0
    for X_batch, y_batch in train_dataloader:
        optimizer.zero_grad()
        X_batch, y_batch = X_batch.to('cuda:0'), y_batch.to('cuda:0')
        output = model(X_batch)
        y_batch = y_batch.long()
        loss = criterion(output, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        _, predicted = torch.max(output, 1)
        correct_train += (predicted == y_batch).sum().item()
        total_train += y_batch.size(0)

    train_accuracy = correct_train / total_train

    model.eval()
    total_val_loss = 0
    correct_val = 0
    total_val = 0
    with torch.no_grad():
        for X_val_batch, y_val_batch in val_dataloader:
            X_val_batch, y_val_batch = X_val_batch.to('cuda:0'), y_val_batch.to('cuda:0')
            output_val = model(X_val_batch)
            y_val_batch = y_val_batch.long()
            loss_val = criterion(output_val, y_val_batch)
            total_val_loss += loss_val.item()
            _, output = torch.max(output_val, 1)
            correct_val += (output == y_val_batch).sum().item()
            total_val += y_val_batch.size(0)

    val_accuracy = correct_val / total_val

    print(f"Epoch [{epoch+1}/{num_epochs}], Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}")

model.eval()
correct_test = 0
total_test = 0
with torch.no_grad():
    for X_test_batch, y_test_batch in test_dataloader:
        X_test_batch, y_test_batch = X_test_batch.to('cuda:0'), y_test_batch.to('cuda:0')
        y_test_batch = y_test_batch.long()
        output_test = model(X_test_batch)
        _, predicted_test = torch.max(output_test, 1)
        correct_test += (predicted_test == y_test_batch).sum().item()
        total_test += y_test_batch.size(0)

test_accuracy = correct_test / total_test
print(f"Final Test Accuracy: {test_accuracy:.4f}")
Execution output
1KB
	Stream
		/tmp/ipykernel_1169806/3519196531.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
		  currlabel = torch.tensor(self.yTrain[idx], dtype=torch.float32)
		Epoch [1/10], Train Acc: 0.9058, Val Acc: 0.8947
		Epoch [2/10], Train Acc: 0.9435, Val Acc: 0.8905
		Epoch [3/10], Train Acc: 0.9679, Val Acc: 0.9762
		Epoch [4/10], Train Acc: 0.9766, Val Acc: 0.9839
		Epoch [5/10], Train Acc: 0.9801, Val Acc: 0.9873
		Epoch [6/10], Train Acc: 0.9837, Val Acc: 0.9788
		Epoch [7/10], Train Acc: 0.9964, Val Acc: 0.9924
		Epoch [8/10], Train Acc: 0.9918, Val Acc: 0.9864
		Epoch [9/10], Train Acc: 0.9873, Val Acc: 0.9907
		Epoch [10/10], Train Acc: 0.9969, Val Acc: 0.9915
		Final Test Accuracy: 0.9936

Code cell <Eov8LE37TWBf>
# %% [code]
y_true = []
y_pred = []

with torch.no_grad():
    for X_test_batch, y_test_batch in test_dataloader:
        X_test_batch, y_test_batch = X_test_batch.to('cuda:0'), y_test_batch.to('cuda:0')
        y_test_batch = y_test_batch.long()

        output_test = model(X_test_batch)
        _, predicted_test = torch.max(output_test, 1)

        y_true.extend(y_test_batch.cpu().numpy())
        y_pred.extend(predicted_test.cpu().numpy())

y_true = np.array(y_true)
y_pred = np.array(y_pred)

EvalMetricsMultiClass(y_true, y_pred, num_classes=5)
Execution output
32KB
	Stream
		/tmp/ipykernel_1169806/3519196531.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
		  currlabel = torch.tensor(self.yTrain[idx], dtype=torch.float32)
		Accuracy: 0.9936
		
		Classification Report:
		Class 0: Precision=0.99, Recall=0.99, F1-score=0.99
		Class 1: Precision=1.00, Recall=0.98, F1-score=0.99
		Class 2: Precision=0.99, Recall=0.99, F1-score=0.99
		Class 3: Precision=0.99, Recall=1.00, F1-score=0.99
		Class 4: Precision=1.00, Recall=1.00, F1-score=1.00
	text/plain
		<Figure size 500x400 with 2 Axes>

Text cell <zJFzBR8KTWBg>
# %% [markdown]
### Lstm+fasttext

Code cell <R3Qbd1JaTWBg>
# %% [code]
from torch.utils.data import Dataset

class EmbeddingDataset2(Dataset):
    def __init__(self, textdata,labels,model,vec_size=100,max_len=100):
        self.xTrain = textdata
        self.vec_size=vec_size
        self.yTrain=labels
        self.model = model
        self.max_len=max_len

    def __getitem__(self, idx):
        text = self.xTrain.iloc[idx]
        # print()
        words = text.split()
        vectorized_words = [self.model.wv[word] for word in words if word in self.model.wv]
        if not vectorized_words:
          return np.zeros(self.vec_size)
        currlabel = torch.tensor(self.yTrain[idx], dtype=torch.float32)

        return torch.tensor(vectorized_words, dtype=torch.float32), currlabel

    def __len__(self):
        return len(self.xTrain)



train_dataset = EmbeddingDataset2(train_texts, y_train_tensor,fasttext_model)
train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)
val_dataset = EmbeddingDataset2(val_texts, y_val_tensor,fasttext_model)
val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)
test_dataset = EmbeddingDataset2(test_texts, y_test_tensor,fasttext_model)
test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)


Code cell <0gST_St2TWBg>
# %% [code]
class LSTMModel(nn.Module):
    def __init__(self, input_dim=100, hidden_dim=256, output_dim=5, num_layers=3, dropout=0.3):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)
        self.layer_norm = nn.LayerNorm(hidden_dim * 2)
        self.fc = nn.Linear(hidden_dim * 2, output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x, _ = self.lstm(x)
        x = x[:, -1, :]
        x = self.layer_norm(x)
        x = self.dropout(x)
        out = self.fc(x)
        return out

Code cell <n89F0fKqTWBg>
# %% [code]

model = LSTMModel(100,128,5,2)
model = model.to('cuda:0')
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
num_epochs = 10

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    correct_train = 0
    total_train = 0
    for X_batch, y_batch in train_dataloader:
        optimizer.zero_grad()
        X_batch, y_batch = X_batch.to('cuda:0'), y_batch.to('cuda:0')
        output = model(X_batch)
        y_batch = y_batch.long()
        loss = criterion(output, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        _, predicted = torch.max(output, 1)
        correct_train += (predicted == y_batch).sum().item()
        total_train += y_batch.size(0)

    train_accuracy = correct_train / total_train

    model.eval()
    total_val_loss = 0
    correct_val = 0
    total_val = 0
    with torch.no_grad():
        for X_val_batch, y_val_batch in val_dataloader:
            X_val_batch, y_val_batch = X_val_batch.to('cuda:0'), y_val_batch.to('cuda:0')
            output_val = model(X_val_batch)
            y_val_batch = y_val_batch.long()
            loss_val = criterion(output_val, y_val_batch)
            total_val_loss += loss_val.item()
            _, output = torch.max(output_val, 1)
            correct_val += (output == y_val_batch).sum().item()
            total_val += y_val_batch.size(0)

    val_accuracy = correct_val / total_val

    print(f"Epoch [{epoch+1}/{num_epochs}], Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}")

model.eval()
correct_test = 0
total_test = 0
with torch.no_grad():
    for X_test_batch, y_test_batch in test_dataloader:
        X_test_batch, y_test_batch = X_test_batch.to('cuda:0'), y_test_batch.to('cuda:0')
        y_test_batch = y_test_batch.long()
        output_test = model(X_test_batch)
        _, predicted_test = torch.max(output_test, 1)
        correct_test += (predicted_test == y_test_batch).sum().item()
        total_test += y_test_batch.size(0)

test_accuracy = correct_test / total_test
print(f"Final Test Accuracy: {test_accuracy:.4f}")

Execution output
1KB
	Stream
		/tmp/ipykernel_1169806/2704010313.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
		  currlabel = torch.tensor(self.yTrain[idx], dtype=torch.float32)
		Epoch [1/10], Train Acc: 0.9159, Val Acc: 0.9610
		Epoch [2/10], Train Acc: 0.9699, Val Acc: 0.9771
		Epoch [3/10], Train Acc: 0.9837, Val Acc: 0.9762
		Epoch [4/10], Train Acc: 0.9888, Val Acc: 0.9907
		Epoch [5/10], Train Acc: 0.9868, Val Acc: 0.9711
		Epoch [6/10], Train Acc: 0.9832, Val Acc: 0.9907
		Epoch [7/10], Train Acc: 0.9954, Val Acc: 0.9873
		Epoch [8/10], Train Acc: 0.9980, Val Acc: 0.9856
		Epoch [9/10], Train Acc: 0.9888, Val Acc: 0.9898
		Epoch [10/10], Train Acc: 0.9934, Val Acc: 0.9873
		Final Test Accuracy: 0.9796

Code cell <VrW5HrLgTWBg>
# %% [code]
y_true = []
y_pred = []

with torch.no_grad():
    for X_test_batch, y_test_batch in test_dataloader:
        X_test_batch, y_test_batch = X_test_batch.to('cuda:0'), y_test_batch.to('cuda:0')
        y_test_batch = y_test_batch.long()

        output_test = model(X_test_batch)
        _, predicted_test = torch.max(output_test, 1)

        y_true.extend(y_test_batch.cpu().numpy())
        y_pred.extend(predicted_test.cpu().numpy())

y_true = np.array(y_true)
y_pred = np.array(y_pred)

EvalMetricsMultiClass(y_true, y_pred, num_classes=5)
Execution output
33KB
	Stream
		/tmp/ipykernel_1169806/2704010313.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
		  currlabel = torch.tensor(self.yTrain[idx], dtype=torch.float32)
		Accuracy: 0.9796
		
		Classification Report:
		Class 0: Precision=0.99, Recall=0.99, F1-score=0.99
		Class 1: Precision=0.99, Recall=0.95, F1-score=0.97
		Class 2: Precision=0.93, Recall=0.97, F1-score=0.95
		Class 3: Precision=0.97, Recall=0.99, F1-score=0.98
		Class 4: Precision=1.00, Recall=0.99, F1-score=1.00
	text/plain
		<Figure size 500x400 with 2 Axes>

Text cell <rUXNwOlaTWBg>
# %% [markdown]
# Conclusions


