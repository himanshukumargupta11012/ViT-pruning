{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84948a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total experiments run (attempted): 3\n",
      "Successfully completed experiments: 3\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(all_results)\n",
    "print(f\"Total experiments run (attempted): {experiment_count}\")\n",
    "if 'status' in results_df.columns:\n",
    "    success_count = len(results_df[results_df['status'] == 'success'])\n",
    "    print(f\"Successfully completed experiments: {success_count}\")\n",
    "else:\n",
    "    print(\"Status column not found in results, cannot count successful experiments.\")\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('layer12.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136ad056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Experiment Runner...\n",
      "Total preprocessing configs: 1\n",
      "Models to try: ['distilbert-base-uncased', 'bert-base-uncased', 'roberta-base']\n",
      "Strategies to try: ['single_task']\n",
      "Loss functions to try: ['FocalLoss']\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Running Experiment 1 ---\n",
      "Config: {'exp_id': 1, 'model': 'distilbert-base-uncased', 'strategy': 'single_task', 'loss': 'FocalLoss', 'remove_punct': False, 'remove_stopwords': False, 'include_history': False}\n",
      "Preparing Dataloaders...\n",
      "Data split sizes:\n",
      "  Train: 1980\n",
      "  Validation: 248\n",
      "  Test: 248\n",
      "Loading tokenizer: distilbert-base-uncased\n",
      "Creating Train Dataset...\n",
      "Creating Validation Dataset...\n",
      "Creating Test Dataset...\n",
      "Using 19 workers for DataLoaders.\n",
      "Dataloaders created successfully.\n",
      "Initializing Single-Task Models (one per task)...\n",
      "Strategy: single_task. Will load distilbert-base-uncased with 3 labels per task during training.\n",
      "Loading custom concat-based single-task model: distilbert-base-uncased with 3 labels.\n",
      "\n",
      "-- Training Single-Task Model for: Mistake_Identification (Task 1/4) --\n",
      "Loading custom concat-based single-task model: distilbert-base-uncased with 3 labels.\n",
      "--- Starting Training --- [Epochs: 5, Strategy: single_task, Loss: FocalLoss, Task Index: 0]\n",
      "  Epoch 1/5, Step 50/62, Batch Loss: 0.2893\n",
      "  Epoch 1/5, Step 62/62, Batch Loss: 0.1612\n",
      "Epoch 1/5 | Time: 18.86s | Avg Train Loss: 0.2418\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.9549\n",
      "    Mistake_Identification: Exact Acc: 0.8629, Exact F1: 0.5908, Lenient Acc: 0.9234, Lenient F1: 0.9549\n",
      "  Validation metric improved (-inf --> 0.9549).\n",
      "  Epoch 2/5, Step 50/62, Batch Loss: 0.2033\n",
      "  Epoch 2/5, Step 62/62, Batch Loss: 0.2416\n",
      "Epoch 2/5 | Time: 19.07s | Avg Train Loss: 0.1622\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.9446\n",
      "    Mistake_Identification: Exact Acc: 0.8387, Exact F1: 0.5977, Lenient Acc: 0.9073, Lenient F1: 0.9446\n",
      "  Validation metric did not improve. Patience: 1/3.\n",
      "  Epoch 3/5, Step 50/62, Batch Loss: 0.0505\n",
      "  Epoch 3/5, Step 62/62, Batch Loss: 0.1058\n",
      "Epoch 3/5 | Time: 19.04s | Avg Train Loss: 0.1222\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.9592\n",
      "    Mistake_Identification: Exact Acc: 0.8710, Exact F1: 0.6537, Lenient Acc: 0.9315, Lenient F1: 0.9592\n",
      "  Validation metric improved (0.9549 --> 0.9592).\n",
      "  Epoch 4/5, Step 50/62, Batch Loss: 0.1303\n",
      "  Epoch 4/5, Step 62/62, Batch Loss: 0.0447\n",
      "Epoch 4/5 | Time: 19.07s | Avg Train Loss: 0.0803\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.9717\n",
      "    Mistake_Identification: Exact Acc: 0.8911, Exact F1: 0.6744, Lenient Acc: 0.9516, Lenient F1: 0.9717\n",
      "  Validation metric improved (0.9592 --> 0.9717).\n",
      "  Epoch 5/5, Step 50/62, Batch Loss: 0.0503\n",
      "  Epoch 5/5, Step 62/62, Batch Loss: 0.0436\n",
      "Epoch 5/5 | Time: 19.04s | Avg Train Loss: 0.0413\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.9667\n",
      "    Mistake_Identification: Exact Acc: 0.8710, Exact F1: 0.7045, Lenient Acc: 0.9435, Lenient F1: 0.9667\n",
      "  Validation metric did not improve. Patience: 1/3.\n",
      "--- Training Finished --- Best Avg Validation Lenient F1: 0.9717\n",
      "Cleaning up resources for task: Mistake_Identification\n",
      "\n",
      "-- Training Single-Task Model for: Mistake_Location (Task 2/4) --\n",
      "Loading custom concat-based single-task model: distilbert-base-uncased with 3 labels.\n",
      "--- Starting Training --- [Epochs: 5, Strategy: single_task, Loss: FocalLoss, Task Index: 1]\n",
      "  Epoch 1/5, Step 50/62, Batch Loss: 0.2905\n",
      "  Epoch 1/5, Step 62/62, Batch Loss: 0.2218\n",
      "Epoch 1/5 | Time: 19.35s | Avg Train Loss: 0.3182\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8634\n",
      "    Mistake_Location: Exact Acc: 0.7218, Exact F1: 0.4737, Lenient Acc: 0.7984, Lenient F1: 0.8634\n",
      "  Validation metric improved (-inf --> 0.8634).\n",
      "  Epoch 2/5, Step 50/62, Batch Loss: 0.3885\n",
      "  Epoch 2/5, Step 62/62, Batch Loss: 0.4217\n",
      "Epoch 2/5 | Time: 19.10s | Avg Train Loss: 0.2523\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8721\n",
      "    Mistake_Location: Exact Acc: 0.7177, Exact F1: 0.5068, Lenient Acc: 0.8024, Lenient F1: 0.8721\n",
      "  Validation metric improved (0.8634 --> 0.8721).\n",
      "  Epoch 3/5, Step 50/62, Batch Loss: 0.1717\n",
      "  Epoch 3/5, Step 62/62, Batch Loss: 0.1736\n",
      "Epoch 3/5 | Time: 19.09s | Avg Train Loss: 0.1831\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.9041\n",
      "    Mistake_Location: Exact Acc: 0.7540, Exact F1: 0.5879, Lenient Acc: 0.8589, Lenient F1: 0.9041\n",
      "  Validation metric improved (0.8721 --> 0.9041).\n",
      "  Epoch 4/5, Step 50/62, Batch Loss: 0.0807\n",
      "  Epoch 4/5, Step 62/62, Batch Loss: 0.1185\n",
      "Epoch 4/5 | Time: 19.13s | Avg Train Loss: 0.1109\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8983\n",
      "    Mistake_Location: Exact Acc: 0.7581, Exact F1: 0.6060, Lenient Acc: 0.8548, Lenient F1: 0.8983\n",
      "  Validation metric did not improve. Patience: 1/3.\n",
      "  Epoch 5/5, Step 50/62, Batch Loss: 0.0266\n",
      "  Epoch 5/5, Step 62/62, Batch Loss: 0.0443\n",
      "Epoch 5/5 | Time: 19.12s | Avg Train Loss: 0.0586\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8914\n",
      "    Mistake_Location: Exact Acc: 0.7339, Exact F1: 0.5455, Lenient Acc: 0.8427, Lenient F1: 0.8914\n",
      "  Validation metric did not improve. Patience: 2/3.\n",
      "--- Training Finished --- Best Avg Validation Lenient F1: 0.9041\n",
      "Cleaning up resources for task: Mistake_Location\n",
      "\n",
      "-- Training Single-Task Model for: Providing_Guidance (Task 3/4) --\n",
      "Loading custom concat-based single-task model: distilbert-base-uncased with 3 labels.\n",
      "--- Starting Training --- [Epochs: 5, Strategy: single_task, Loss: FocalLoss, Task Index: 2]\n",
      "  Epoch 1/5, Step 50/62, Batch Loss: 0.5145\n",
      "  Epoch 1/5, Step 62/62, Batch Loss: 0.3318\n",
      "Epoch 1/5 | Time: 19.32s | Avg Train Loss: 0.3991\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.9010\n",
      "    Providing_Guidance: Exact Acc: 0.6895, Exact F1: 0.4963, Lenient Acc: 0.8387, Lenient F1: 0.9010\n",
      "  Validation metric improved (-inf --> 0.9010).\n",
      "  Epoch 2/5, Step 50/62, Batch Loss: 0.3027\n",
      "  Epoch 2/5, Step 62/62, Batch Loss: 0.2277\n",
      "Epoch 2/5 | Time: 19.09s | Avg Train Loss: 0.3177\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8928\n",
      "    Providing_Guidance: Exact Acc: 0.6613, Exact F1: 0.4788, Lenient Acc: 0.8266, Lenient F1: 0.8928\n",
      "  Validation metric did not improve. Patience: 1/3.\n",
      "  Epoch 3/5, Step 50/62, Batch Loss: 0.3036\n",
      "  Epoch 3/5, Step 62/62, Batch Loss: 0.2646\n",
      "Epoch 3/5 | Time: 19.08s | Avg Train Loss: 0.2507\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8901\n",
      "    Providing_Guidance: Exact Acc: 0.5887, Exact F1: 0.5266, Lenient Acc: 0.8306, Lenient F1: 0.8901\n",
      "  Validation metric did not improve. Patience: 2/3.\n",
      "  Epoch 4/5, Step 50/62, Batch Loss: 0.1853\n",
      "  Epoch 4/5, Step 62/62, Batch Loss: 0.1570\n",
      "Epoch 4/5 | Time: 19.07s | Avg Train Loss: 0.1588\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8783\n",
      "    Providing_Guidance: Exact Acc: 0.6210, Exact F1: 0.5224, Lenient Acc: 0.8145, Lenient F1: 0.8783\n",
      "  Validation metric did not improve. Patience: 3/3.\n",
      "Early stopping triggered after 3 epochs without improvement.\n",
      "--- Training Finished --- Best Avg Validation Lenient F1: 0.9010\n",
      "Cleaning up resources for task: Providing_Guidance\n",
      "\n",
      "-- Training Single-Task Model for: Actionability (Task 4/4) --\n",
      "Loading custom concat-based single-task model: distilbert-base-uncased with 3 labels.\n",
      "--- Starting Training --- [Epochs: 5, Strategy: single_task, Loss: FocalLoss, Task Index: 3]\n",
      "  Epoch 1/5, Step 50/62, Batch Loss: 0.3044\n",
      "  Epoch 1/5, Step 62/62, Batch Loss: 0.2070\n",
      "Epoch 1/5 | Time: 19.07s | Avg Train Loss: 0.3632\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8638\n",
      "    Actionability: Exact Acc: 0.6815, Exact F1: 0.4837, Lenient Acc: 0.8105, Lenient F1: 0.8638\n",
      "  Validation metric improved (-inf --> 0.8638).\n",
      "  Epoch 2/5, Step 50/62, Batch Loss: 0.2350\n",
      "  Epoch 2/5, Step 62/62, Batch Loss: 0.2840\n",
      "Epoch 2/5 | Time: 19.07s | Avg Train Loss: 0.2675\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8598\n",
      "    Actionability: Exact Acc: 0.6976, Exact F1: 0.5574, Lenient Acc: 0.8185, Lenient F1: 0.8598\n",
      "  Validation metric did not improve. Patience: 1/3.\n",
      "  Epoch 3/5, Step 50/62, Batch Loss: 0.1417\n",
      "  Epoch 3/5, Step 62/62, Batch Loss: 0.1465\n",
      "Epoch 3/5 | Time: 19.07s | Avg Train Loss: 0.2014\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8829\n",
      "    Actionability: Exact Acc: 0.7339, Exact F1: 0.6270, Lenient Acc: 0.8427, Lenient F1: 0.8829\n",
      "  Validation metric improved (0.8638 --> 0.8829).\n",
      "  Epoch 4/5, Step 50/62, Batch Loss: 0.1696\n",
      "  Epoch 4/5, Step 62/62, Batch Loss: 0.1443\n",
      "Epoch 4/5 | Time: 19.06s | Avg Train Loss: 0.1382\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8727\n",
      "    Actionability: Exact Acc: 0.7177, Exact F1: 0.6430, Lenient Acc: 0.8306, Lenient F1: 0.8727\n",
      "  Validation metric did not improve. Patience: 1/3.\n",
      "  Epoch 5/5, Step 50/62, Batch Loss: 0.0599\n",
      "  Epoch 5/5, Step 62/62, Batch Loss: 0.0459\n",
      "Epoch 5/5 | Time: 19.05s | Avg Train Loss: 0.0723\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8537\n",
      "    Actionability: Exact Acc: 0.7097, Exact F1: 0.6066, Lenient Acc: 0.8065, Lenient F1: 0.8537\n",
      "  Validation metric did not improve. Patience: 2/3.\n",
      "--- Training Finished --- Best Avg Validation Lenient F1: 0.8829\n",
      "Cleaning up resources for task: Actionability\n",
      "--- Experiment 1 Completed Successfully ---\n",
      "Cleaning up experiment resources...\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Running Experiment 2 ---\n",
      "Config: {'exp_id': 2, 'model': 'bert-base-uncased', 'strategy': 'single_task', 'loss': 'FocalLoss', 'remove_punct': False, 'remove_stopwords': False, 'include_history': False}\n",
      "Preparing Dataloaders...\n",
      "Data split sizes:\n",
      "  Train: 1980\n",
      "  Validation: 248\n",
      "  Test: 248\n",
      "Loading tokenizer: bert-base-uncased\n",
      "Creating Train Dataset...\n",
      "Creating Validation Dataset...\n",
      "Creating Test Dataset...\n",
      "Using 19 workers for DataLoaders.\n",
      "Dataloaders created successfully.\n",
      "Initializing Single-Task Models (one per task)...\n",
      "Strategy: single_task. Will load bert-base-uncased with 3 labels per task during training.\n",
      "Loading custom concat-based single-task model: bert-base-uncased with 3 labels.\n",
      "\n",
      "-- Training Single-Task Model for: Mistake_Identification (Task 1/4) --\n",
      "Loading custom concat-based single-task model: bert-base-uncased with 3 labels.\n",
      "--- Starting Training --- [Epochs: 5, Strategy: single_task, Loss: FocalLoss, Task Index: 0]\n",
      "  Epoch 1/5, Step 50/62, Batch Loss: 0.1111\n",
      "  Epoch 1/5, Step 62/62, Batch Loss: 0.1258\n",
      "Epoch 1/5 | Time: 38.68s | Avg Train Loss: 0.2257\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.9676\n",
      "    Mistake_Identification: Exact Acc: 0.8750, Exact F1: 0.6038, Lenient Acc: 0.9435, Lenient F1: 0.9676\n",
      "  Validation metric improved (-inf --> 0.9676).\n",
      "  Epoch 2/5, Step 50/62, Batch Loss: 0.1644\n",
      "  Epoch 2/5, Step 62/62, Batch Loss: 0.1993\n",
      "Epoch 2/5 | Time: 38.56s | Avg Train Loss: 0.1499\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.9596\n",
      "    Mistake_Identification: Exact Acc: 0.8710, Exact F1: 0.6501, Lenient Acc: 0.9315, Lenient F1: 0.9596\n",
      "  Validation metric did not improve. Patience: 1/3.\n",
      "  Epoch 3/5, Step 50/62, Batch Loss: 0.1793\n",
      "  Epoch 3/5, Step 62/62, Batch Loss: 0.0642\n",
      "Epoch 3/5 | Time: 38.54s | Avg Train Loss: 0.1018\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.9544\n",
      "    Mistake_Identification: Exact Acc: 0.8710, Exact F1: 0.6860, Lenient Acc: 0.9234, Lenient F1: 0.9544\n",
      "  Validation metric did not improve. Patience: 2/3.\n",
      "  Epoch 4/5, Step 50/62, Batch Loss: 0.0911\n",
      "  Epoch 4/5, Step 62/62, Batch Loss: 0.0712\n",
      "Epoch 4/5 | Time: 38.52s | Avg Train Loss: 0.0560\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.9621\n",
      "    Mistake_Identification: Exact Acc: 0.8589, Exact F1: 0.6714, Lenient Acc: 0.9355, Lenient F1: 0.9621\n",
      "  Validation metric did not improve. Patience: 3/3.\n",
      "Early stopping triggered after 3 epochs without improvement.\n",
      "--- Training Finished --- Best Avg Validation Lenient F1: 0.9676\n",
      "Cleaning up resources for task: Mistake_Identification\n",
      "\n",
      "-- Training Single-Task Model for: Mistake_Location (Task 2/4) --\n",
      "Loading custom concat-based single-task model: bert-base-uncased with 3 labels.\n",
      "--- Starting Training --- [Epochs: 5, Strategy: single_task, Loss: FocalLoss, Task Index: 1]\n",
      "  Epoch 1/5, Step 50/62, Batch Loss: 0.3070\n",
      "  Epoch 1/5, Step 62/62, Batch Loss: 0.1742\n",
      "Epoch 1/5 | Time: 39.24s | Avg Train Loss: 0.3302\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8548\n",
      "    Mistake_Location: Exact Acc: 0.7056, Exact F1: 0.4541, Lenient Acc: 0.7823, Lenient F1: 0.8548\n",
      "  Validation metric improved (-inf --> 0.8548).\n",
      "  Epoch 2/5, Step 50/62, Batch Loss: 0.3630\n",
      "  Epoch 2/5, Step 62/62, Batch Loss: 0.2050\n",
      "Epoch 2/5 | Time: 39.72s | Avg Train Loss: 0.2572\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8696\n",
      "    Mistake_Location: Exact Acc: 0.7258, Exact F1: 0.5885, Lenient Acc: 0.8065, Lenient F1: 0.8696\n",
      "  Validation metric improved (0.8548 --> 0.8696).\n",
      "  Epoch 3/5, Step 50/62, Batch Loss: 0.1194\n",
      "  Epoch 3/5, Step 62/62, Batch Loss: 0.1394\n",
      "Epoch 3/5 | Time: 38.47s | Avg Train Loss: 0.1800\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8473\n",
      "    Mistake_Location: Exact Acc: 0.6694, Exact F1: 0.5739, Lenient Acc: 0.7863, Lenient F1: 0.8473\n",
      "  Validation metric did not improve. Patience: 1/3.\n",
      "  Epoch 4/5, Step 50/62, Batch Loss: 0.0606\n",
      "  Epoch 4/5, Step 62/62, Batch Loss: 0.1338\n",
      "Epoch 4/5 | Time: 38.48s | Avg Train Loss: 0.1088\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8787\n",
      "    Mistake_Location: Exact Acc: 0.7339, Exact F1: 0.5699, Lenient Acc: 0.8185, Lenient F1: 0.8787\n",
      "  Validation metric improved (0.8696 --> 0.8787).\n",
      "  Epoch 5/5, Step 50/62, Batch Loss: 0.0384\n",
      "  Epoch 5/5, Step 62/62, Batch Loss: 0.0484\n",
      "Epoch 5/5 | Time: 38.51s | Avg Train Loss: 0.0445\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8282\n",
      "    Mistake_Location: Exact Acc: 0.7016, Exact F1: 0.5857, Lenient Acc: 0.7742, Lenient F1: 0.8282\n",
      "  Validation metric did not improve. Patience: 1/3.\n",
      "--- Training Finished --- Best Avg Validation Lenient F1: 0.8787\n",
      "Cleaning up resources for task: Mistake_Location\n",
      "\n",
      "-- Training Single-Task Model for: Providing_Guidance (Task 3/4) --\n",
      "Loading custom concat-based single-task model: bert-base-uncased with 3 labels.\n",
      "--- Starting Training --- [Epochs: 5, Strategy: single_task, Loss: FocalLoss, Task Index: 2]\n",
      "  Epoch 1/5, Step 50/62, Batch Loss: 0.4133\n",
      "  Epoch 1/5, Step 62/62, Batch Loss: 0.3363\n",
      "Epoch 1/5 | Time: 38.67s | Avg Train Loss: 0.3813\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8838\n",
      "    Providing_Guidance: Exact Acc: 0.6573, Exact F1: 0.4956, Lenient Acc: 0.8145, Lenient F1: 0.8838\n",
      "  Validation metric improved (-inf --> 0.8838).\n",
      "  Epoch 2/5, Step 50/62, Batch Loss: 0.2995\n",
      "  Epoch 2/5, Step 62/62, Batch Loss: 0.2718\n",
      "Epoch 2/5 | Time: 38.45s | Avg Train Loss: 0.3039\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8647\n",
      "    Providing_Guidance: Exact Acc: 0.6129, Exact F1: 0.4759, Lenient Acc: 0.7944, Lenient F1: 0.8647\n",
      "  Validation metric did not improve. Patience: 1/3.\n",
      "  Epoch 3/5, Step 50/62, Batch Loss: 0.1473\n",
      "  Epoch 3/5, Step 62/62, Batch Loss: 0.3430\n",
      "Epoch 3/5 | Time: 54.99s | Avg Train Loss: 0.2430\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8711\n",
      "    Providing_Guidance: Exact Acc: 0.6613, Exact F1: 0.4685, Lenient Acc: 0.7984, Lenient F1: 0.8711\n",
      "  Validation metric did not improve. Patience: 2/3.\n",
      "  Epoch 4/5, Step 50/62, Batch Loss: 0.1778\n",
      "  Epoch 4/5, Step 62/62, Batch Loss: 0.1327\n",
      "Epoch 4/5 | Time: 78.50s | Avg Train Loss: 0.1527\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8375\n",
      "    Providing_Guidance: Exact Acc: 0.6048, Exact F1: 0.4995, Lenient Acc: 0.7621, Lenient F1: 0.8375\n",
      "  Validation metric did not improve. Patience: 3/3.\n",
      "Early stopping triggered after 3 epochs without improvement.\n",
      "--- Training Finished --- Best Avg Validation Lenient F1: 0.8838\n",
      "Cleaning up resources for task: Providing_Guidance\n",
      "\n",
      "-- Training Single-Task Model for: Actionability (Task 4/4) --\n",
      "Loading custom concat-based single-task model: bert-base-uncased with 3 labels.\n",
      "--- Starting Training --- [Epochs: 5, Strategy: single_task, Loss: FocalLoss, Task Index: 3]\n",
      "  Epoch 1/5, Step 50/62, Batch Loss: 0.2518\n",
      "  Epoch 1/5, Step 62/62, Batch Loss: 0.2731\n",
      "Epoch 1/5 | Time: 74.55s | Avg Train Loss: 0.3352\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8895\n",
      "    Actionability: Exact Acc: 0.7097, Exact F1: 0.5001, Lenient Acc: 0.8387, Lenient F1: 0.8895\n",
      "  Validation metric improved (-inf --> 0.8895).\n",
      "  Epoch 2/5, Step 50/62, Batch Loss: 0.1291\n",
      "  Epoch 2/5, Step 62/62, Batch Loss: 0.2170\n",
      "Epoch 2/5 | Time: 78.71s | Avg Train Loss: 0.2503\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8922\n",
      "    Actionability: Exact Acc: 0.7379, Exact F1: 0.6170, Lenient Acc: 0.8548, Lenient F1: 0.8922\n",
      "  Validation metric improved (0.8895 --> 0.8922).\n",
      "  Epoch 3/5, Step 50/62, Batch Loss: 0.1986\n",
      "  Epoch 3/5, Step 62/62, Batch Loss: 0.2373\n",
      "Epoch 3/5 | Time: 74.81s | Avg Train Loss: 0.1782\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8862\n",
      "    Actionability: Exact Acc: 0.7339, Exact F1: 0.6042, Lenient Acc: 0.8468, Lenient F1: 0.8862\n",
      "  Validation metric did not improve. Patience: 1/3.\n",
      "  Epoch 4/5, Step 50/62, Batch Loss: 0.0843\n",
      "  Epoch 4/5, Step 62/62, Batch Loss: 0.0789\n",
      "Epoch 4/5 | Time: 78.51s | Avg Train Loss: 0.1016\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8982\n",
      "    Actionability: Exact Acc: 0.7379, Exact F1: 0.5872, Lenient Acc: 0.8629, Lenient F1: 0.8982\n",
      "  Validation metric improved (0.8922 --> 0.8982).\n",
      "  Epoch 5/5, Step 50/62, Batch Loss: 0.0512\n",
      "  Epoch 5/5, Step 62/62, Batch Loss: 0.0804\n",
      "Epoch 5/5 | Time: 75.11s | Avg Train Loss: 0.0530\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8758\n",
      "    Actionability: Exact Acc: 0.7218, Exact F1: 0.6203, Lenient Acc: 0.8387, Lenient F1: 0.8758\n",
      "  Validation metric did not improve. Patience: 1/3.\n",
      "--- Training Finished --- Best Avg Validation Lenient F1: 0.8982\n",
      "Cleaning up resources for task: Actionability\n",
      "--- Experiment 2 Completed Successfully ---\n",
      "Cleaning up experiment resources...\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Running Experiment 3 ---\n",
      "Config: {'exp_id': 3, 'model': 'roberta-base', 'strategy': 'single_task', 'loss': 'FocalLoss', 'remove_punct': False, 'remove_stopwords': False, 'include_history': False}\n",
      "Preparing Dataloaders...\n",
      "Data split sizes:\n",
      "  Train: 1980\n",
      "  Validation: 248\n",
      "  Test: 248\n",
      "Loading tokenizer: roberta-base\n",
      "Creating Train Dataset...\n",
      "Creating Validation Dataset...\n",
      "Creating Test Dataset...\n",
      "Using 19 workers for DataLoaders.\n",
      "Dataloaders created successfully.\n",
      "Initializing Single-Task Models (one per task)...\n",
      "Strategy: single_task. Will load roberta-base with 3 labels per task during training.\n",
      "Loading custom concat-based single-task model: roberta-base with 3 labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Training Single-Task Model for: Mistake_Identification (Task 1/4) --\n",
      "Loading custom concat-based single-task model: roberta-base with 3 labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training --- [Epochs: 5, Strategy: single_task, Loss: FocalLoss, Task Index: 0]\n",
      "  Epoch 1/5, Step 50/62, Batch Loss: 0.1807\n",
      "  Epoch 1/5, Step 62/62, Batch Loss: 0.1898\n",
      "Epoch 1/5 | Time: 77.54s | Avg Train Loss: 0.2557\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.9626\n",
      "    Mistake_Identification: Exact Acc: 0.8669, Exact F1: 0.5954, Lenient Acc: 0.9355, Lenient F1: 0.9626\n",
      "  Validation metric improved (-inf --> 0.9626).\n",
      "  Epoch 2/5, Step 50/62, Batch Loss: 0.1682\n",
      "  Epoch 2/5, Step 62/62, Batch Loss: 0.0899\n",
      "Epoch 2/5 | Time: 76.24s | Avg Train Loss: 0.1522\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.9446\n",
      "    Mistake_Identification: Exact Acc: 0.8468, Exact F1: 0.6351, Lenient Acc: 0.9073, Lenient F1: 0.9446\n",
      "  Validation metric did not improve. Patience: 1/3.\n",
      "  Epoch 3/5, Step 50/62, Batch Loss: 0.2639\n",
      "  Epoch 3/5, Step 62/62, Batch Loss: 0.1764\n",
      "Epoch 3/5 | Time: 75.78s | Avg Train Loss: 0.1357\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.9287\n",
      "    Mistake_Identification: Exact Acc: 0.8226, Exact F1: 0.6057, Lenient Acc: 0.8831, Lenient F1: 0.9287\n",
      "  Validation metric did not improve. Patience: 2/3.\n",
      "  Epoch 4/5, Step 50/62, Batch Loss: 0.0754\n",
      "  Epoch 4/5, Step 62/62, Batch Loss: 0.1050\n",
      "Epoch 4/5 | Time: 76.85s | Avg Train Loss: 0.0996\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.9644\n",
      "    Mistake_Identification: Exact Acc: 0.8427, Exact F1: 0.6606, Lenient Acc: 0.9395, Lenient F1: 0.9644\n",
      "  Validation metric improved (0.9626 --> 0.9644).\n",
      "  Epoch 5/5, Step 50/62, Batch Loss: 0.2347\n",
      "  Epoch 5/5, Step 62/62, Batch Loss: 0.0509\n",
      "Epoch 5/5 | Time: 52.56s | Avg Train Loss: 0.0609\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.9717\n",
      "    Mistake_Identification: Exact Acc: 0.8750, Exact F1: 0.6608, Lenient Acc: 0.9516, Lenient F1: 0.9717\n",
      "  Validation metric improved (0.9644 --> 0.9717).\n",
      "--- Training Finished --- Best Avg Validation Lenient F1: 0.9717\n",
      "Cleaning up resources for task: Mistake_Identification\n",
      "\n",
      "-- Training Single-Task Model for: Mistake_Location (Task 2/4) --\n",
      "Loading custom concat-based single-task model: roberta-base with 3 labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training --- [Epochs: 5, Strategy: single_task, Loss: FocalLoss, Task Index: 1]\n",
      "  Epoch 1/5, Step 50/62, Batch Loss: 0.3848\n",
      "  Epoch 1/5, Step 62/62, Batch Loss: 0.2451\n",
      "Epoch 1/5 | Time: 38.45s | Avg Train Loss: 0.3791\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8282\n",
      "    Mistake_Location: Exact Acc: 0.6855, Exact F1: 0.4486, Lenient Acc: 0.7540, Lenient F1: 0.8282\n",
      "  Validation metric improved (-inf --> 0.8282).\n",
      "  Epoch 2/5, Step 50/62, Batch Loss: 0.1608\n",
      "  Epoch 2/5, Step 62/62, Batch Loss: 0.2368\n",
      "Epoch 2/5 | Time: 38.56s | Avg Train Loss: 0.2871\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8627\n",
      "    Mistake_Location: Exact Acc: 0.7298, Exact F1: 0.4857, Lenient Acc: 0.8024, Lenient F1: 0.8627\n",
      "  Validation metric improved (0.8282 --> 0.8627).\n",
      "  Epoch 3/5, Step 50/62, Batch Loss: 0.3854\n",
      "  Epoch 3/5, Step 62/62, Batch Loss: 0.2304\n",
      "Epoch 3/5 | Time: 38.56s | Avg Train Loss: 0.2343\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8780\n",
      "    Mistake_Location: Exact Acc: 0.7419, Exact F1: 0.4900, Lenient Acc: 0.8185, Lenient F1: 0.8780\n",
      "  Validation metric improved (0.8627 --> 0.8780).\n",
      "  Epoch 4/5, Step 50/62, Batch Loss: 0.1462\n",
      "  Epoch 4/5, Step 62/62, Batch Loss: 0.2559\n",
      "Epoch 4/5 | Time: 38.59s | Avg Train Loss: 0.1763\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8871\n",
      "    Mistake_Location: Exact Acc: 0.7500, Exact F1: 0.5853, Lenient Acc: 0.8266, Lenient F1: 0.8871\n",
      "  Validation metric improved (0.8780 --> 0.8871).\n",
      "  Epoch 5/5, Step 50/62, Batch Loss: 0.0782\n",
      "  Epoch 5/5, Step 62/62, Batch Loss: 0.1458\n",
      "Epoch 5/5 | Time: 38.64s | Avg Train Loss: 0.1061\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8488\n",
      "    Mistake_Location: Exact Acc: 0.7056, Exact F1: 0.5238, Lenient Acc: 0.7903, Lenient F1: 0.8488\n",
      "  Validation metric did not improve. Patience: 1/3.\n",
      "--- Training Finished --- Best Avg Validation Lenient F1: 0.8871\n",
      "Cleaning up resources for task: Mistake_Location\n",
      "\n",
      "-- Training Single-Task Model for: Providing_Guidance (Task 3/4) --\n",
      "Loading custom concat-based single-task model: roberta-base with 3 labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training --- [Epochs: 5, Strategy: single_task, Loss: FocalLoss, Task Index: 2]\n",
      "  Epoch 1/5, Step 50/62, Batch Loss: 0.3384\n",
      "  Epoch 1/5, Step 62/62, Batch Loss: 0.4159\n",
      "Epoch 1/5 | Time: 59.71s | Avg Train Loss: 0.4161\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8696\n",
      "    Providing_Guidance: Exact Acc: 0.6492, Exact F1: 0.4267, Lenient Acc: 0.7944, Lenient F1: 0.8696\n",
      "  Validation metric improved (-inf --> 0.8696).\n",
      "  Epoch 2/5, Step 50/62, Batch Loss: 0.4222\n",
      "  Epoch 2/5, Step 62/62, Batch Loss: 0.2570\n",
      "Epoch 2/5 | Time: 78.32s | Avg Train Loss: 0.3470\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.9059\n",
      "    Providing_Guidance: Exact Acc: 0.6935, Exact F1: 0.4620, Lenient Acc: 0.8468, Lenient F1: 0.9059\n",
      "  Validation metric improved (0.8696 --> 0.9059).\n",
      "  Epoch 3/5, Step 50/62, Batch Loss: 0.2319\n",
      "  Epoch 3/5, Step 62/62, Batch Loss: 0.4197\n",
      "Epoch 3/5 | Time: 75.94s | Avg Train Loss: 0.3000\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8684\n",
      "    Providing_Guidance: Exact Acc: 0.6532, Exact F1: 0.4596, Lenient Acc: 0.7984, Lenient F1: 0.8684\n",
      "  Validation metric did not improve. Patience: 1/3.\n",
      "  Epoch 4/5, Step 50/62, Batch Loss: 0.2284\n",
      "  Epoch 4/5, Step 62/62, Batch Loss: 0.1959\n",
      "Epoch 4/5 | Time: 78.88s | Avg Train Loss: 0.2367\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.9003\n",
      "    Providing_Guidance: Exact Acc: 0.6774, Exact F1: 0.5442, Lenient Acc: 0.8427, Lenient F1: 0.9003\n",
      "  Validation metric did not improve. Patience: 2/3.\n",
      "  Epoch 5/5, Step 50/62, Batch Loss: 0.1477\n",
      "  Epoch 5/5, Step 62/62, Batch Loss: 0.1610\n",
      "Epoch 5/5 | Time: 75.12s | Avg Train Loss: 0.1660\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8602\n",
      "    Providing_Guidance: Exact Acc: 0.6492, Exact F1: 0.5368, Lenient Acc: 0.7863, Lenient F1: 0.8602\n",
      "  Validation metric did not improve. Patience: 3/3.\n",
      "Early stopping triggered after 3 epochs without improvement.\n",
      "--- Training Finished --- Best Avg Validation Lenient F1: 0.9059\n",
      "Cleaning up resources for task: Providing_Guidance\n",
      "\n",
      "-- Training Single-Task Model for: Actionability (Task 4/4) --\n",
      "Loading custom concat-based single-task model: roberta-base with 3 labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training --- [Epochs: 5, Strategy: single_task, Loss: FocalLoss, Task Index: 3]\n",
      "  Epoch 1/5, Step 50/62, Batch Loss: 0.3698\n",
      "  Epoch 1/5, Step 62/62, Batch Loss: 0.2898\n",
      "Epoch 1/5 | Time: 78.54s | Avg Train Loss: 0.4000\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8296\n",
      "    Actionability: Exact Acc: 0.6734, Exact F1: 0.4836, Lenient Acc: 0.7863, Lenient F1: 0.8296\n",
      "  Validation metric improved (-inf --> 0.8296).\n",
      "  Epoch 2/5, Step 50/62, Batch Loss: 0.2523\n",
      "  Epoch 2/5, Step 62/62, Batch Loss: 0.3501\n",
      "Epoch 2/5 | Time: 74.85s | Avg Train Loss: 0.2872\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8693\n",
      "    Actionability: Exact Acc: 0.6815, Exact F1: 0.5728, Lenient Acc: 0.8266, Lenient F1: 0.8693\n",
      "  Validation metric improved (0.8296 --> 0.8693).\n",
      "  Epoch 3/5, Step 50/62, Batch Loss: 0.2771\n",
      "  Epoch 3/5, Step 62/62, Batch Loss: 0.3625\n",
      "Epoch 3/5 | Time: 78.56s | Avg Train Loss: 0.2259\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.9138\n",
      "    Actionability: Exact Acc: 0.7500, Exact F1: 0.5543, Lenient Acc: 0.8790, Lenient F1: 0.9138\n",
      "  Validation metric improved (0.8693 --> 0.9138).\n",
      "  Epoch 4/5, Step 50/62, Batch Loss: 0.2141\n",
      "  Epoch 4/5, Step 62/62, Batch Loss: 0.1486\n",
      "Epoch 4/5 | Time: 76.23s | Avg Train Loss: 0.1731\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8909\n",
      "    Actionability: Exact Acc: 0.7339, Exact F1: 0.5291, Lenient Acc: 0.8548, Lenient F1: 0.8909\n",
      "  Validation metric did not improve. Patience: 1/3.\n",
      "  Epoch 5/5, Step 50/62, Batch Loss: 0.0657\n",
      "  Epoch 5/5, Step 62/62, Batch Loss: 0.0762\n",
      "Epoch 5/5 | Time: 77.86s | Avg Train Loss: 0.1219\n",
      "  Avg Validation Lenient F1 (over evaluated tasks): 0.8848\n",
      "    Actionability: Exact Acc: 0.7258, Exact F1: 0.6074, Lenient Acc: 0.8468, Lenient F1: 0.8848\n",
      "  Validation metric did not improve. Patience: 2/3.\n",
      "--- Training Finished --- Best Avg Validation Lenient F1: 0.9138\n",
      "Cleaning up resources for task: Actionability\n",
      "--- Experiment 3 Completed Successfully ---\n",
      "Cleaning up experiment resources...\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Experiment Runner Finished ---\n",
      "Total experiments run (attempted): 3\n",
      "Successfully completed experiments: 3\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim import AdamW # Import specifically AdamW from torch.optim\n",
    "from transformers import get_linear_schedule_with_warmup # Optional scheduler\n",
    "import gc # Garbage collector\n",
    "import traceback # For error printing\n",
    "import time # Added for potential timing if needed\n",
    "\n",
    "# Ensure df_raw, CLASS_WEIGHTS and all necessary functions/constants are defined\n",
    "# (create_dataloaders, get_model, train_and_evaluate, compute_metrics, FocalLoss)\n",
    "# Also needs: PREPROCESSING_OPTIONS, BASE_MODELS_TO_TRY, TASK_STRATEGIES,\n",
    "# LOSS_FUNCTIONS_TO_TRY, BATCH_SIZE, MAX_LENGTH, LEARNING_RATE, EPOCHS,\n",
    "# DEVICE, SEED, TASK_LIST, NUM_TASKS\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# --- Generate all preprocessing combinations ---\n",
    "pp_keys = PREPROCESSING_OPTIONS.keys()\n",
    "pp_value_combinations = list(itertools.product(*PREPROCESSING_OPTIONS.values()))\n",
    "preprocessing_configs = [dict(zip(pp_keys, values)) for values in pp_value_combinations]\n",
    "\n",
    "print(f\"Starting Experiment Runner...\")\n",
    "print(f\"Total preprocessing configs: {len(preprocessing_configs)}\")\n",
    "print(f\"Models to try: {BASE_MODELS_TO_TRY}\")\n",
    "print(f\"Strategies to try: {TASK_STRATEGIES}\")\n",
    "print(f\"Loss functions to try: {LOSS_FUNCTIONS_TO_TRY}\") # Should include new ones now\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Main Experiment Loop ---\n",
    "experiment_count = 0\n",
    "for pp_flags in preprocessing_configs:\n",
    "    for base_model_name in BASE_MODELS_TO_TRY:\n",
    "        for strategy in TASK_STRATEGIES:\n",
    "            for loss_fn_name in LOSS_FUNCTIONS_TO_TRY: # Now iterates through new losses\n",
    "                experiment_count += 1\n",
    "                config = {\n",
    "                    'exp_id': experiment_count,\n",
    "                    'model': base_model_name,\n",
    "                    'strategy': strategy,\n",
    "                    'loss': loss_fn_name,\n",
    "                    **pp_flags # Add preprocessing flags\n",
    "                }\n",
    "                print(f\"\\n--- Running Experiment {experiment_count} ---\")\n",
    "                print(f\"Config: {config}\")\n",
    "\n",
    "                # --- 1. Data Preparation ---\n",
    "                print(\"Preparing Dataloaders...\")\n",
    "                # (Dataloader creation code remains the same)\n",
    "                try:\n",
    "                    train_loader, val_loader, test_loader, tokenizer = create_dataloaders(\n",
    "                        df=df_raw,\n",
    "                        tokenizer_name=base_model_name,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        max_len=MAX_LENGTH,\n",
    "                        preprocessing_flags=pp_flags,\n",
    "                        random_state=SEED\n",
    "                    )\n",
    "                    if not train_loader or not val_loader:\n",
    "                         print(\"Error: Dataloader creation failed. Skipping experiment.\")\n",
    "                         result_entry = {'config': config, 'status': 'dataloader_error', 'val_metrics': None}\n",
    "                         all_results.append(result_entry)\n",
    "                         continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during dataloader creation for config {config}: {e}\")\n",
    "                    traceback.print_exc()\n",
    "                    result_entry = {'config': config, 'status': 'dataloader_exception', 'val_metrics': None}\n",
    "                    all_results.append(result_entry)\n",
    "                    continue\n",
    "\n",
    "                # --- 2. Model, Optimizer, Scheduler Setup ---\n",
    "                model = None\n",
    "                optimizer = None\n",
    "                scheduler = None\n",
    "                best_val_metrics_agg = {}\n",
    "\n",
    "                try:\n",
    "                    if strategy == 'multi_task':\n",
    "                        print(\"Initializing Multi-Task Model...\")\n",
    "                        model = get_model(strategy, base_model_name)\n",
    "                        if model:\n",
    "                            model.to(DEVICE)\n",
    "                            optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "                            # scheduler = ... # Optional\n",
    "                        else:\n",
    "                            raise ValueError(\"Multi-task model creation failed.\")\n",
    "\n",
    "                        print(\"Starting Multi-Task Training...\")\n",
    "                        best_model_state, history = train_and_evaluate(\n",
    "                            model=model,\n",
    "                            train_loader=train_loader,\n",
    "                            val_loader=val_loader,\n",
    "                            optimizer=optimizer,\n",
    "                            loss_fn_provider=loss_fn_name, # Pass loss name\n",
    "                            class_weights=CLASS_WEIGHTS, # *** Pass class weights ***\n",
    "                            device=DEVICE,\n",
    "                            epochs=EPOCHS,\n",
    "                            strategy=strategy,\n",
    "                            scheduler=scheduler\n",
    "                        )\n",
    "                        # (Result extraction remains the same)\n",
    "                        best_epoch_metrics = {}\n",
    "                        if history and history.get('val_metrics'):\n",
    "                             epoch_avg_f1s = [np.mean([m.get('lenient_f1', 0.0) for m in epoch_metrics.values()]) if epoch_metrics else -1 for epoch_metrics in history['val_metrics']]\n",
    "                             if epoch_avg_f1s:\n",
    "                                  best_epoch_idx = np.argmax(epoch_avg_f1s)\n",
    "                                  best_epoch_metrics = history['val_metrics'][best_epoch_idx]\n",
    "                             else: print(\"Warning: No valid validation metrics found in history.\")\n",
    "                        best_val_metrics_agg = best_epoch_metrics\n",
    "\n",
    "\n",
    "                    elif strategy == 'single_task':\n",
    "                        print(\"Initializing Single-Task Models (one per task)...\")\n",
    "                        model_creator = get_model(strategy, base_model_name)\n",
    "                        if not model_creator: raise ValueError(\"Single-task model creator function not obtained.\")\n",
    "\n",
    "                        task_histories = {}\n",
    "                        task_best_metrics = {}\n",
    "\n",
    "                        for task_idx, task_name in enumerate(TASK_LIST):\n",
    "                            print(f\"\\n-- Training Single-Task Model for: {task_name} (Task {task_idx+1}/{NUM_TASKS}) --\")\n",
    "                            task_model = model_creator(base_model_name, NUM_CLASSES)\n",
    "                            if not task_model: print(f\"Error creating model for task {task_name}. Skipping task.\"); continue\n",
    "                            task_model.to(DEVICE)\n",
    "                            task_optimizer = AdamW(task_model.parameters(), lr=LEARNING_RATE)\n",
    "                            # task_scheduler = ... # Optional\n",
    "\n",
    "                            best_task_model_state, task_history = train_and_evaluate(\n",
    "                                model=task_model,\n",
    "                                train_loader=train_loader,\n",
    "                                val_loader=val_loader,\n",
    "                                optimizer=task_optimizer,\n",
    "                                loss_fn_provider=loss_fn_name, # Pass loss name\n",
    "                                class_weights=CLASS_WEIGHTS, # *** Pass class weights ***\n",
    "                                device=DEVICE,\n",
    "                                epochs=EPOCHS,\n",
    "                                strategy=strategy,\n",
    "                                task_index=task_idx,\n",
    "                                scheduler=None # Optional: task_scheduler\n",
    "                            )\n",
    "                            task_histories[task_name] = task_history\n",
    "                            # (Result extraction remains the same)\n",
    "                            best_task_epoch_metrics = {}\n",
    "                            if task_history and task_history.get('val_metrics'):\n",
    "                                 epoch_task_f1s = [epoch_metrics.get(task_name, {}).get('lenient_f1', -1) if epoch_metrics else -1 for epoch_metrics in task_history['val_metrics']]\n",
    "                                 if epoch_task_f1s:\n",
    "                                      best_epoch_idx = np.argmax(epoch_task_f1s)\n",
    "                                      if best_epoch_idx < len(task_history['val_metrics']):\n",
    "                                           best_task_epoch_metrics = task_history['val_metrics'][best_epoch_idx].get(task_name, {})\n",
    "                                      else: print(f\"Warning: Best epoch index {best_epoch_idx} out of range for task {task_name} history.\")\n",
    "                                 else: print(f\"Warning: No valid validation metrics found in history for task {task_name}.\")\n",
    "                            task_best_metrics[task_name] = best_task_epoch_metrics\n",
    "\n",
    "                            print(f\"Cleaning up resources for task: {task_name}\")\n",
    "                            del task_model, task_optimizer\n",
    "                            if 'best_task_model_state' in locals(): del best_task_model_state\n",
    "                            gc.collect()\n",
    "                            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "                        best_val_metrics_agg = task_best_metrics\n",
    "\n",
    "\n",
    "                    # --- Store results for this configuration ---\n",
    "                    result_entry = { 'config': config, 'status': 'success', 'val_metrics': best_val_metrics_agg }\n",
    "                    all_results.append(result_entry)\n",
    "                    print(f\"--- Experiment {experiment_count} Completed Successfully ---\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during training/evaluation for config {config}: {e}\")\n",
    "                    traceback.print_exc()\n",
    "                    result_entry = {'config': config, 'status': 'train_eval_exception', 'val_metrics': None}\n",
    "                    all_results.append(result_entry)\n",
    "                    print(f\"--- Experiment {experiment_count} Failed ---\")\n",
    "\n",
    "                # --- Cleanup for the experiment ---\n",
    "                # (Cleanup code remains the same)\n",
    "                print(\"Cleaning up experiment resources...\")\n",
    "                if 'model' in locals() and model is not None: del model\n",
    "                if 'optimizer' in locals() and optimizer is not None : del optimizer\n",
    "                if 'scheduler' in locals() and scheduler is not None: del scheduler\n",
    "                if 'train_loader' in locals() and train_loader is not None: del train_loader\n",
    "                if 'val_loader' in locals() and val_loader is not None: del val_loader\n",
    "                if 'test_loader' in locals() and test_loader is not None: del test_loader\n",
    "                if 'tokenizer' in locals() and tokenizer is not None: del tokenizer\n",
    "                if 'best_model_state' in locals(): del best_model_state\n",
    "                if 'history' in locals(): del history\n",
    "                if 'task_histories' in locals(): del task_histories\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- Convert results to DataFrame ---\n",
    "# (Result conversion and saving code remains the same)\n",
    "print(\"\\n--- Experiment Runner Finished ---\")\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(f\"Total experiments run (attempted): {experiment_count}\")\n",
    "if 'status' in results_df.columns:\n",
    "    success_count = len(results_df[results_df['status'] == 'success'])\n",
    "    print(f\"Successfully completed experiments: {success_count}\")\n",
    "else:\n",
    "    print(\"Status column not found in results, cannot count successful experiments.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d33056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total experiments run (attempted): 3\n",
      "Successfully completed experiments: 3\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(all_results)\n",
    "print(f\"Total experiments run (attempted): {experiment_count}\")\n",
    "if 'status' in results_df.columns:\n",
    "    success_count = len(results_df[results_df['status'] == 'success'])\n",
    "    print(f\"Successfully completed experiments: {success_count}\")\n",
    "else:\n",
    "    print(\"Status column not found in results, cannot count successful experiments.\")\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('layer12.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
